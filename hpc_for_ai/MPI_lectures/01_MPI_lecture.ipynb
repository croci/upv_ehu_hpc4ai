{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ea61ad-4a7b-4c18-aefc-0ef67e4abce0",
   "metadata": {},
   "source": [
    "# Lecture 6: Parallel Computing II: Message Passing Interface (MPI)\n",
    "\n",
    "## NOTES\n",
    "\n",
    "* For these practicals we will be using a different `conda environment`. When opening a notebook or a terminal make sure you are using the **CuPy Kernel**!!!\n",
    " \n",
    "## IMPORTANT!!!\n",
    "\n",
    "MPI code must typically be invoked from the **command line** and not from a notebook. However, there is a workaround. I will demonstrate both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f5ce3-70e9-4d4e-ad4b-7bc3b8edc5f7",
   "metadata": {},
   "source": [
    "## Practical 1 - Hello World!\n",
    "\n",
    "This practical shows you the simplest possible MPI script and teaches you how to run it.\n",
    "\n",
    "### The non-standard way\n",
    "\n",
    "First, let's start with the non-standard way by using the notebook.\n",
    "\n",
    "Adding the line `%%writefile FILENAME.py` at the beginning of a cell will create and overwrite a file called `FILENAME.py`. We can thus write: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a87c45-ac10-4a1d-b2a9-b45bd1bcd0f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hello_world.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hello_world.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD # this is the standard MPI communicator (there are others). It is the WORLD COMMUNICATOR, i.e. it allows to communicate across all processes.\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "print(\"I am rank: %d. There are: %d of us.\" % (rank,size), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb460f-3241-44f1-ba70-dd7c0f116a85",
   "metadata": {},
   "source": [
    "Running the above will create a file called `hello_world.py` with the cell contents inside. Warning: if the file exists already it will be overwritten!\n",
    "\n",
    "Once the file is created we can call it. However, **it must be called from the terminal**, not from Python. Jupyter has a trick that allows us to do so:\n",
    "any command starting with an exclamation mark will be executed in bash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbd14bb-0fab-4820-a281-e75d8a16278e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am rank: 3. There are: 4 of us.\n",
      "I am rank: 0. There are: 4 of us.\n",
      "I am rank: 2. There are: 4 of us.\n",
      "I am rank: 1. There are: 4 of us.\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python3 hello_world.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f90d6-a33c-44ce-aac0-92b85547cfc9",
   "metadata": {},
   "source": [
    "**AN IMPORTANT CATCH WITH THE NON-STANDARD WAY**\n",
    "\n",
    "Do not run mpi4py code in Jupyter directly without using `writefile` and a separate bash command. If you really must, then please interrupt the kernel afterwards (but please avoid doing this in the first place).\n",
    "\n",
    "The long explanation (you do not have to read this):\n",
    "\n",
    "Any MPI execution must be initialised and terminated with matching MPI_Init and MPI_Finalize calls. You never see this in Python since `mpi4py` instantiates this calls for you under the hood. However,  the moment you import `mpi4py` interactively in Jupyter the kernel will call MPI_Init with a single process (you are not using `mpiexec` so it is like you were doing it with `-n 1`). However, MPI_Finalize won't be called until the kernel is interrupted. I cannot fully explain why, but I noticed that if you then go on and try to run parallel code with `mpiexec` this seems to cause trouble and a deadlock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa535bb0-69df-478e-937b-7affc5acdc4c",
   "metadata": {},
   "source": [
    "### The standard way - A premise\n",
    "\n",
    "Frankly, for the purposes of this lectures I advise you to use the non-standard way, but let it be clear that **you should be using the standard way in general**.\n",
    "\n",
    "The reason is that Jupyter was designed for serial work and it interacts a bit weirdly with MPI (e.g., see above note). Depending on Jupyter for coding with MPI is therefore silly.\n",
    "\n",
    "**Remember:** If you ever need to use MPI again, invoke `mpiexec` (or `mpirun`) directly from terminal, not from jupyter.\n",
    "\n",
    "### The standard way\n",
    "\n",
    "The standard way works as follows:\n",
    "\n",
    "1- Create and save a file called `hello_world.py` with the content of the above cell, but without the writefile line.\n",
    "\n",
    "2- Open a terminal (in Jupyter press on the $+$ sign).\n",
    "\n",
    "3- Make sure you are in the correct conda environment:\n",
    "```bash\n",
    "    conda init\n",
    "    conda activate cupy\n",
    "```\n",
    "\n",
    "4- Run the script in the terminal using `mpiexec`:\n",
    "```bash\n",
    "    mpiexec -n 4 python3 hello_world.py\n",
    "```\n",
    "\n",
    "**Note:** From now on I will be using the non-standard way since for teaching purposes it is just too convenient. However, I warned you!\n",
    "\n",
    "### Explanation of the `hello_world.py` code and a few comments\n",
    "\n",
    "* comm is a MPI communicator. It allows communication between processes (we will see how in the next slides). It also holds rank, size and other information.\n",
    "* Here we are passing flush=True to the print function so that the stdout is flushed when print is called. Otherwise all print statements may execute only after the program has finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49383e2-7de4-4af9-8e2d-bbb4c5e1f85d",
   "metadata": {},
   "source": [
    "## Interlude - Which MPI installation am I using?\n",
    "\n",
    "You can check which MPI version and installation you are using by querying the version of the MPI executable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bad78c-bbbc-4cc5-ad2e-69c336b3910f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYDRA build details:\n",
      "    Version:                                 4.0\n",
      "    Release Date:                            Fri Jan 21 10:42:29 CST 2022\n",
      "    CC:                              gcc -Wdate-time -D_FORTIFY_SOURCE=2 -g -O2 -ffile-prefix-map=/build/mpich-0xgrG5/mpich-4.0=. -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security  -Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects -flto=auto -Wl,-z,relro \n",
      "    Configure options:                       '--with-hwloc-prefix=/usr' '--with-device=ch4:ofi' 'FFLAGS=-O2 -ffile-prefix-map=/build/mpich-0xgrG5/mpich-4.0=. -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -fallow-invalid-boz -fallow-argument-mismatch' '--prefix=/usr' 'CFLAGS=-g -O2 -ffile-prefix-map=/build/mpich-0xgrG5/mpich-4.0=. -flto=auto -ffat-lto-objects -flto=auto -ffat-lto-objects -fstack-protector-strong -Wformat -Werror=format-security' 'LDFLAGS=-Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects -flto=auto -Wl,-z,relro' 'CPPFLAGS=-Wdate-time -D_FORTIFY_SOURCE=2'\n",
      "    Process Manager:                         pmi\n",
      "    Launchers available:                     ssh rsh fork slurm ll lsf sge manual persist\n",
      "    Topology libraries available:            hwloc\n",
      "    Resource management kernels available:   user slurm ll lsf sge pbs cobalt\n",
      "    Demux engines available:                 poll select\n"
     ]
    }
   ],
   "source": [
    "!mpiexec --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fae732-4d45-4b9b-9d10-acd53ea1e3cf",
   "metadata": {},
   "source": [
    "## Practical 2 - Distributed memory and process management\n",
    "\n",
    "Remember the rules of the MPI club: each process reads and executes the same code and will store a full independent copy of each variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fe73ad-5eea-41fd-b46c-f839a1a2bc9b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp.py\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Useful to always include this in your code\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "a = np.arange(1000)\n",
    "b = np.linalg.norm(a)\n",
    "\n",
    "print(\"Rank: %d. I hold a copy of a of size %d and of norm %.3f\" % (rank,a.size, b), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f221d22-9125-410f-9288-1eb380c31c23",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 0. I hold a copy of a of size 1000 and of norm 18243.725\n",
      "Rank: 3. I hold a copy of a of size 1000 and of norm 18243.725\n",
      "Rank: 1. I hold a copy of a of size 1000 and of norm 18243.725\n",
      "Rank: 2. I hold a copy of a of size 1000 and of norm 18243.725\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python3 temp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78326a18-636b-4f78-9744-4d1bc33bfd39",
   "metadata": {},
   "source": [
    "As you can see, each process will have its own copy and all copies will be the same!\n",
    "\n",
    "**WARNING!** It is important to keep this in mind: If you create or load a very large array and you load it on many processes you may make the computer run out of memory!!!\n",
    "\n",
    "**Note:** I do not know if you noticed already, but the order in which each process prints out its message is completely arbitrary. In fact you cannot even decide which CPU thread will be assigned which rank!\n",
    "\n",
    "### Question: How can I tell different processes to do different things then?\n",
    "\n",
    "###\n",
    "\n",
    "<details>\n",
    "    <summary> <b>Want a hint?</b> Click here!</summary>\n",
    "    Think about it: There is only one variable that has a different value across processes!\n",
    "</details>\n",
    "\n",
    "###\n",
    "\n",
    "<details>\n",
    "    <summary> <b>Solution</b></summary>\n",
    "    Different processes have different ranks so we can use that single information!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb6849e-e66b-487e-b1e6-786423d14c86",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp.py\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Useful to always include this in your code\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "M = 10\n",
    "a = np.arange(M*size)[rank*M:(rank+1)*M]\n",
    "b = np.linalg.norm(a)\n",
    "\n",
    "print(\"Rank: %d. I hold a copy of a of size %d and of norm %.3f\" % (rank, a.size, b), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd83a8d5-db0f-483e-aaaa-a7d6ba9856c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 0. I hold a copy of a of size 10 and of norm 16.882\n",
      "Rank: 1. I hold a copy of a of size 10 and of norm 46.744\n",
      "Rank: 2. I hold a copy of a of size 10 and of norm 78.006\n",
      "Rank: 3. I hold a copy of a of size 10 and of norm 109.476\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python3 temp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc231e3-b854-4856-bf01-cc40b62fdeeb",
   "metadata": {},
   "source": [
    "Now each process holds a smaller copy of the array. This sort of appreach is very useful to prevent running out of memory and is the basis of domain decomposition methods used in scientific computing to solve partial differential equations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
