{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30064e54-b029-4b4d-934a-717b160469cb",
   "metadata": {},
   "source": [
    "# High-performance and parallel computing for AI - Practical 9: Numba-CUDA and more GPU programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431376a-ccf3-4363-bb6f-bf439327b872",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "=========\n",
    "\n",
    "* CuPy behaves weirdly for me. Restart the kernel if you encounter weird errors.\n",
    "* For these practicals we will be using a different `conda environment`. When opening a notebook or a terminal make sure you are using the **CuPy Kernel**!!!\n",
    "* It's fine if you do not finish everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888116a-89b5-469b-ba3e-8083162580a9",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "\n",
    "Before you start (and before running any other GPU code on the servers) please run the following code, which limits the maximum GPU memory usage to $1.5$ GB and picks an L40s GPU and a Quadro GPU at random. **Please only run the code below once every time you restart the kernel!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04aeea5d-0afe-402f-8988-e1dce92543d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CuPy-specific environment variables\n",
    "os.environ[\"CUPY_GPU_MEMORY_LIMIT\"] = \"1573741824\" # roughly 1.5 GB\n",
    "os.environ[\"CUPY_ACCELERATORS\"]=\"cutensor\" # activates cutensor acceleration\n",
    "os.environ[\"CUPY_TF32\"] = \"1\" # activates tf32 tensor cores\n",
    "\n",
    "## On goliat we have FIVE GPUs so here we pick two of those at random\n",
    "## so that we do not overload the system.\n",
    "## The way we do it is by figuring out the GPU UUIDs and then setting\n",
    "## The CUDA_VISIBLE_DEVICES environment variable.\n",
    "## Note: this is useful for other libraries as well (e.g., Jax, PyTorch, TF) in multi-GPU servers.\n",
    "\n",
    "# To get these UUIDs you need to run nvidia-smi -q on the command line\n",
    "quadro_UUIDs = [\"GPU-4efa947b-abbd-7c6e-84f5-61241d34bb4b\",\n",
    "                \"GPU-5eb524b0-2b1b-fe98-e6ed-b8fb5185e993\"]\n",
    "\n",
    "L40s_UUIDs = [\"GPU-7bba1f33-03d2-016b-d42e-ced83c3ac243\",\n",
    "              \"GPU-179d068a-3bea-91d7-1a8c-7017f55d6298\",\n",
    "              \"GPU-ae634859-dd49-de46-9182-195639405eaa\"]\n",
    "\n",
    "from numpy.random import randint\n",
    "# Picks an L40s and a Quadro GPU at random. The others will be invisible to CuPy\n",
    "# NOTE: this only works if the environment variable is set BEFORE CuPy is first imported.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = L40s_UUIDs[randint(3)] + \",\" + quadro_UUIDs[randint(2)]\n",
    "\n",
    "## CuPy and Numba will only see these GPUs and will assign them these device numbers:\n",
    "L40sID = 0\n",
    "quadro_ID = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81648cee-8dd5-4d3b-9e72-0ab35c5a1cfd",
   "metadata": {},
   "source": [
    "## Tutorial 1 - Numba-CUDA\n",
    "\n",
    "Numba-CUDA is a a spinoff of Numba which is now being developed separately. However, the numba-cuda docs are still currently part of the mainline numba docs. You can find them [here](https://numba.readthedocs.io/en/stable/cuda/index.html).\n",
    "\n",
    "Numba-CUDA works similarly to CuPy JIT-Rawkernel. However, it is much better documented.\n",
    "\n",
    "As a first example we show the solution to Question 3 of Practical 8 implemented using numba-CUDA (with a little help from CuPy). Please study this code as it will help you understand the basics of Numba-CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1766bde0-23fa-4795-a23c-e7681567eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mywrappedfun        :    CPU:    62.316 us   +/-  4.039 (min:    58.040 / max:    79.410) us     GPU-0:   344.753 us   +/-  3.965 (min:   339.968 / max:   361.472) us\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "# NOTE: the following CUDA variables are available\n",
    "# when writing Numba-CUDA kernels.\n",
    "#\n",
    "#     cuda.threadIdx.x, cuda.blockIdx.x, cuda.blockDim.x\n",
    "#\n",
    "# So that, for instance, you can get the thread ID\n",
    "# and the stride in the x,y directions as follows:\n",
    "#\n",
    "#     tidx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "#     ntidx = cuda.gridDim.x * cuda.blockDim.x\n",
    "#     tidy = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "#     ntidy = cuda.gridDim.y * cuda.blockDim.y\n",
    "#\n",
    "# However, in Numba-CUDA this is made simpler thanks\n",
    "# to the helper functions cuda.grid(dim) and cuda.gridsize(dim).\n",
    "# Just do:\n",
    "#\n",
    "#     tidx, tidy = cuda.grid(2)\n",
    "#     ntidx,ntidy = cuda.gridsize(2)\n",
    "\n",
    "@cuda.jit\n",
    "def myfun(x, y, z): # z is the OUTPUT. It will be overwritten!\n",
    "    tidx, tidy = cuda.grid(2) \n",
    "    ntidx,ntidy = cuda.gridsize(2)\n",
    "    for i in range(tidy, n, ntidy):\n",
    "        for j in range(tidx, n, ntidx):\n",
    "            z[i, j] = x[i,j]*y[i,j]\n",
    "\n",
    "gridDim = (128, 128)\n",
    "blockSize = (32, 32)\n",
    "\n",
    "n = 4096 \n",
    "\n",
    "# Initialising CuPy arrays directly on the device\n",
    "acp = cp.random.randn(n, n, dtype=np.float32)\n",
    "bcp = cp.random.randn(n, n, dtype=np.float32)\n",
    "ccp = cp.empty((n, n), dtype=np.float32)\n",
    "\n",
    "# While Numba-CUDA kernels are compatible with CuPy arrays\n",
    "# in my experiment it is faster to convert CuPy arrays to\n",
    "# Numba-CUDA arrays by hand. To do so, you simply call\n",
    "# cuda.to_device(arr).\n",
    "# The same command also works to move arrays stored on the host onto the device.\n",
    "# However, doing this from CuPy avoids memory movement since they are already on the device.\n",
    "a = cuda.to_device(acp)\n",
    "b = cuda.to_device(bcp)\n",
    "c = cuda.to_device(ccp)\n",
    "\n",
    "# You can now call the kernel like this. c will be overwritten\n",
    "myfun[gridDim, blockSize](a, b, c)\n",
    "\n",
    "# Getting the array back into CuPy\n",
    "# Again, CuPy is smart and no copy/memory movement\n",
    "# will be made\n",
    "ccp = cp.asarray(c)\n",
    "\n",
    "# This is not needed here, but this is how you get\n",
    "# a numba array back to the host.\n",
    "ch = c.copy_to_host()\n",
    "\n",
    "assert (ccp == acp*bcp).all()\n",
    "\n",
    "# You can wrap everything into a function \n",
    "# that only works with CuPy arrays for convenience\n",
    "# This is also so that we can benchmark it\n",
    "def mywrappedfun(gd, bs, cp_inputs):\n",
    "    nc_inputs = (cuda.to_device(item) for item in cp_inputs)\n",
    "    myfun[gd,bs](*nc_inputs)\n",
    "\n",
    "ccp = cp.zeros((n, n), dtype=np.float32)\n",
    "mywrappedfun(gridDim, blockSize, (acp, bcp, ccp))\n",
    "\n",
    "# Just to make sure c actually gets overwritten.\n",
    "# This also proves that using cuda.to_device does not move any memory\n",
    "assert ccp.sum() != 0 \n",
    "\n",
    "print(benchmark(mywrappedfun, (gridDim, blockSize, (a,b,c)), n_repeat=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775afef-0b4c-4e78-a15f-1a9dddffab43",
   "metadata": {},
   "source": [
    "Note that the above is as fast as the CuPy JIT-Rawkernel.\n",
    "However, the Numba-CUDA documentation is more extensive so I hope that it gives the developer more options. Both Numba-CUDA and CuPy are in active development so it is hard to tell how these functionalities will evolve in the future. It may be useful to know both, and Numba-CUDA shares some similarities with actual CUDA so it is good for teaching.\n",
    "\n",
    "Before we proceed, a quick timing of the cost of memory movement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dece90a8-1a3a-4a94-85ef-5293bae3e17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asarray             :    CPU:     9.890 us   +/-  1.817 (min:     9.088 / max:    50.526) us     GPU-0:    12.725 us   +/-  1.923 (min:    11.264 / max:    53.248) us\n",
      "to_device           :    CPU:    44.798 us   +/-  9.732 (min:    41.469 / max:   249.523) us     GPU-0:    48.653 us   +/-  9.826 (min:    44.096 / max:   253.952) us\n",
      "<lambda>            :    CPU: 10812.485 us   +/- 782.521 (min:  9434.302 / max: 12527.588) us     GPU-0: 10819.399 us   +/- 782.586 (min:  9441.248 / max: 12535.136) us\n",
      "asnumpy             :    CPU: 11146.435 us   +/- 742.569 (min:  9433.901 / max: 14843.432) us     GPU-0: 11152.253 us   +/- 742.669 (min:  9439.552 / max: 14849.664) us\n"
     ]
    }
   ],
   "source": [
    "# From Numba-CUDA to CuPy\n",
    "print(benchmark(cp.asarray, (c,), n_repeat=1000))\n",
    "\n",
    "# From CuPy to Numba-CUDA\n",
    "print(benchmark(cuda.to_device, (ccp,), n_repeat=1000))\n",
    "\n",
    "# From Numba-CUDA to Host\n",
    "print(benchmark(lambda x : x.copy_to_host(), (c,), n_repeat=100))\n",
    "\n",
    "# From CuPy to host\n",
    "print(benchmark(cp.asnumpy, (ccp,), n_repeat=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde1f83-5f46-4424-9846-c0e69d981105",
   "metadata": {},
   "source": [
    "As you can see, moving memory between host and device is much more expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34558d1-52e4-4fa4-aeab-9bc677d70acc",
   "metadata": {},
   "source": [
    "## Question 1 - Local reductions and dynamic shared memory\n",
    "\n",
    "Consider the following CUDA snippet that implements a local reduction operation:\n",
    "\n",
    "```C++\n",
    "__global__ void reduction(float *g_odata, float *g_idata)\n",
    "{\n",
    "    // dynamically allocated shared memory\n",
    "    extern  __shared__  float temp[];\n",
    "\n",
    "    int tid = threadIdx.x;\n",
    "\n",
    "    // first, each thread loads data into shared memory\n",
    "    temp[tid] = g_idata[tid];\n",
    "\n",
    "    // next, we perform binary tree reduction\n",
    "    for (int d=blockDim.x/2; d>0; d=d/2) {\n",
    "      __syncthreads();  // ensure previous step completed \n",
    "      if (tid<d)\n",
    "          temp[tid] += temp[tid+d];\n",
    "    }\n",
    "\n",
    "    // finally, first thread puts result into global memory\n",
    "    if (tid == 0)\n",
    "        g_odata[0] = temp[0];\n",
    "}\n",
    "```\n",
    "\n",
    "Note that this uses syncthreads and dynamic shared memory. First, read the code above and understand what it does (it computes a sum of all entries in `g_idata` and stores the output in `g_odata`).\n",
    "\n",
    "* Why was syncthreads used?\n",
    "* This looks like a convoluted approach to perform a sum. Can you think about why something like this would be needed?\n",
    "\n",
    "By looking at the above tutorial and at the lecture slides convert this code into Python by using Numba-CUDA and call it by inserting it into the Python script provided below, which you will also have to modify (follow the code comments).\n",
    "\n",
    "**Hint:** Note that this example uses dynamic shared memory! The [Numba docs](https://numba.readthedocs.io/en/stable/cuda/memory.html) and the course slides may be helpful.\n",
    "\n",
    "**Hint:** In Python you will have to use the following while loop in place of a foor loop:\n",
    "```python\n",
    "d = cuda.blockDim.x//2 # You need integer division here\n",
    "while d > 0:\n",
    "    # loop body here\n",
    "    d = d//2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fbf6619-4f34-451d-914c-373b970c6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script true\n",
    "# NOTE: remove the line above else the cell won't run\n",
    "\n",
    "import cupy as cp\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "num_blocks = 1\n",
    "num_threads = 512\n",
    "num_elements = num_blocks*num_threads\n",
    "\n",
    "# NOTE: Allocated on the host\n",
    "h_idata = 10.*np.random.rand(num_elements)\n",
    "h_idata = h_idata.astype(np.float32) # input data\n",
    "ex_sum = h_idata.sum() # exact sum computed by host\n",
    "\n",
    "d_idata = None # FIX THIS. Move h_idata onto the device\n",
    "d_odata = None # Fix this. Initialise an empty array with a single float32 entry on the device to hold the output\n",
    "\n",
    "# Modify this and JIT-it with Numba\n",
    "def reduction(g_odata, g_idata):\n",
    "    raise NotImplementedError\n",
    "\n",
    "shared_memory_size = None # MODIFY THIS. How big should this be? Remember: here you need memory in bytes, not array entries!!!\n",
    "reduction[FIXME](d_odata, d_idata) # MODIFY THIS by replacing FIXME with the correct kernel parameters.\n",
    "\n",
    "computed_sum = None # Get d_odata back to the host. Its first entry is the computed sum.\n",
    "\n",
    "print(\"Reduction error: %.3e\" % abs(computed_sum[0] - ex_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5dc220-f8c2-49eb-bfe2-8c639ee133c6",
   "metadata": {},
   "source": [
    "## Solution to question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a15e5fe-8102-43dd-9bbf-543ebb8f7d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction error: 0.000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/cupy/lib/python3.12/site-packages/numba_cuda/numba/cuda/dispatcher.py:663: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "num_blocks = 1\n",
    "num_threads = 512\n",
    "num_elements = num_blocks*num_threads\n",
    "\n",
    "# NOTE: Allocated on the host\n",
    "h_idata = 10.*np.random.rand(num_elements)\n",
    "h_idata = h_idata.astype(np.float32) # input data\n",
    "ex_sum = h_idata.sum() # exact sum computed by host\n",
    "\n",
    "d_idata = cuda.to_device(h_idata) # FIX THIS. Move h_idata onto the device\n",
    "cp_odata = cp.empty((1,), dtype=np.float32)\n",
    "d_odata = cuda.to_device(cp_odata) # Fix this. Initialise an empty array with a single float32 entry on the device to hold the output\n",
    "\n",
    "# Modify this and JIT-it with Numba\n",
    "@cuda.jit\n",
    "def reduction(g_odata, g_idata):\n",
    "    temp = cuda.shared.array(0, dtype=np.float32)\n",
    "\n",
    "    tid = cuda.threadIdx.x\n",
    "\n",
    "    # first, each thread loads data into shared memory\n",
    "    temp[tid] = g_idata[tid]\n",
    "\n",
    "    # next, we perform binary tree reduction\n",
    "    d = cuda.blockDim.x//2 # You need integer division here\n",
    "    while d > 0:\n",
    "        cuda.syncthreads() # ensure previous step completed\n",
    "        if tid < d:\n",
    "            temp[tid] += temp[tid+d]\n",
    "            \n",
    "        d = d//2\n",
    "\n",
    "    # finally, first thread puts result into global memory\n",
    "    if tid == 0:\n",
    "        g_odata[0] = temp[0]\n",
    "        \n",
    "shared_memory_size = np.dtype(np.float32).itemsize*num_threads # MODIFY THIS. How big should this be?\n",
    "reduction[num_blocks, num_threads, 0, shared_memory_size](d_odata, d_idata) # MODIFY THIS by replacing FIXME with the correct kernel parameters.\n",
    "\n",
    "computed_sum = d_odata.copy_to_host() # Get d_odata back to the host. Its first entry is the computed sum.\n",
    "\n",
    "print(\"Reduction error: %.3e\" % abs(computed_sum[0] - ex_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5e890-f7a7-4e72-ad40-3a13d0396ee2",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "The code from Question 1 currently assumes the number of threads is a power of 2.\n",
    "Extend it to handle the general case by finding the largest power of 2 less than\n",
    "`blockSize`, and adding the elements beyond that point to the corresponding\n",
    "first set of elements of that size. Test it with 192 threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf41ec2-4d3b-40a8-9b33-d31d8ab09ed7",
   "metadata": {},
   "source": [
    "## Solution to Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d1cab9-0ad9-454f-ba95-5df2da47a323",
   "metadata": {},
   "source": [
    "## Question 3 - Constant memory, static shared memory, device functions\n",
    "\n",
    "This question is a bit silly, but is a good way to try different things. Modify the code from Question 1 in three ways:\n",
    "\n",
    "1- Using static memory.\n",
    "\n",
    "2- Saving the number `2` used in the loop as a constant variable using `cuda.const.array_like`.\n",
    "\n",
    "3- Defining a [device function](https://numba.readthedocs.io/en/stable/cuda/device-functions.html) that computes $d=d//2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92931542-9cf8-4050-a4d3-b6005f6fa722",
   "metadata": {},
   "source": [
    "## Solution to question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86848d4d-852e-4e21-8395-18065e0c9fa5",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "The code from Question 1 currently performs the reduction operation for a single thread block.\n",
    "Modify the code to perform reduction using multiple blocks (say $4$ of them) with each block working with a different section of the input array.\n",
    "\n",
    "There are two ways in which the partial sums from each block can be summed:\n",
    "* Each block puts its partial sum into a different element of the output\n",
    "array, and then these are transferred to the host and summed there;\n",
    "* An atomic addition is used to safely increment a single global sum.\n",
    "\n",
    "Implement both and check that you get the correct answer.\n",
    "\n",
    "Finally, implement the reduction using:\n",
    "* The `cuda.reduce` decorator.\n",
    "* `cupy.sum`.\n",
    "\n",
    "Time all four versions. Which one is faster? Can you see why it is much easier to not write reductions yourself?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3731e25-0382-4456-815d-3f0bbbec9f97",
   "metadata": {},
   "source": [
    "## Solution to question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea0bffb-42e5-4f56-85a6-d487259062ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuPy Kernel",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
