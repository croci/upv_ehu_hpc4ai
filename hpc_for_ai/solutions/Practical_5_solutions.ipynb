{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab2b62f-c9f3-44a6-911c-d5fdbda375f9",
   "metadata": {},
   "source": [
    "High-performance and parallel computing for AI - Practical 5: Reduced- and mixed-precision computing\n",
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c481355-b47f-47b0-83f4-c7322b8a7129",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "=========\n",
    "\n",
    "For these practicals we will be using a different `conda environment`. When opening a notebook or a terminal make sure you are using the **CuPy Kernel**!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df621af-a236-43c8-bd06-cb3de61488cc",
   "metadata": {},
   "source": [
    "Question 1\n",
    "----------\n",
    "\n",
    "The `cast` function provided below takes an `LPV` array and changes its format to a new format specified by an `Option` `op`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30adfa0-0ea8-44a6-ab5f-e7c0e6eceb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589793 3.1415927410125732 3.140625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chopping import *\n",
    "\n",
    "def cast(a, op):\n",
    "    return LPV(a.array(), op)\n",
    "\n",
    "opb = Option(True)\n",
    "opb.set_format('b') # bfloat16 half precision\n",
    "\n",
    "op = Option(True)\n",
    "op.set_format('s') # single precision\n",
    "\n",
    "a = np.array([np.pi])\n",
    "aa = LPV(a, op) # single precision\n",
    "ab = cast(aa, opb) # bfloat16 half precision. Note: this will round the array from single to bfloat16.\n",
    "\n",
    "print(a[0], aa.array()[0], ab.array()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66fc096-21d3-4e9c-b444-b658ac68db74",
   "metadata": {},
   "source": [
    "Use the cast function to define a new function `c = mpp(a,b)` which takes two half-precision `LPV` variables `a` and `b` as inputs and computes their dot product using single precision, returning a single-precision variable `c` as output.\n",
    "\n",
    "Let $x,a,b\\in\\mathbb{R}^n$ be given in numpy by `x = np.linspace(0,1,n)`, `a = np.exp(x)`, `b = np.sin(np.pi*x)**2`. Compute the dot product $a\\cdot b$ using only half precision and using the `mpp` function you just wrote. By evaluating the exact dot product in double precision, compute the resulting relative rounding error for both approaches. Plot (in loglog scale) the error as a function of $n$. Are the results consistent with what we have learnt in the lectures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72605479-36cf-49b0-8273-5691e02846f8",
   "metadata": {},
   "source": [
    "Solution to Question 1\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "449d9d2d-33d7-4956-a255-687927e24faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAG1CAYAAADpzbD2AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAARoJJREFUeJzt3X1cVHXe//H3cA8KoqHcKELlTWAJiUpuW3lDq25rars92i4rULNfV2QWWZeum1ptsetV5rZStqW56tWle5Pda165uHbjqkBgeYcW4j03KSAooMz5/UFMTsjIyAwzI6/n4zGPw/meM+d8Bud4PnzP53yPyTAMQwAAALggL1cHAAAA4M5IlgAAAGwgWQIAALCBZAkAAMAGkiUAAAAbSJYAAABsIFkCAACwgWQJAADABh9XB+DuzGazjh49quDgYJlMJleHAwAAWsEwDJ06dUpRUVHy8mpb3xDJ0kUcPXpU0dHRrg4DAABcgkOHDqlXr15t2gbJ0kUEBwdLavxlh4SEuDgaAADQGlVVVYqOjracx9uCZOkimi69hYSEkCwBAOBhHFFCQ4E3AACADSRLAAAANnAZzgHMZrPq6+tdHQZgF19fX3l7e7s6DABweyRLbVRfX6+ioiKZzWZXhwLYLTQ0VBEREQyLAQA2kCy1gWEYOnbsmLy9vRUdHd3mcRyA9mIYhk6fPq3S0lJJUmRkpIsjAgD31SGSpQ8++ECPP/64zGaz/uu//kv333+/Q7Z77tw5nT59WlFRUQoKCnLINoH2EhgYKEkqLS1Vjx49uCQHAC247JOlc+fOKSMjQ9nZ2erSpYuSkpI0ceJEXXHFFW3edkNDgyTJz8+vzdsCXKEpyT979izJEgC04LK/brRt2zYNGDBAPXv2VOfOnTV27Fht2LDBofug3gOeiu8uAFyc2ydLmzdv1rhx4xQVFSWTyaR33nmn2TpZWVmKjY1VQECAkpOTtW3bNsuyo0ePqmfPnpb5nj176siRI+0ROgAAuAy4fbJUU1OjhIQEZWVlXXD5mjVrlJGRoXnz5ikvL08JCQkaPXq0pXAVAAC4qZwcaeTIxqkbc/tkaezYsfrd736niRMnXnD5woULNW3aNE2ePFnx8fFasmSJgoKCtGzZMklSVFSUVU/SkSNHFBUV1eL+6urqVFVVZfVC62zatEkmk0kVFRUujWP+/PlKTEx0aQwtGT58uB599FGHrwsAHmnFCik7W1q50tWR2OT2yZIt9fX1ys3NVUpKiqXNy8tLKSkp2rJliyRp6NCh+vrrr3XkyBFVV1dr3bp1Gj16dIvbzMzMVJcuXSyv6Ohop38OSe2aXaelpclkMslkMsnX11dXXnmlnnzySdXW1jp93x3d22+/rWeffdbh6wKAxygulnJzpbw8ac2axrbVqxvnc3Mbl7sZj06WysvL1dDQoPDwcKv28PBwHT9+XJLk4+OjF198USNGjFBiYqIef/xxm3fCzZ49W5WVlZbXoUOHnPoZLNo5ux4zZoyOHTumb7/9Vi+99JJee+01zZs3r1327YkaGhocMvBot27dWv0EbHvWBeChPOQylEPFxkqDB0tJSVJZWWNbWVnj/ODBjcvdjEcnS611++23q7CwUPv379cDDzxgc11/f3+FhIRYvZzGhdm1v7+/IiIiFB0drQkTJiglJUX/93//Z1luNpuVmZmpK6+8UoGBgUpISNDf//53q2189NFH6tevnwIDAzVixAgdOHDAavmFLoctWrRIsT86EJYtW6YBAwbI399fkZGRevjhhy3LKioqdP/996t79+4KCQnRyJEjVVBQYPX+3//+9woPD1dwcLCmTp160R6ypsuFH374oQYOHKiAgADdcMMN+vrrry3rLF++XKGhoXrvvfcUHx8vf39/HTx4UHV1dZo5c6Z69uypTp06KTk5WZs2bbLa/ueff67hw4crKChIXbt21ejRo3Xy5ElJzS+tvfLKK+rbt68CAgIUHh6uX/3qV5ZlP1735MmTuu+++9S1a1cFBQVp7Nix2rdvX7OYP/74Y8XFxalz586WpBiAm/KQy1AOtWqV5PP9yEWGYT318Wlc7mY8OlkKCwuTt7e3SkpKrNpLSkoUERHRpm1nZWUpPj5eQ4YMadN2bHKT7Prrr7/WF198YTVeVGZmplasWKElS5Zo586deuyxx3TPPffoX//6lyTp0KFDuuOOOzRu3Djl5+fr/vvv16xZs+ze96uvvqr09HQ98MAD+uqrr/Tee++pT58+luV33nmnSktLtW7dOuXm5mrQoEEaNWqUTpw4IUn661//qvnz5+v5559XTk6OIiMj9corr7Rq30888YRefPFFbd++Xd27d9e4ceN09uxZy/LTp0/rD3/4g9544w3t3LlTPXr00MMPP6wtW7Zo9erV2rFjh+68806NGTPGkrTk5+dr1KhRio+P15YtW/TZZ59p3LhxljG5zpeTk6NHHnlEzzzzjPbu3av169fr5ptvbjHetLQ05eTk6L333tOWLVtkGIZ+/vOfN4v5hRde0MqVK7V582YdPHhQM2fObNXvA0A78cDLUA41aZK0deuFl23d2rjc3RgeRJKxdu1aq7ahQ4caDz/8sGW+oaHB6Nmzp5GZmemQfVZWVhqSjMrKymbLzpw5Y+zatcs4c+bMpW181SrD8PExjMac2vrl49O43AlSU1MNb29vo1OnToa/v78hyfDy8jL+/ve/G4ZhGLW1tUZQUJDxxRdfWL1v6tSpxt13320YhmHMnj3biI+Pt1r+X//1X4Yk4+TJk4ZhGMa8efOMhIQEq3VeeuklIyYmxjIfFRVlzJkz54Jxfvrpp0ZISIhRW1tr1X711Vcbr732mmEYhjFs2DDjoYceslqenJzcbL/ny87ONiQZq1evtrR99913RmBgoLFmzRrDMAzjzTffNCQZ+fn5lnWKi4sNb29v48iRI1bbGzVqlDF79mzDMAzj7rvvNm688cYW933LLbcYM2bMMAzDMP7xj38YISEhRlVV1UXXLSwsNCQZn3/+uWV5eXm5ERgYaPz1r3+1inn//v2WdbKysozw8PAW42nzdxiA/c7/v95ksp42vS53ubmNn9PLy3qam+uwXdg6f9vL7Ufwrq6u1v79+y3zRUVFys/PV7du3dS7d29lZGQoNTVVgwcP1tChQ7Vo0SLV1NRo8uTJLoy6lSZNkuLiGnuSfmzrVmnQIKftesSIEXr11VdVU1Ojl156ST4+PvrlL38pSdq/f79Onz6tW2+91eo99fX1uv766yVJu3fvVnJystXyYcOG2RVDaWmpjh49qlGjRl1weUFBgaqrq5vVmJ05c0bffPONJY4HH3ywWRzZ2dkX3f/58Xbr1k39+/fX7t27LW1+fn4aOHCgZf6rr75SQ0OD+vXrZ7Wduro6S4z5+fm68847L7pvSbr11lsVExOjq666SmPGjNGYMWM0ceLECz46Z/fu3fLx8bH6nV9xxRXNYg4KCtLVV19tmY+MjGQYDcDdrFolpaVJ585d+DLU8uWuiqz99OghRURI0dHS1KnS0qXSoUON7W7I7ZOlnJwcjRgxwjKfkZEhSUpNTdXy5ct11113qaysTHPnztXx48eVmJio9evXNyv6dnteXpLZ/MPUyTp16mS53LVs2TIlJCRo6dKlmjp1qqqrqyVJH374odWAnlJjrVNreXl5yWj6D+B7518yano2WUuqq6sVGRnZrCZIkkJDQ1sdx6UKDAy0GuG6urpa3t7eys3NbfZokM6dO1ve01rBwcHKy8vTpk2btGHDBs2dO1fz58/X9u3bL/nz+fr6Ws2bTKZm/wYAXMyFfyi7jV69pAMHJD8/yWSSHnhAqq+X7DjHtCe3r1kaPny4DMNo9lp+Xub98MMPq7i4WHV1ddq6dWuzHo9L0S41S9IP2XVSkrRkSeM0IqJds2svLy/95je/0W9/+1udOXPGqqC5T58+Vq+moRTi4uKsRkqXpH//+99W8927d9fx48etTtb5+fmWn4ODgxUbG6uNGzdeMK5Bgwbp+PHj8vHxaRZHWFiYJY6tP7r2/eM4WnL+eidPnlRhYaHi4uJaXP/6669XQ0ODSktLm8XTVCM3cODAFj/Phfj4+CglJUULFizQjh07dODAAf3zn/9stl5cXJzOnTtn9Vm/++477d27V/Hx8a3eH+BWOuKdYD/m5WU97Uj8/RsTJalx6qaJkuQByZKrpKena9euXdq+fbtzd9SUXW/dKv2//9c4PXCgsb0d3XnnnfL29lZWVpaCg4M1c+ZMPfbYY/rLX/6ib775Rnl5efrTn/6kv/zlL5KkBx98UPv27dMTTzyhvXv36q233rJKYKXGRLesrEwLFizQN998o6ysLK1bt85qnfnz5+vFF1/Uyy+/rH379ln2I0kpKSkaNmyYJkyYoA0bNujAgQP64osvNGfOHOV8/5/rjBkztGzZMr355psqLCzUvHnztHPnzlZ95meeeUYbN27U119/rbS0NIWFhWnChAktrt+vXz9NmjRJ9913n95++20VFRVp27ZtyszM1IcffiipceiJ7du366GHHtKOHTu0Z88evfrqqyovL2+2vQ8++EAvv/yy8vPzVVxcrBUrVshsNqt///7N1u3bt6/Gjx+vadOm6bPPPlNBQYHuuece9ezZU+PHj2/V5wXcTke8E6yJG/yhDDu0uerpMufUAm8XSU1NNcaPH9+sPTMz0+jevbtRXV1tmM1mY9GiRUb//v0NX19fo3v37sbo0aONf/3rX5b133//faNPnz6Gv7+/cdNNNxnLli2zKvA2DMN49dVXjejoaKNTp07GfffdZzz33HNWBd6GYRhLliyx7CcyMtKYPn26ZVlVVZUxffp0IyoqyvD19TWio6ONSZMmGQcPHrSs89xzzxlhYWFG586djdTUVOPJJ59sVYH3+++/bwwYMMDw8/Mzhg4dahQUFFjWefPNN40uXbo0e299fb0xd+5cIzY21hLvxIkTjR07dljW2bRpk/GTn/zE8Pf3N0JDQ43Ro0dbfifnF21/+umnxi233GJ07drVCAwMNAYOHGgpMP/xuoZhGCdOnDDuvfdeo0uXLkZgYKAxevRoo7Cw0GbMa9euNWwd5p76HYYHO3DAMHJyGgt5e/RoLOrt0aNxPiencXlHUVtrGGZz489mc+M8HMaRBd4mw6CgwZaqqip16dJFlZWVzcZcqq2tVVFRka688koFBAS4KELYa9OmTRoxYoROnjzZLrVP7ozvMNrdeXWAMpkaC5ubpk04LcEBbJ2/7cVluBa0W80SAHQkHjggIUCy1IJ2q1kCgI7EEwckRIfn9kMHAI7WdIclABdr5yFTgEtFzxIAoH1xJxg8DD1LAID25WEDEgL0LLWAAm8AcCIPGpAQIFlqAQXeAABAIlkCAACwiWQJFzV8+HA9+uijTt3H/PnzlZiY6NR9XCp7Pn97/K4AAO2LZKkDSktLk8lk0oMPPthsWXp6ukwmk9LS0ixtb7/9tp599tl2jNC92PP5O/rvCgAuRyRLLWjvAu+cqiqNzM9XTlVVu+wvOjpaq1ev1pkzZyxttbW1euutt9S7d2+rdbt166bg4OB2icuRGhoaZHbA2C32fH5P/V0BAFpGstSC9i7wXlFSouyKCq0sKWmX/Q0aNEjR0dF6++23LW1vv/22evfureuvv95q3fMvLe3Zs0dBQUF66623LMv/+te/KjAwULt27ZIkVVRU6P7771f37t0VEhKikSNHqqCgwGqbv//97xUeHq7g4GBNnTpVtbW1NuPdtGmTTCaTPvzwQw0cOFABAQG64YYb9PXXX1vWWb58uUJDQ/Xee+8pPj5e/v7+OnjwoOrq6jRz5kz17NlTnTp1UnJysjZt2mS1/c8//1zDhw9XUFCQunbtqtGjR+vkyZPNPr8kvfLKK+rbt68CAgIUHh6uX/3qVxf8XUnSyZMndd9996lr164KCgrS2LFjtW/fvmYxf/zxx4qLi1Pnzp01ZswYHTt2zObvAwDQfkiWXKi4tla5p04p79QprSktlSStLi1V3qlTyj11SsUXSSDaasqUKXrzzTct88uWLdPkyZNtvueaa67RCy+8oIceekgHDx7U4cOH9eCDD+oPf/iD4uPjJUl33nmnSktLtW7dOuXm5mrQoEEaNWqUTpw4IakxuZo/f76ef/555eTkKDIyUq+88kqrYn7iiSf04osvavv27erevbvGjRuns2fPWpafPn1af/jDH/TGG29o586d6tGjhx5++GFt2bJFq1ev1o4dO3TnnXdqzJgxlqQlPz9fo0aNUnx8vLZs2aLPPvtM48aNU0NDQ7P95+Tk6JFHHtEzzzyjvXv3av369br55ptbjDctLU05OTl67733tGXLFhmGoZ///OfNYn7hhRe0cuVKbd68WQcPHtTMmTNb9fsAALQDAzZVVlYakozKyspmy86cOWPs2rXLOHPmzCVtW9nZlpfpR9OmlzOkpqYa48ePN0pLSw1/f3/jwIEDxoEDB4yAgACjrKzMGD9+vJGammpZ/5ZbbjFmzJhhtY3bbrvNuOmmm4xRo0YZP/vZzwyz2WwYhmF8+umnRkhIiFFbW2u1/tVXX2289tprhmEYxrBhw4yHHnrIanlycrKRkJDQYszZ2dmGJGP16tWWtu+++84IDAw01qxZYxiGYbz55puGJCM/P9+yTnFxseHt7W0cOXLEanujRo0yZs+ebRiGYdx9993GjTfe2OK+z//8//jHP4yQkBCjqqrqousWFhYakozPP//csry8vNwIDAw0/vrXv1rFvH//fss6WVlZRnh4eIvxOFJbv8Noo+3bDWPEiMYpAIeydf62FyN4u9CquDil7dmjc4ahpieVNU19TCYtv+Yap+6/e/fuuu2227R8+XIZhqHbbrtNYWFhrXrvsmXL1K9fP3l5eWnnzp0yfT+4XEFBgaqrq3XFFVdYrX/mzBl98803kqTdu3c3Ky4fNmyYsrOzL7rfYcOGWX7u1q2b+vfvr927d1va/Pz8NHDgQMv8V199pYaGBvXr189qO3V1dZYY8/Pzdeedd7bmY+vWW29VTEyMrrrqKo0ZM0ZjxozRxIkTFRQU1Gzd3bt3y8fHR8nJyZa2K664olnMQUFBuvrqqy3zkZGRKv2+pxGXuRUrpOxsaeVKafBgV0cDoAUkSy40KTxccUFBSsrNbbZs66BBGtQOhcJTpkzRww8/LKmxqL21CgoKVFNTIy8vLx07dkyRkZGSpOrqakVGRjarCZKk0NBQR4RsU2BgoCVxa4rH29tbubm58vb2tlq3c+fOlve0VnBwsPLy8rRp0yZt2LBBc+fO1fz587V9+/ZL/ny+vr5W8yaTiQf9Xs6Ki6Xy8sZRq9esaWxbvVpKTZUMQwoLk2JiXBsjACvULLkJrx9N28uYMWNUX1+vs2fPavTo0a16z4kTJ5SWlqY5c+YoLS1NkyZNstxVN2jQIB0/flw+Pj7q06eP1aup1youLk5bt2612ua///3vVu37/PVOnjypwsJCxcXFtbj+9ddfr4aGBpWWljaLJyIiQpI0cOBAbdy4sVX7lyQfHx+lpKRowYIF2rFjhw4cOKB//vOfzdaLi4vTuXPnrD7rd999p71791rqu9ABxcY29iIlJUllZY1tZWWN84MHNy4H4FZIllrQXkMH9PD1VYSvr5KCg7WkXz8lBQcrwtdXPX7U2+As3t7e2r17t3bt2tWs56UlDz74oKKjo/Xb3/5WCxcuVENDg6UgOSUlRcOGDdOECRO0YcMGHThwQF988YXmzJmjnJwcSdKMGTO0bNkyvfnmmyosLNS8efO0c+fOVu37mWee0caNG/X1118rLS1NYWFhmjBhQovr9+vXT5MmTdJ9992nt99+W0VFRdq2bZsyMzP14YcfSpJmz56t7du366GHHtKOHTu0Z88evfrqqyovL2+2vQ8++EAvv/yy8vPzVVxcrBUrVshsNqt///7N1u3bt6/Gjx+vadOm6bPPPlNBQYHuuece9ezZU+PHj2/V58VlaNUqyef7Tv2mHsSmqY9P43IAboXLcC1IT09Xenq6qqqq1KVLF6ftp1dAgA4MGyY/k0kmk0kPREaq3jDk79V+eWxISEir112xYoU++ugjffnll/Lx8ZGPj49WrVqln/70p/rFL36hsWPH6qOPPtKcOXM0efJklZWVKSIiQjfffLPCw8MlSXfddZe++eYbPfnkk6qtrdUvf/lL/ed//qc+/vjji+7/97//vWbMmKF9+/YpMTFR77//vvz8/Gy+580339Tvfvc7Pf744zpy5IjCwsJ0ww036Be/+IWkxoRqw4YN+s1vfqOhQ4cqMDBQycnJuvvuu5ttKzQ0VG+//bbmz5+v2tpa9e3bV//7v/+rAQMGtLjvGTNm6Be/+IXq6+t1880366OPPmp26Q0dyKRJUlxcY0/Sj23dKg0a1P4xAbDJZFAcYVNTslRZWdksqaitrVVRUZGuvPJKBQQEuCjCjmHTpk0aMWKETp482S61Tx0F32EXyctrTJa8vCSz+Ydpbi7JEuAgts7f9uIyHAC0tx49pIiIxoRpyZLGaUREYzsAt8NlOABob716SQcOSH5+jXfFPfCAVF8v+fu7OjIAF0CyBI8wfPhwbqfH5eX8xMhkIlEC3BiX4QAAAGwgWQIAALCBZMkBuDwET8V3FwAujmSpBa0ZlLJpEMf6+vr2CgtwqNOnT0tq/sgVAMAPGGfpImyN02AYhg4ePKizZ88qKipKXu04kCTQFoZh6PTp0yotLVVoaKjl2X4AcLlw5DhL3A3XBiaTSZGRkSoqKlJxcbGrwwHsFhoaanlGHgDgwkiW2sjPz099+/blUhw8jq+vb6ufBwgAHRnJkgN4eXnxqAgAAC5TFNkAAADYQLIEAABgA8kSAACADSRLAAAANpAsAQAA2ECyBAAAYAPJEgAAgA0kSy1ozbPhAADA5Y9nw12EI58tAwAA2ocjz9/0LAEAANhAsgQAAGADyRIAAIANJEsAAAA2kCwBAADYQLIEAABgA8kSAACADSRLAAAANpAsAQAA2ECyBAAAYAPJEgAAgA0kSwAAADaQLAEAANhAsgQAAGBDh0iWJk6cqK5du+pXv/qVq0MBAAAepkMkSzNmzNCKFStcHQYAAPBAHSJZGj58uIKDg10dBgAA8EAuT5Y2b96scePGKSoqSiaTSe+8806zdbKyshQbG6uAgAAlJydr27Zt7R8oAADokHxcHUBNTY0SEhI0ZcoU3XHHHc2Wr1mzRhkZGVqyZImSk5O1aNEijR49Wnv37lWPHj0kSYmJiTp37lyz927YsEFRUVF2xVNXV6e6ujrLfFVVlZ2fCAAAXE5cniyNHTtWY8eObXH5woULNW3aNE2ePFmStGTJEn344YdatmyZZs2aJUnKz893WDyZmZl6+umnHbY9AADg2Vx+Gc6W+vp65ebmKiUlxdLm5eWllJQUbdmyxSn7nD17tiorKy2vQ4cOOWU/AADAM7i8Z8mW8vJyNTQ0KDw83Ko9PDxce/bsafV2UlJSVFBQoJqaGvXq1Ut/+9vfNGzYsAuu6+/vL39//zbFDQAALh9unSw5yieffGL3e7KyspSVlaWGhgYnRAQAADyFW1+GCwsLk7e3t0pKSqzaS0pKFBER4dR9p6ena9euXdq+fbtT9wMAANybWydLfn5+SkpK0saNGy1tZrNZGzdubPEyGgAPkpMjjRzZOAUAN+Xyy3DV1dXav3+/Zb6oqEj5+fnq1q2bevfurYyMDKWmpmrw4MEaOnSoFi1apJqaGsvdcQA82IoVUna2tHKlNHiwq6MBgAtyebKUk5OjESNGWOYzMjIkSampqVq+fLnuuusulZWVae7cuTp+/LgSExO1fv36ZkXfjkbNEuAkxcVSeblkMklr1jS2rV4tpaZKhiGFhUkxMa6NEQDOYzIMw3B1EO6sqqpKXbp0UWVlpUJCQlwdDuD5TCbrnw3jh2kT/lsC0EaOPH+7dc0SgMvQqlWSz/ed2k1JUdPUx6dxOQC4EZdfhgPQwUyaJMXFSUlJzZdt3SoNGtT+MQGADfQstSArK0vx8fEaMmSIq0MBLl9eXtZTAHBD1CxdBDVLgBMcPiwNGSJFR0tTp0pLl0qHDknbt0u9erk6OgCXAUeev7kMB6D99eolHTgg+fk1Fnc/8IBUXy/xqCEAbohkCYBrnJ8YmUwkSgDcFoUCAAAANpAstYACbwAAIFHgfVEUeAMA4HkYlBIAAKCdkCwBAADYQLIEuEpOjjRyZOMUAOC2SJZaQIE3nG7FCik7W1q50tWRAABsoMD7IijwhkMVF0vl5Y3jCo0dK5WWSj16SOvWNT5MNixMiolxdZQA4PFcNoL32bNnFRgYqPz8fF177bVt2jHQIcXG/vCzydQ4LSuzfqgsf78AgFux6zKcr6+vevfurYaGBmfFA1zeVq2SfL7/G6UpKWqa+vg0LgcAuBW7a5bmzJmj3/zmNzpx4oQz4gEub5MmSVu3XnjZ1q2NywEAbsXuZ8MtXrxY+/fvV1RUlGJiYtSpUyer5Xl5eQ4LDriseXlJZvMPUwCAW7I7WZowYYITwgA6kB49pIgIKTpamjpVWrpUOnSosR0A4Ha4G64FWVlZysrKUkNDgwoLC7kbDo5VVyf5+TUWeRuGVF8v+fu7OioAuGw48m64S06WcnNztXv3bknSgAEDdP3117cpEHfF0AEAAHgelw0dIEmlpaX69a9/rU2bNik0NFSSVFFRoREjRmj16tXq3r17mwICAABwJ3bfDTd9+nSdOnVKO3fu1IkTJ3TixAl9/fXXqqqq0iOPPOKMGAEAAFzG7stwXbp00SeffNLsMSDbtm3Tz372M1VUVDgyPpfjMhwAAJ7Hkedvu3uWzGazfH19m7X7+vrKzO3PAADgMmN3sjRy5EjNmDFDR48etbQdOXJEjz32mEaNGuXQ4AAAAFzN7mRp8eLFqqqqUmxsrK6++mpdffXVuvLKK1VVVaU//elPzogRAADAZey+Gy46Olp5eXn65JNPtGfPHklSXFycUlJSHB4cAACAq9mVLJ09e1aBgYHKz8/XrbfeqltvvdVZcbnc+YNSAgCAjsuuy3C+vr7q3bt3h0gg0tPTtWvXLm3fvt3VoQAAABeyu2Zpzpw5+s1vfqMTJ044Ix4AAAC3YnfN0uLFi7V//35FRUUpJiZGnTp1slqel5fnsOAAAABcze5kacKECU4IAwAAwD3ZlSydO3dOJpNJU6ZMUa9evZwVEwAAgNuwq2bJx8dH//3f/61z5845Kx4AAAC3ckkjeP/rX/9yRiwAAABux+6apbFjx2rWrFn66quvlJSU1KzA+/bbb3dYcAAAAK5mMgzDsOcNXl4td0aZTKbLbgwmRz61GAAAtA9Hnr/t7lkym81t2iEAAIAnsbtm6Xy1tbWOigMAAMAt2Z0sNTQ06Nlnn1XPnj3VuXNnffvtt5Kkp556SkuXLnV4gK6SlZWl+Ph4DRkyxNWhAAAAF7I7WXruuee0fPlyLViwQH5+fpb2a6+9Vm+88YZDg3Mlng0HAACkS0iWVqxYoT//+c+aNGmSvL29Le0JCQnas2ePQ4MDAABwNbuTpSNHjqhPnz7N2s1ms86ePeuQoAAAANyF3clSfHy8Pv3002btf//733X99dc7JCgAAAB3YffQAXPnzlVqaqqOHDkis9mst99+W3v37tWKFSv0wQcfOCNGAAAAl7G7Z2n8+PF6//339cknn6hTp06aO3eudu/erffff1+33nqrM2IEAABwGbtH8O5oGMEbAADP48jzd5sGpQQAALjckSwBAADYQLIEAABgA8kSAACADSRLAAAANtg9zlJGRsYF200mkwICAtSnTx+NHz9e3bp1a3NwAAAArmb30AEjRoxQXl6eGhoa1L9/f0lSYWGhvL29dc0112jv3r0ymUz67LPPFB8f75Sg2xNDBwAA4HlcOnTA+PHjlZKSoqNHjyo3N1e5ubk6fPiwbr31Vt199906cuSIbr75Zj322GNtCgwAAMAd2N2z1LNnT/3f//1fs16jnTt36mc/+5mOHDmivLw8/exnP1N5eblDg70Uhw4d0r333qvS0lL5+Pjoqaee0p133tnq99OzBACA53Fpz1JlZaVKS0ubtZeVlamqqkqSFBoaqvr6+jYF5ig+Pj5atGiRdu3apQ0bNujRRx9VTU2Nq8MCAAAe4pIuw02ZMkVr167V4cOHdfjwYa1du1ZTp07VhAkTJEnbtm1Tv379HB3rJYmMjFRiYqIkKSIiQmFhYTpx4oRrgwIAAB7D7mTptdde06hRo/TrX/9aMTExiomJ0a9//WuNGjVKS5YskSRdc801euONN1q1vc2bN2vcuHGKioqSyWTSO++802ydrKwsxcbGKiAgQMnJydq2bZu9YUuScnNz1dDQoOjo6Et6PwAA6HjsHjqgc+fOev311/XSSy/p22+/lSRdddVV6ty5s2Wdpp6c1qipqVFCQoKmTJmiO+64o9nyNWvWKCMjQ0uWLFFycrIWLVqk0aNHa+/everRo4dlf+fOnWv23g0bNigqKkqSdOLECd133316/fXX7fm4AACgg7O7wNuZTCaT1q5da7mcJ0nJyckaMmSIFi9eLEkym82Kjo7W9OnTNWvWrFZtt66uTrfeequmTZume++996Lr1tXVWearqqoUHR1NgTcAAB7EpQXeNTU1euqpp/STn/xEffr00VVXXWX1cqT6+nrl5uYqJSXlh4C9vJSSkqItW7a0ahuGYSgtLU0jR468aKIkSZmZmerSpYvlxSU7J8rJkUaObJwCAOCm7L4Md//99+tf//qX7r33XkVGRspkMjkjLklSeXm5GhoaFB4ebtUeHh6uPXv2tGobn3/+udasWaOBAwda6qFWrlyp66677oLrz54922qU8qaeJTjBihVSdra0cqU0eLCrowEA4ILsTpbWrVunDz/8UDfeeKMz4nG4n/70pzKbza1e39/fX/7+/k6MqIMrLpbKyyWTSVqzprFt9WopNVUyDCksTIqJcW2MAACcx+5kqWvXru323LewsDB5e3urpKTEqr2kpEQRERFO3XdWVpaysrLU0NDg1P10OLGxP/zc1CtZViYlJf3Q7j5ldAAA2F+z9Oyzz2ru3Lk6ffq0M+Kx4ufnp6SkJG3cuNHSZjabtXHjRg0bNsyp+05PT9euXbu0fft2p+6nw1m1SvL5PkdvSoqapj4+jcsBAHAjdvcsvfjii/rmm28UHh6u2NhY+fr6Wi3Py8uza3vV1dXav3+/Zb6oqEj5+fnq1q2bevfurYyMDKWmpmrw4MEaOnSoFi1apJqaGk2ePNne0OEOJk2S4uKse5KabN0qDRrU/jEBAGCD3cnS+bf1O0JOTo5GjBhhmW8qrk5NTdXy5ct11113qaysTHPnztXx48eVmJio9evXNyv6djQuw7UDLy/JbP5hCgCAG3KrcZbcEQ/SdYLDh6UhQ6ToaGnqVGnpUunQIWn7dqlXL1dHBwC4DDjy/G13zxLQZr16SQcOSH5+jUXeDzwg1ddL3IUIAHBDrUqWunXrpsLCQoWFhalr1642x1biIbVolfMTI5OJRAkA4LZalSy99NJLCg4OliQtWrTImfG4DWqWAACARM3SRVGzBACA52n3mqWqqqpWb5CEAgAAXE5alSyFhoa2+hlwXLYCAACXk1YlS9nZ2ZafDxw4oFmzZiktLc0yivaWLVv0l7/8RZmZmc6J8nKUkyM9+aS0YAEPkQUAwI21Klm65ZZbLD8/88wzWrhwoe6++25L2+23367rrrtOf/7zn5Wamur4KF3A6QXeK1ZI2dnSypUkSwAAuDG7C7yDgoJUUFCgvn37WrUXFhYqMTGxXZ4Z154cWuBdXCyVlzfeKj92rFRaKvXoIa1b1/h8tLAwKSbGMYEDANCBuXRQyujoaL3++utasGCBVfsbb7yh6OjoNgVz2YuN/eHnphqwsjLr56RxcyIAAG7F7mTppZde0i9/+UutW7dOycnJkqRt27Zp3759+sc//uHwAC8rq1ZJaWnSuXM/JEVNUx8faflyV0UGAABacEnjLB0+fFivvPKK9uzZI0mKi4vTgw8+eFn2LDl8nKW8POuepCa5udKgQW3fPgAAcP2z4Xr16qXnn3++TTvu8Ly8JLP5hykAAHBLl5QsVVRUaNu2bSotLZX5Ryf6++67zyGBuZrT7obr0UOKiJCio6WpU6WlS6VDhxrbAQCA27H7Mtz777+vSZMmqbq6WiEhIVaDVZpMpsvuQbpOedxJXZ3k59dY5G0YUn09D5IFAMCBHHn+9rL3DY8//rimTJmi6upqVVRU6OTJk5bX5ZYoOY2//w93w5lMJEoAALgxu5OlI0eO6JFHHlFQUJAz4gEAAHArdidLo0ePVk5OjjNiAQAAcDt2F3jfdttteuKJJ7Rr1y5dd9118vX1tVp+++23Oyw4AAAAV7O7wNvLq+XOKJPJ5LxnqbWz8++GKywsdGyBNwAAcCpHFnhf0qCUHYlT7oYDAABO5dK74QAAADoSu2uWnnnmGZvL586de8nBAAAAuBu7k6W1a9dazZ89e1ZFRUXy8fHR1VdfTbIEAAAuK3YnS19++WWztqqqKqWlpWnixIkOCQoAAMBdOKRmKSQkRE8//bSeeuopR2wOAADAbTiswLuyslKVlZWO2hwAAIBbsPsy3Msvv2w1bxiGjh07ppUrV2rs2LEOCwwAAMAd2J0svfTSS1bzXl5e6t69u1JTUzV79myHBeZq5w9KCQAAOi4GpbwIBqUEAMDzuM2glIcPH9bhw4fbFAAAAIA7sztZMpvNeuaZZ9SlSxfFxMQoJiZGoaGhevbZZ2U2m50RIwAAgMvYXbM0Z84cLV26VL///e914403SpI+++wzzZ8/X7W1tXruueccHiQAAICr2F2zFBUVpSVLluj222+3an/33Xf10EMP6ciRIw4N0NWoWQIAwPO4tGbpxIkTuuaaa5q1X3PNNTpx4kSbggEAAHA3didLCQkJWrx4cbP2xYsXKyEhwSFBAQAAuAu7a5YWLFig2267TZ988omGDRsmSdqyZYsOHTqkjz76yOEBAgAAuJLdPUu33HKLCgsLNXHiRFVUVKiiokJ33HGH9u7dq5tuuskZMQIAALiMXT1LZ8+e1ZgxY7RkyRLuegMAAB2CXT1Lvr6+2rFjh7NiAQAAcDt2X4a75557tHTpUmfEAgAA4HbsLvA+d+6cli1bpk8++URJSUnq1KmT1fKFCxc6LDhX4kG6AABAuoRBKUeMGNHyxkwm/fOf/2xzUO6EQSkBAPA8jjx/292zlJ2d3aYdAgAAeBK7a5YAAAA6EpIlAAAAG0iWAAAAbCBZAgAAsIFkCQAAwIZLSpZWrlypG2+8UVFRUSouLpYkLVq0SO+++65DgwMAAHA1u5OlV199VRkZGfr5z3+uiooKy6CNoaGhWrRokaPjAwAAcCm7k6U//elPev311zVnzhx5e3tb2gcPHqyvvvrKocEBAAC4mt3JUlFRka6//vpm7f7+/qqpqXFIUAAAAO7C7mTpyiuvVH5+frP29evXKy4uzhExAQAAuA27H3eSkZGh9PR01dbWyjAMbdu2Tf/7v/+rzMxMvfHGG86IEQAAwGXsTpbuv/9+BQYG6re//a1Onz6t//iP/1BUVJT++Mc/6te//rUzYgQAAHAZk2EYxqW++fTp06qurlaPHj0cGZNDVVRUKCUlRefOndO5c+c0Y8YMTZs2rdXvd+RTiwEAQPtw5Pnb7pql3/3udyoqKpIkBQUFuXWiJEnBwcHavHmz8vPztXXrVj3//PP67rvvXB0WAADwEHYnS3/729/Up08f/eQnP9Err7yi8vJyZ8TlMN7e3goKCpIk1dXVyTAMtaEzDQAAdDB2J0sFBQXasWOHhg8frhdeeEFRUVG67bbb9NZbb+n06dN2B7B582aNGzdOUVFRMplMeuedd5qtk5WVpdjYWAUEBCg5OVnbtm2zax8VFRVKSEhQr1699MQTTygsLMzuOAEAQMd0SY87GTBggJ5//nl9++23ys7OVmxsrB599FFFRETYva2amholJCQoKyvrgsvXrFmjjIwMzZs3T3l5eUpISNDo0aNVWlpqWScxMVHXXntts9fRo0clNY4uXlBQoKKiIr311lsqKSm5lI8NAAA6ILvvhvuxTp06KTAwUH5+fjp16pTd7x87dqzGjh3b4vKFCxdq2rRpmjx5siRpyZIl+vDDD7Vs2TLNmjVLki447tOFhIeHKyEhQZ9++ql+9atfXXCduro61dXVWearqqpa+UkAAMDl6JJ6loqKivTcc89pwIABGjx4sL788ks9/fTTOn78uEODq6+vV25urlJSUixtXl5eSklJ0ZYtW1q1jZKSEksSV1lZqc2bN6t///4trp+ZmakuXbpYXtHR0W37EAAAwKPZ3bN0ww03aPv27Ro4cKAmT56su+++Wz179nRGbCovL1dDQ4PCw8Ot2sPDw7Vnz55WbaO4uFgPPPCApbB7+vTpuu6661pcf/bs2crIyLDMV1VVkTABANCB2Z0sjRo1SsuWLVN8fLwz4nG4oUOHtvoyndT4jDt/f3/nBQQAADyK3cnSc88954w4LigsLEze3t7NCrJLSkouqZjcHllZWcrKylJDQ4NT9wMAANxbq5KljIwMPfvss+rUqZPVJaoLWbhwoUMCkyQ/Pz8lJSVp48aNmjBhgiTJbDZr48aNevjhhx22nwtJT09Xenq6ZQRQAADQMbUqWfryyy919uxZy8+OVF1drf3791vmi4qKlJ+fr27duql3797KyMhQamqqBg8erKFDh2rRokWqqamx3B0HAADgTG16NpwjbNq0SSNGjGjWnpqaquXLl0uSFi9erP/+7//W8ePHlZiYqJdfflnJycntEh/PhgMAwPM48vxtd7I0ZcoU/fGPf1RwcLBVe01NjaZPn65ly5a1KSB3cX7NUmFhIckSAAAexKXJkre3t44dO9bsAbrl5eWKiIjQuXPn2hSQu6FnCQAAz+PI83er74arqqqyjFV06tQpBQQEWJY1NDToo48+apZAAQAAeLpWJ0uhoaEymUwymUzq169fs+Umk0lPP/20Q4MDAABwtVYnS9nZ2TIMQyNHjtQ//vEPdevWzbLMz89PMTExioqKckqQrsA4SwAAQLqEmqXi4mJFR0fLy+uSHivncahZAgDA87ikZqlJTEyMJOn06dM6ePCg6uvrrZYPHDiwTQEBAAC4E7uTpbKyMk2ePFnr1q274HIuWwEAgMuJ3dfSHn30UVVUVGjr1q0KDAzU+vXr9Ze//EV9+/bVe++954wYXSIrK0vx8fEaMmSIq0MBAAAuZHfNUmRkpN59910NHTpUISEhysnJUb9+/fTee+9pwYIF+uyzz5wVq0tQswQAgOdx5Pnb7p6lmpoay3hKXbt2VVlZmSTpuuuuU15eXpuCAQAAcDd2J0v9+/fX3r17JUkJCQl67bXXdOTIES1ZskSRkZEODxAAAMCV7C7wnjFjho4dOyZJmjdvnsaMGaP/+Z//kZ+fn+XBtwAAAJcLu2uWfuz06dPas2ePevfurbCwMEfF5TaoWQIAwPO4dJylHwsKCtKgQYPauhm3wwjeAABAamXPUkZGRqs3uHDhwjYF5G7oWQIAwPO0e8/Sl19+2aqNmUymNgUDAADgblqVLGVnZzs7DgAAALd0yU/D3b9/vz7++GOdOXNGktTGOnEAAAC3ZHey9N1332nUqFHq16+ffv7zn1uGEZg6daoef/xxhwcIAADgSnYnS4899ph8fX118OBBBQUFWdrvuusurV+/3qHBAQAAuJrdQwds2LBBH3/8sXr16mXV3rdvXxUXFzssMFdj6AAAACBd4rPhzu9RanLixAn5+/s7JCh3kJ6erl27dmn79u2uDgUAALiQ3cnSTTfdpBUrVljmTSaTzGazFixYoBEjRjg0OAAAAFez+zLcggULNGrUKOXk5Ki+vl5PPvmkdu7cqRMnTujzzz93RowAAAAuY3fP0rXXXqvCwkL99Kc/1fjx41VTU6M77rhDX375pa6++mpnxAgAAOAydvUsnT17VmPGjNGSJUs0Z84cZ8UEAADgNuzqWfL19dWOHTucFQsAAIDbsfsy3D333KOlS5c6IxYAAAC3Y3eB97lz57Rs2TJ98sknSkpKUqdOnayWL1y40GHBAQAAuJrdydLXX3+tQYMGSZIKCwutlplMJsdE5QYYlBIAAEiSyeAJuDZVVVWpS5cuqqysVEhIiKvDAQAAreDI87fdNUsAAAAdCckSAACADSRLAAAANpAsAQAA2ECyBAAAYAPJEgAAgA0kSwAAADaQLAEAANhAsgS4SE5VlUbm5yunqsrVoQAAbCBZAlxkRUmJsisqtLKkxNWhAABssPvZcB0Fz4aDMxTX1qr87FmZJK0pLZUkrS4tVWpEhAxJYb6+igkIcGmMAABrPBvuIng2HBzJtGnTDz9LMs6bNjGGD2/XmADgcsSz4QAPtSouTj4mk6QfEqSmqY/JpFVxcS6JCwDQMi7DAe1oUni44oKClJSb22zZ1kGDNCg42AVRAQBsoWcJcBGvH00BAO6J/6eBdtbD11cRvr5KCg7Wkn79lBQcrAhfX/Xw9XV1aACAC+AyHNDOegUE6MCwYfIzmWQymfRAZKTqDUP+XvztAgDuiGQJcIHzEyOTyST/74u+AQDuhz9lAQAAbCBZAgAAsIFkCQAAwAaSJQAAABtIlgAAAGwgWQIAALCBZAkAAMCGDpMsnT59WjExMZo5c6arQwEAAB6kwyRLzz33nG644QZXhwEAADxMh0iW9u3bpz179mjs2LGuDgUAAHgYlydLmzdv1rhx4xQVFSWTyaR33nmn2TpZWVmKjY1VQECAkpOTtW3bNrv2MXPmTGVmZjooYgAA0JG4PFmqqalRQkKCsrKyLrh8zZo1ysjI0Lx585SXl6eEhASNHj1apaWllnUSExN17bXXNnsdPXpU7777rvr166d+/fq110cCAACXEZNhGIarg2hiMpm0du1aTZgwwdKWnJysIUOGaPHixZIks9ms6OhoTZ8+XbNmzbroNmfPnq1Vq1bJ29tb1dXVOnv2rB5//HHNnTv3guvX1dWprq7OMl9VVaXo6GhVVlYqJCSkbR8QAAC0i6qqKnXp0sUh52+X9yzZUl9fr9zcXKWkpFjavLy8lJKSoi1btrRqG5mZmTp06JAOHDigF154QdOmTWsxUWpav0uXLpZXdHR0mz/HheRUVWlkfr5yqqqcsn0AAOAYbp0slZeXq6GhQeHh4Vbt4eHhOn78uFP2OXv2bFVWVlpehw4dcsp+VpSUKLuiQitLSpyyfQAA4Bg+rg6gPaWlpV10HX9/f/n7+ztl/8W1tSo/e1YmSWu+r7laXVqq1IgIGZLCfH0VExDglH27o5yqKj357bdacNVVGswlTgCAm3LrZCksLEze3t4q+VHvS0lJiSIiIpy676ysLGVlZamhocFh24z9978tP5u+n5adPauk3FxLuzF8uMP25+7O710jWQIAuCu3vgzn5+enpKQkbdy40dJmNpu1ceNGDRs2zKn7Tk9P165du7R9+3aHbXNVXJx8TI1pUlNVfdPUx2TSqrg4h+3LXRXX1ir31CnlnTpl1buWd+qUck+dUnFtrYsjBADAmst7lqqrq7V//37LfFFRkfLz89WtWzf17t1bGRkZSk1N1eDBgzV06FAtWrRINTU1mjx5sgujvjSTwsMVFxRk1ZPUZOugQRoUHOyCqNoXvWsAAE/j8mQpJydHI0aMsMxnZGRIklJTU7V8+XLdddddKisr09y5c3X8+HElJiZq/fr1zYq+PY2XJPN5045iVVyc0vbs0TnDuGDv2vJrrnFVaHAB6tYAeAK3GmfJnZxfs1RYWOiwcZYO19ZqSG6uogMCNDUyUkuPHdOh2lptT0pSrw5S3J136tQFe9dyk5I6RO8afvDIvn3605EjeqRnT/2xb19XhwPgMuLIcZZIli7Ckb/sJnVms/xMJplMJhmGoXrDkL+XW5ePOVRTsvTj3jWSpY7h/LtCx+7YodKzZ9XD11frBg7skHeFAnAOR56/XX4ZriM6PzEymUzyN5lsrH356eHrqwhf32a9az18fV0dGtoBdWsAPA3JEtpdr4AAHRg2zNK79kBkZIfrXevIqFsD4Gk4O7UgKytL8fHxGjJkiKtDuSz5e3nJ9H2PmslkIlHqQCaFh2vroEEXXLZ10CBN8vCbNwBcfjhDtcAZ4ywBsOb1oykAuCP+jwLQ7prq1pKCg7WkXz8lBQcrwteXujUAbomaJQDtjro1AJ6EZAmAS3T0u0IBeA7+jGsBBd4AAEBiUMqLcsaglAAAwLkcef6mZwkAAMAGkiUAAAAbSJYAAABsIFlqAQXeAABAosD7oijwBgDA81DgDQAA0E5IlgAAAGwgWQIAALCBZAkAAMAGkiUAAAAbSJZawNABAABAYuiAi2LoAAAAPA9DBwAAALQTkiUAAAAbSJYAAABsIFkCAACwgWQJAADABpIlAAAAG0iWAAAAbCBZagGDUgIAAIlBKS+KQSkBAPA8DEoJAPB4OVVVGpmfr5yqKleHAhfxlO8AyRIAwCVWlJQou6JCK0tKXB0KXMRTvgM+rg4AANBxFNfWqvzsWZkkrSktlSStLi1VakSEDElhvr6KCQhwaYxwLk/8DlCzdBHULAGA45g2bfrhZ0nGedMmxvDh7RoT2ld7fQeoWQIAeKRVcXHyMZkk/XBybJr6mExaFRfnkrjQfjzxO8BlOABAu5kUHq64oCAl5eY2W7Z10CANCg52QVRoT574HaBnCQDgEl4/mqLj8ZTvgLvHBwC4zPTw9VWEr6+SgoO1pF8/JQUHK8LXVz18fV0dGtqJp30HKPC+CAq8AcDx6sxm+ZlMMplMMgxD9YYhfy/+fu9InP0dcOT5m5olAEC7O/+kaDKZ5P99wS86Dk/6DpDGt4BnwwEAAInLcBfFZTgAADwP4ywBAAC0E5IlAAAAG0iWAMBFPOWJ63AevgOegWQJAFzEU564DufhO+AZGDoAANqRJz5xHY7Fd8DzcDfcRXA3HABHaq8nrsN98R1oH9wNBwAeyhOfuA7H4jvgebgMBwDtyBOfuA7H4jvgeehZAgAX8ZQnrsN5+A54Bv59AKCdedoT1+F4fAc8CwXeF0GBNwBncPYT1+H++A44lyPP39QsAYALeNIT1+EcfAc8ByksAACADSRLAAAANnSIy3CxsbEKCQmRl5eXunbtquzsbFeHBAAAPESHSJYk6YsvvlDnzp1dHQYAAPAwXIYDAACwweXJ0ubNmzVu3DhFRUXJZDLpnXfeabZOVlaWYmNjFRAQoOTkZG3bts2ufZhMJt1yyy0aMmSI/ud//sdBkQMAgI7A5ZfhampqlJCQoClTpuiOO+5otnzNmjXKyMjQkiVLlJycrEWLFmn06NHau3evevToIUlKTEzUuXPnmr13w4YNioqK0meffaaePXvq2LFjSklJ0XXXXaeBAwdeMJ66ujrV1dVZ5quqqhz0SQEAgCdyq0EpTSaT1q5dqwkTJljakpOTNWTIEC1evFiSZDabFR0drenTp2vWrFl27+OJJ57QgAEDlJaWdsHl8+fP19NPP92snUEpAQDwHI4clNLll+Fsqa+vV25urlJSUixtXl5eSklJ0ZYtW1q1jZqaGp06dUqSVF1drX/+858aMGBAi+vPnj1blZWVltehQ4fa9iEAAIBHc/llOFvKy8vV0NCg8PBwq/bw8HDt2bOnVdsoKSnRxIkTJUkNDQ2aNm2ahgwZ0uL6/v7+8vf3v/SgAQDAZcWtkyVHuOqqq1RQUHDJ72+6SkntEgAAnqPpvO2IaiO3TpbCwsLk7e2tkpISq/aSkhJFREQ4dd9ZWVnKyspSfX29JCk6Otqp+wMAAI536tQpdenSpU3bcOtkyc/PT0lJSdq4caOl6NtsNmvjxo16+OGHnbrv9PR0paeny2w26+jRowoODpbpEh9yOGTIEG3fvr1d3mvP+q1Zt6qqStHR0Tp06FCHLXBvy7+fM7VnXI7elyO2x3Hl2TiuOK6cfVwZhqFTp04pKiqq1TG1xOXJUnV1tfbv32+ZLyoqUn5+vrp166bevXsrIyNDqampGjx4sIYOHapFixappqZGkydPbpf4vLy81KtXrzZtw9vb+5L/Q7T3vfasb8+6ISEhHfY/9bb8+zlTe8bl6H05YnscV56N44rjqj2Oq7b2KDVxebKUk5OjESNGWOYzMjIkSampqVq+fLnuuusulZWVae7cuTp+/LgSExO1fv36ZkXf7iw9Pb3d3mvP+m2JqyNx199Te8bl6H05YnscV57NXX9PHFccVxfiVuMswf04cpwKAI04rgDHc+Zx5dbjLMH1/P39NW/ePIZTAByI4wpwPGceV/QsAQAA2EDPEgAAgA0kSwAAADaQLAEAANhAsgQAAGADyRIAAIANJEu4JIcOHdLw4cMVHx+vgQMH6m9/+5urQwI8XkVFhQYPHqzExERde+21ev31110dEnDZOH36tGJiYjRz5ky738vQAbgkx44dU0lJiRITE3X8+HElJSWpsLBQnTp1cnVogMdqaGhQXV2dgoKCVFNTo2uvvVY5OTm64oorXB0a4PHmzJmj/fv3Kzo6Wi+88IJd76VnCZckMjJSiYmJkqSIiAiFhYXpxIkTrg0K8HDe3t4KCgqSJNXV1ckwDPH3LNB2+/bt0549ezR27NhLej/JUge1efNmjRs3TlFRUTKZTHrnnXearZOVlaXY2FgFBAQoOTlZ27Ztu+C2cnNz1dDQoOjoaCdHDbg3RxxXFRUVSkhIUK9evfTEE08oLCysnaIH3JMjjquZM2cqMzPzkmMgWeqgampqlJCQoKysrAsuX7NmjTIyMjRv3jzl5eUpISFBo0ePVmlpqdV6J06c0H333ac///nP7RE24NYccVyFhoaqoKBARUVFeuutt1RSUtJe4QNuqa3H1bvvvqt+/fqpX79+lx6EgQ5PkrF27VqrtqFDhxrp6emW+YaGBiMqKsrIzMy0tNXW1ho33XSTsWLFivYKFfAYl3pcne8///M/jb/97W/ODBPwKJdyXM2aNcvo1auXERMTY1xxxRVGSEiI8fTTT9u1X3qW0Ex9fb1yc3OVkpJiafPy8lJKSoq2bNkiSTIMQ2lpaRo5cqTuvfdeV4UKeIzWHFclJSU6deqUJKmyslKbN29W//79XRIv4Alac1xlZmbq0KFDOnDggF544QVNmzZNc+fOtWs/JEtopry8XA0NDQoPD7dqDw8P1/HjxyVJn3/+udasWaN33nlHiYmJSkxM1FdffeWKcAGP0Jrjqri4WDfddJMSEhJ00003afr06bruuutcES7gEVpzXDmCj8O2hA7lpz/9qcxms6vDAC4rQ4cOVX5+vqvDAC5baWlpl/Q+epbQTFhYmLy9vZsVlpaUlCgiIsJFUQGejeMKcLz2Oq5IltCMn5+fkpKStHHjRkub2WzWxo0bNWzYMBdGBngujivA8drruOIyXAdVXV2t/fv3W+aLioqUn5+vbt26qXfv3srIyFBqaqoGDx6soUOHatGiRaqpqdHkyZNdGDXg3jiuAMdzi+OqrbfxwTNlZ2cbkpq9UlNTLev86U9/Mnr37m34+fkZQ4cONf7973+7LmDAA3BcAY7nDscVz4YDAACwgZolAAAAG0iWAAAAbCBZAgAAsIFkCQAAwAaSJQAAABtIlgAAAGwgWQIAALCBZAkAAMAGkiUAAAAbSJYAAABsIFkCAACwgWQJQIfzzTffyGQy6YMPPtCoUaMUFBSk/v37a+vWra4ODYAbIlkC0OEUFBTIZDJp4cKFeuqpp1RQUKDevXtr1qxZrg4NgBsiWQLQ4RQUFCg0NFRr1qzR8OHD1bdvX91+++0qKytzdWgA3BDJEoAOp6CgQOPHj1f37t0tbUVFRerTp48LowLgrkiWAHQ4BQUFGjZsmFVbfn6+EhMTXRMQALdGsgSgQ6msrNSBAwd0/fXXW7WTLAFoCckSgA5lx44d8vHx0XXXXWdpKy4u1smTJ0mWAFwQyRKADqWgoED9+/dXQECApe3LL79UaGioYmNjXRcYALdlMgzDcHUQAAAA7oqeJQAAABtIlgAAAGwgWQIAALCBZAkAAMAGkiUAAAAbSJYAAABsIFkCAACwgWQJAADABpIlAAAAG0iWAAAAbCBZAgAAsIFkCQAAwIb/D1dKR8bMhXZNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mpp(a,b):\n",
    "    assert a.option.get_format() == b.option.get_format()\n",
    "    return cast(a, op).T()@cast(b,op)\n",
    "\n",
    "def test(n):\n",
    "    x = np.linspace(0,1,n)\n",
    "    a = np.exp(x)\n",
    "    b = np.sin(np.pi*x)**2\n",
    "\n",
    "    ex = a@b\n",
    "\n",
    "    aa = LPV(a, opb)\n",
    "    bb = LPV(b, opb)\n",
    "    \n",
    "    half = (aa.T()@bb).array()[0]\n",
    "    mixed = mpp(aa,bb).array()[0]\n",
    "\n",
    "    return abs(half-ex)/abs(ex), abs(mixed-ex)/abs(ex)\n",
    "\n",
    "NN = 2**np.arange(6,14)\n",
    "errs = np.array([list(test(n)) for n in NN])\n",
    "plt.loglog(NN, errs[:,0], 'r*', label=\"Reduced precision\")\n",
    "plt.loglog(NN, errs[:,1], 'c*', label=\"Mixed precision\")\n",
    "plt.xlabel(\"$n$\")\n",
    "plt.ylabel(\"relative rounding error\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# In the lectures we saw that rounding errors in dot products grow linearly in the vector size n, which is what we observe by looking at the half-precision computations.\n",
    "# In mixed-precision, we instead expect O(u) accuracy as long as u_single_precision*n^2 << 1, which is what we observe in the figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304da780-8bd4-4409-be8d-fca01d2c3e7e",
   "metadata": {},
   "source": [
    "Question 2 - Mixed-precision and overflow\n",
    "-----------------------------------------\n",
    "\n",
    "Repeat Question 5 in Practical 4, but now accumulate and store the sum using single precision (use the `cast` function). Does the computation overflow at the value computed in Question 5 of Practical 4?\n",
    "\n",
    "For which theoretical value will the calculation overflow now? You can compute this value numerically, but, please, DO NOT try to make it overflow in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530f5c6-a869-4207-9d06-a5d19f5bea75",
   "metadata": {},
   "source": [
    "Solution to Question 2\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1ba0c0-95d2-4104-a107-6bb399db0a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65792.0\n",
      "The computation will overflow for n = 1.844674352395373e+19\n"
     ]
    }
   ],
   "source": [
    "oph = Option(True)\n",
    "oph.set_format('h')\n",
    "\n",
    "zero = np.array([0.])\n",
    "\n",
    "def mpsum(a):\n",
    "    n = a.size\n",
    "    aa = LPV(a, oph)\n",
    "    aa = cast(aa, op)\n",
    "    s = LPV(zero, op)\n",
    "    for i in range(n):\n",
    "        s = s + aa[i]\n",
    "        \n",
    "    return s.array()[0]\n",
    "\n",
    "def overflow_test(n):\n",
    "    a = 2*np.arange(1,n+1)\n",
    "    return mpsum(a)\n",
    "\n",
    "# Overflow will occur when s becomes greater than xmax\n",
    "# HOWEVER, now xmax is the single-precision xmax\n",
    "xmax = op.get_floating_point_parameters()['xmax']\n",
    "# s = n(n+1) so we need to solve a quadratic equation: n(n+1) == xmax\n",
    "roots = np.roots([1, 1, -xmax])\n",
    "roots = np.real(roots[np.isreal(roots)])\n",
    "roots = roots[roots > 0]\n",
    "n_overflow = np.ceil(min(roots))\n",
    "\n",
    "print(overflow_test(256)) # it does not overflow\n",
    "\n",
    "print(\"The computation will overflow for n =\", n_overflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0b4f1-739a-491e-b59d-7d1140a93b87",
   "metadata": {},
   "source": [
    "Question 3 - Mixed-precision Gradient Descent\n",
    "----------------------------------------------\n",
    "\n",
    "Implement the gradient descent method in double precision for minimizing the 2D Rosenbrock function:\n",
    "\n",
    "$$ f(x,y) = (a-x)^2 + b(y-x^2)^2 $$\n",
    "\n",
    "where $a=\\pi-3$, $b=100$. This function has a unique global minimum at the point $(a, a^2)$. For the method, set the learning rate $\\gamma = 10^{-3}$, the initial condition to zero, and the maximum number of iterations to $2 \\times 10^4$. \n",
    "\n",
    "1- Implement the scheme using only double precision (i.e., no need for the `chopping` library).\n",
    "\n",
    "2- Implement the scheme using only bfloat16 half precision.\n",
    "\n",
    "3- Implement the scheme in mixed precision by evaluating gradients in double precision and then casting the result to bfloat16. Use bfloat16 for storing the current iterate as well.\n",
    "\n",
    "4- Same as 3, but evaluate gradients in single precision rather than double (use numpy for this, no need to use the `chopping` library).\n",
    "\n",
    "In order to monitor the convergence behaviour of these methods, compute the Euclidean norm of the gradient at the current iterate every $1000$ iterations. Run all four implementations and compare their gradient norms. Can you explain this convergence behaviour based on what we have learnt in the lectures?\n",
    "\n",
    "**Hint:** The functions provided below may be useful. It is not necessary, but feel free to modify them at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52db18a6-d371-4d45-aeb7-f049d9ef47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chopping import *\n",
    "\n",
    "op = Option(True)\n",
    "op.set_format(\"b\")\n",
    "op.set_round(1)\n",
    "u = op.get_floating_point_parameters()[\"u\"] # roundoff unit\n",
    "\n",
    "# Define rosenbrock function, its gradient and Hessian, and its exact solution\n",
    "a = np.pi-3\n",
    "b = 100\n",
    "rosenbrock = lambda x,y : (a-x)*(a-x) + b*(y - x*x)*(y - x*x)\n",
    "grad = lambda x,y : [-2*a - 4*b*x*(-x*x + y) + 2*x, b*(-2*x*x + 2*y)]\n",
    "hess = lambda x,y : [[2*(4*b*x*x + 2*b*(x*x - y) + 1), -4*b*x], [-4*b*x, 2*b]] # the Hessian is not really needed here\n",
    "ex_sol = np.array([a, a**2])\n",
    "\n",
    "# Computes the gradient in single precision and casts the output to double. Feel free to modify this.\n",
    "def grad_single(x,y):\n",
    "    return list(np.array(grad(x.astype(np.float32), y.astype(np.float32))).astype(np.float64))\n",
    "\n",
    "# libchopping was not really designed for this so we define some utilitiy functions\n",
    "# to cast to and from LPV objects\n",
    "no_lpv = lambda z : (z[0].array()[0], z[1].array()[0]) if isinstance(z[0], LPV) else z\n",
    "to_lpv = lambda z : (LPV(np.array([z[0]]), op), LPV(np.array([z[1]]), op)) if not isinstance(z[0], LPV) else z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c96836-dd84-419c-be83-6d0f1914692e",
   "metadata": {},
   "source": [
    "Solution to Question 3\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1c9a5b-635e-42d4-9b1d-65753c92af3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GRADIENT DESCENT:\n",
      "\n",
      "Iteration  1000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 3.939141e-02  Gradient norm MP (f64,b16): 3.939141e-02  Gradient norm exact method: 3.939130e-02\n",
      "Iteration  2000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 6.024613e-03  Gradient norm MP (f64,b16): 6.024613e-03  Gradient norm exact method: 6.024229e-03\n",
      "Iteration  3000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 9.416232e-04  Gradient norm MP (f64,b16): 9.416087e-04  Gradient norm exact method: 9.414709e-04\n",
      "Iteration  4000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 1.476879e-04  Gradient norm MP (f64,b16): 1.476804e-04  Gradient norm exact method: 1.476492e-04\n",
      "Iteration  5000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 2.317602e-05  Gradient norm MP (f64,b16): 2.317413e-05  Gradient norm exact method: 2.316832e-05\n",
      "Iteration  6000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 3.642584e-06  Gradient norm MP (f64,b16): 3.636721e-06  Gradient norm exact method: 3.635764e-06\n",
      "Iteration  7000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 5.814707e-07  Gradient norm MP (f64,b16): 5.707262e-07  Gradient norm exact method: 5.705618e-07\n",
      "Iteration  8000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 1.682241e-07  Gradient norm MP (f64,b16): 8.957026e-08  Gradient norm exact method: 8.953866e-08\n",
      "Iteration  9000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 2.197956e-08  Gradient norm MP (f64,b16): 1.405593e-08  Gradient norm exact method: 1.405137e-08\n",
      "Iteration 10000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 9.891567e-08  Gradient norm MP (f64,b16): 2.205756e-09  Gradient norm exact method: 2.205092e-09\n",
      "Iteration 11000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 1.826797e-07  Gradient norm MP (f64,b16): 3.461644e-10  Gradient norm exact method: 3.460471e-10\n",
      "Iteration 12000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 2.197814e-08  Gradient norm MP (f64,b16): 5.432762e-11  Gradient norm exact method: 5.430506e-11\n",
      "Iteration 13000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 9.891333e-08  Gradient norm MP (f64,b16): 8.525965e-12  Gradient norm exact method: 8.522198e-12\n",
      "Iteration 14000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 1.826773e-07  Gradient norm MP (f64,b16): 1.338195e-12  Gradient norm exact method: 1.337600e-12\n",
      "Iteration 15000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 2.197673e-08  Gradient norm MP (f64,b16): 2.108320e-13  Gradient norm exact method: 2.108320e-13\n",
      "Iteration 16000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 9.891099e-08  Gradient norm MP (f64,b16): 3.519407e-14  Gradient norm exact method: 3.514014e-14\n",
      "Iteration 17000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 1.826750e-07  Gradient norm MP (f64,b16): 1.389177e-14  Gradient norm exact method: 1.389177e-14\n",
      "Iteration 18000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 2.197531e-08  Gradient norm MP (f64,b16): 1.389177e-14  Gradient norm exact method: 1.389177e-14\n",
      "Iteration 19000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 9.890865e-08  Gradient norm MP (f64,b16): 1.389177e-14  Gradient norm exact method: 1.389177e-14\n",
      "Iteration 20000:  Gradient norm RP: 1.575406e-01  Gradient norm MP (f32,b16): 1.826726e-07  Gradient norm MP (f64,b16): 1.389177e-14  Gradient norm exact method: 1.389177e-14\n",
      "\n",
      "Distance from global minima.\t RP: 8.072921e-02,\t MP (f32,b16): 9.732524e-09,\t MP (f64,b16): 7.386768e-15,\t exact: 7.386768e-15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chopping import *\n",
    "\n",
    "op = Option(True)\n",
    "op.set_format(\"b\")\n",
    "op.set_round(1)\n",
    "u = op.get_floating_point_parameters()[\"u\"] # roundoff unit\n",
    "\n",
    "# Define rosenbrock function and exact solution\n",
    "a = np.pi-3\n",
    "b = 100\n",
    "rosenbrock = lambda x,y : (a-x)*(a-x) + b*(y - x*x)*(y - x*x)\n",
    "grad = lambda x,y : [-2*a - 4*b*x*(-x*x + y) + 2*x, b*(-2*x*x + 2*y)]\n",
    "hess = lambda x,y : [[2*(4*b*x*x + 2*b*(x*x - y) + 1), -4*b*x], [-4*b*x, 2*b]]\n",
    "ex_sol = np.array([a, a**2])\n",
    "\n",
    "# Computes the gradient in single precision and casts the output to double. Feel free to modify this.\n",
    "def grad_single(x,y):\n",
    "    return list(np.array(grad(x.astype(np.float32), y.astype(np.float32))).astype(np.float64))\n",
    "\n",
    "# libchopping was not really designed for this so we define some utilitiy functions\n",
    "# to cast to and from LPV objects\n",
    "no_lpv = lambda z : (z[0].array()[0], z[1].array()[0]) if isinstance(z[0], LPV) else z\n",
    "to_lpv = lambda z : (LPV(np.array([z[0]]), op), LPV(np.array([z[1]]), op)) if not isinstance(z[0], LPV) else z\n",
    "\n",
    "def exact_gd(z, gamma=1e-3):\n",
    "    g = np.array(grad(*z))\n",
    "    z = np.array(z)\n",
    "    return z - gamma*g\n",
    "\n",
    "def gd_update(z, mixed_precision=False, single_precision_gradients=False):\n",
    "    if mixed_precision:\n",
    "        # if mixed_precision, then the input z is not LPV so whatever we compute with that\n",
    "        # will be in double precision\n",
    "        if not single_precision_gradients:\n",
    "            g = to_lpv(grad(*z)) # double precision evaluation of gradients + reduced-precision casting\n",
    "        else:\n",
    "            g = to_lpv(grad_single(*z)) # single precision evaluation of gradients + reduced-precision casting\n",
    "\n",
    "    else:\n",
    "        # if not mixed_precision, then the input z is LPV so whatever we compute with that\n",
    "        # will be in reduced precision\n",
    "        g = grad(*z) # reduced-precision gradient\n",
    "\n",
    "    out = g\n",
    "\n",
    "    if mixed_precision:\n",
    "        out = no_lpv(out) # cast to double precision so that update is performed in double\n",
    "\n",
    "    return out\n",
    "\n",
    "def reduced_precision_gd(z, gamma = 1.0e-3, mixed_precision=False, single_precision_gradients=False):\n",
    "    if not mixed_precision:\n",
    "        z = to_lpv(z) # if not mixed-precision, updates are performed in reduced precision\n",
    "    update = gd_update(z, mixed_precision, single_precision_gradients)\n",
    "    return no_lpv([z[0] - gamma*update[0], z[1] - gamma*update[1]])\n",
    "\n",
    "def gd(z, gamma=1.0e-3, mode=0):\n",
    "    \n",
    "    if   mode == 0: return reduced_precision_gd(z, gamma, mixed_precision=False)\n",
    "    elif mode == 1: return reduced_precision_gd(z, gamma, mixed_precision=True, single_precision_gradients=True)\n",
    "    elif mode == 2: return reduced_precision_gd(z, gamma, mixed_precision=True)\n",
    "    elif mode == 3: return exact_gd(z, gamma)\n",
    "    else: raise NotImplementedError\n",
    "\n",
    "maxits = 20000\n",
    "gamma = 1.0e-3\n",
    "\n",
    "z = np.zeros((4,2))\n",
    "gnorms = np.zeros((4,))\n",
    "\n",
    "print(\"\\n\\nGRADIENT DESCENT:\\n\")\n",
    "for i in range(1,maxits+1):\n",
    "    for j in range(4):\n",
    "        z[j] = gd(z[j], gamma=gamma, mode=j)\n",
    "        gnorms[j] = np.linalg.norm(grad(*z[j]))\n",
    "        \n",
    "    if i%1000 == 0:\n",
    "        print(\"Iteration %5d:\" % i, \" Gradient norm RP: %e\" % gnorms[0], \" Gradient norm MP (f32,b16): %e\" % gnorms[1], \" Gradient norm MP (f64,b16): %e\" % gnorms[2], \" Gradient norm exact method: %e\" % gnorms[3])\n",
    "\n",
    "distances = [np.linalg.norm(np.array(item) - ex_sol) for item in z]\n",
    "print(\"\\nDistance from global minima.\\t RP: %e,\\t MP (f32,b16): %e,\\t MP (f64,b16): %e,\\t exact: %e\" % tuple(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433ddee-3ca4-4eb1-82d9-e3918aca9597",
   "metadata": {},
   "source": [
    "Question 4 (Optional)\n",
    "---------------------\n",
    "\n",
    "**Note:** This question is optional, only work on this if you are done with everything and you still have time.\n",
    "\n",
    "Repeat Question 3, but now implement Newton's method rather than gradient descent. Here you can play with the precision of three quantities: the iterate, the gradient, the Hessian (and its inverse). How do all possible precision choices affect convergence? For this question $10$ iterates is enough.\n",
    "\n",
    "**Hint:** The Hessian inverse times the gradient can be computed by hand since the Hessian is a $2$-by-$2$ matrix:\n",
    "\n",
    "```python\n",
    "    # computes H^{-1}g. Feel free to modify it at will\n",
    "    A = H[0][0]\n",
    "    B = H[0][1]\n",
    "    D = H[1][1]\n",
    "    det = A*D - B*B\n",
    "    iH = [[D/det, -B/det], [-B/det, A/det]]\n",
    "    out = [iH[0][0]*g[0] + iH[0][1]*g[1], iH[1][0]*g[0] + iH[1][1]*g[1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843fce40-7feb-4902-bc0f-f6558b7fa15e",
   "metadata": {},
   "source": [
    "Solution to Question 4\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea90f28-859d-4dc8-bd3d-9d34ebb3e6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     1:  Gradient norm RP: 4.167921e+00  Gradient norm MP: 4.167921e+00  Gradient norm exact method: 4.167372e+00\n",
      "Iteration     2:  Gradient norm RP: 4.420182e-02  Gradient norm MP: 4.420182e-02  Gradient norm exact method: 0.000000e+00\n",
      "Iteration     3:  Gradient norm RP: 6.546672e-03  Gradient norm MP: 2.038136e-04  Gradient norm exact method: 0.000000e+00\n",
      "Iteration     4:  Gradient norm RP: 6.546672e-03  Gradient norm MP: 2.239137e-05  Gradient norm exact method: 0.000000e+00\n",
      "Iteration     5:  Gradient norm RP: 6.546672e-03  Gradient norm MP: 2.345981e-06  Gradient norm exact method: 0.000000e+00\n",
      "Iteration     6:  Gradient norm RP: 6.546672e-03  Gradient norm MP: 2.144184e-07  Gradient norm exact method: 0.000000e+00\n",
      "Iteration     7:  Gradient norm RP: 6.546672e-03  Gradient norm MP: 1.892255e-08  Gradient norm exact method: 0.000000e+00\n",
      "Iteration     8:  Gradient norm RP: 6.546672e-03  Gradient norm MP: 1.635559e-09  Gradient norm exact method: 0.000000e+00\n",
      "Iteration     9:  Gradient norm RP: 6.546672e-03  Gradient norm MP: 1.355426e-10  Gradient norm exact method: 0.000000e+00\n",
      "Iteration    10:  Gradient norm RP: 6.546672e-03  Gradient norm MP: 3.029689e-12  Gradient norm exact method: 0.000000e+00\n",
      "Distance from global minima.\t RP: 3.028816e-05,\t MP: 2.059386e-14,\t exact: 0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chopping import *\n",
    "\n",
    "# NOTE: This is not a complete solution as it does not try all possible precision combinations. However, it is easy enough to modify this to do so.\n",
    "\n",
    "op = Option(True)\n",
    "op.set_format(\"b\")\n",
    "op.set_round(1)\n",
    "u = op.get_floating_point_parameters()[\"u\"] # roundoff unit\n",
    "\n",
    "# Define rosenbrock function and exact solution\n",
    "a = np.pi-3\n",
    "b = 100\n",
    "rosenbrock = lambda x,y : (a-x)*(a-x) + b*(y - x*x)*(y - x*x)\n",
    "grad = lambda x,y : [-2*a - 4*b*x*(-x*x + y) + 2*x, b*(-2*x*x + 2*y)]\n",
    "hess = lambda x,y : [[2*(4*b*x*x + 2*b*(x*x - y) + 1), -4*b*x], [-4*b*x, 2*b]]\n",
    "ex_sol = np.array([a, a**2])\n",
    "\n",
    "# Computes the gradient in single precision and casts the output to double. Feel free to modify this.\n",
    "def grad_single(x,y):\n",
    "    return list(np.array(grad(x.astype(np.float32), y.astype(np.float32))).astype(np.float64))\n",
    "\n",
    "# libchopping was not really designed for this so we define some utilitiy functions\n",
    "# to cast to and from LPV objects\n",
    "no_lpv = lambda z : (z[0].array()[0], z[1].array()[0]) if isinstance(z[0], LPV) else z\n",
    "to_lpv = lambda z : (LPV(np.array([z[0]]), op), LPV(np.array([z[1]]), op)) if not isinstance(z[0], LPV) else z\n",
    "\n",
    "def newton_update(z, mixed_precision=False):\n",
    "    if mixed_precision:\n",
    "        # if mixed_precision, then the input z is not LPV so whatever we compute with that\n",
    "        # will be in double precision\n",
    "        g = to_lpv(grad(*z)) # double precision evaluation of gradients + reduced-precision casting\n",
    "        H = hess(*to_lpv(z)) # reduced-precision hessians\n",
    "    else:\n",
    "        # if not mixed_precision, then the input z is LPV so whatever we compute with that\n",
    "        # will be in reduced precision\n",
    "        g = grad(*z) # reduced-precision gradient\n",
    "        H = hess(*z) # reduced-precision hessian\n",
    "\n",
    "    # compute H^{-1}g by hand since it is easier here\n",
    "    A = H[0][0]\n",
    "    B = H[0][1]\n",
    "    D = H[1][1]\n",
    "    det = A*D - B*B\n",
    "    iH = [[D/det, -B/det], [-B/det, A/det]]\n",
    "    out = [iH[0][0]*g[0] + iH[0][1]*g[1], iH[1][0]*g[0] + iH[1][1]*g[1]]\n",
    "\n",
    "    if mixed_precision:\n",
    "        out = no_lpv(out) # cast to double precision so that update is performed in double\n",
    "\n",
    "    return out\n",
    "\n",
    "def reduced_precision_newton(z, mixed_precision=False):\n",
    "    if not mixed_precision:\n",
    "        z = to_lpv(z) # if not mixed-precision, updates are performed in reduced precision\n",
    "    update = newton_update(z, mixed_precision)\n",
    "    return no_lpv([z[0] - update[0], z[1] - update[1]])\n",
    "\n",
    "# provided for comparison\n",
    "def exact_newton(z):\n",
    "    g = np.array(grad(*z))\n",
    "    H = np.array(hess(*z))\n",
    "    z = np.array(z)\n",
    "    return z - np.linalg.solve(H,g)\n",
    "\n",
    "\n",
    "maxits = 10\n",
    "z = np.zeros((2,))\n",
    "zm = np.zeros((2,))\n",
    "zex = np.zeros((2,))\n",
    "\n",
    "for i in range(1,maxits+1):\n",
    "    z = reduced_precision_newton(z, mixed_precision=False)\n",
    "    zm = reduced_precision_newton(zm, mixed_precision=True)\n",
    "    zex = exact_newton(zex)\n",
    "    gnorm = np.linalg.norm(grad(*z))\n",
    "    gnormm = np.linalg.norm(grad(*zm))\n",
    "    gnormex = np.linalg.norm(grad(*zex))\n",
    "    print(\"Iteration %5d:\" % i, \" Gradient norm RP: %e\" % gnorm, \" Gradient norm MP: %e\" % gnormm, \" Gradient norm exact method: %e\" % gnormex)\n",
    "    \n",
    "print(\"Distance from global minima.\\t RP: %e,\\t MP: %e,\\t exact: %e\" % (np.linalg.norm(np.array(z)-ex_sol), np.linalg.norm(np.array(zm)-ex_sol), np.linalg.norm(np.array(zex)-ex_sol)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132ea51-544c-4056-8108-6c6c0881781a",
   "metadata": {},
   "source": [
    "Question 5 - Scaling\n",
    "--------------------\n",
    "\n",
    "In this problem we will investigate the use of mixed-precision scaling techniques in a simplified setting. Consider the following optimization problem:\n",
    "\n",
    "$$ \\min_x f(x),\\quad\\text{where}\\quad f(x) = e^{x^2} - 1. $$\n",
    "\n",
    "This problem is strongly convex and has a unique global minimum at $x=0$, therefore gradient descent will converge to the global minimum in exact arithmetic and as the number of iterations approach infinity.\n",
    "\n",
    "Consider the following gradient descent code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4fe5bd7-9952-4510-bed2-1cfd82152234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     0:  Gradient norm: 9.246165e+04\n",
      "Iteration     1:  Gradient norm: 4.940542e+02\n",
      "Iteration     2:  Gradient norm: 4.824614e+02\n",
      "Iteration     3:  Gradient norm: 4.714230e+02\n",
      "Iteration     4:  Gradient norm: 4.608996e+02\n",
      "Iteration     5:  Gradient norm: 4.508556e+02\n",
      "Iteration 10000:  Gradient norm: 3.157494e+00\n",
      "Iteration 20000:  Gradient norm: 1.664137e+00\n",
      "Iteration 30000:  Gradient norm: 1.112368e+00\n",
      "Iteration 40000:  Gradient norm: 8.137534e-01\n",
      "Iteration 50000:  Gradient norm: 6.226381e-01\n",
      "Iteration 60000:  Gradient norm: 4.885887e-01\n",
      "Iteration 70000:  Gradient norm: 3.892610e-01\n",
      "Iteration 80000:  Gradient norm: 3.130816e-01\n",
      "Iteration 90000:  Gradient norm: 2.533453e-01\n",
      "Iteration 100000:  Gradient norm: 2.058187e-01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chopping import *\n",
    "\n",
    "op = Option(True)\n",
    "op.set_format('h') # fp16 half precision\n",
    "\n",
    "f = lambda x : np.exp(x**2) - 1\n",
    "grad = lambda x : 2*x*np.exp(x**2)\n",
    "\n",
    "def gd0(x, gamma=1e-5, double_precision=False):    \n",
    "    \n",
    "    g = grad(x)\n",
    "    \n",
    "    if not double_precision:\n",
    "        g = chop(g, op) # rounds the gradient to fp16 half precision (it stays a Python scalar, this example does not use LPV)\n",
    "    \n",
    "    return x - gamma*g\n",
    "\n",
    "x = 3.1\n",
    "gamma = 1e-5\n",
    "max_its = 100000\n",
    "for i in range(max_its+1):\n",
    "    if i%10000 == 0 or i <= 5:\n",
    "        print(\"Iteration %5d:\" % i, \" Gradient norm: %e\" % abs(grad(x)))\n",
    "\n",
    "    x = gd0(x, gamma=gamma, double_precision=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793025a3-a0ec-49d0-bf2e-6c9c2ef9894e",
   "metadata": {},
   "source": [
    "Read the code and understand what it does, then change the `double_precision` flag to `False` and observer that the method blows up.\n",
    "\n",
    "This happens because the function gradient is very large at the initial condition and causes an overflow in fp16 half precision.\n",
    "\n",
    "By only changing either the `grad` function or by modifying the computed gradient before or after `chop` is called, try to make gradient descent converge in half-precision by preventing an overflow. You have different options (try them all, they are quick):\n",
    "\n",
    "1- **Gradient clipping.** Set the gradient to the maximum value in the half-precision range before it is rounded.\n",
    "\n",
    "2- **Gradient scaling.** Divide the gradient by a scalar $s$ in high precision before rounding and then multiply `gamma` by $s$ before the update. Note that gradient scaling works a bit differently in practice, but this gives you an idea.\n",
    "\n",
    "3- **Autocasting.** Only for this one, do not apply the function `chop` if the computed gradient is beyond `xmax`, the largest number representable in float16 half precision.\n",
    "\n",
    "4- **Initialisation.** Change the initial guess. Note that this is much trickier in AI, but there is lots of literature on good initialization techniques.\n",
    "\n",
    "5- **SignGD.** This is a crazy one: replace the gradient with its sign before the chop. Here you can set the learning rate `gamma` to be $10$ times larger.\n",
    "\n",
    "6- **Change gradient.** This is somehow related to the previous one. Replace the gradient function with another function that has smaller magnitude, but has the same sign as the gradient. Note: this is just for fun, it will work here, but it is something that is impossible to do (or at least very very hard) for more complicated problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77933c4-ea39-4880-9eee-37cca614c129",
   "metadata": {},
   "source": [
    "Solution to Question 5\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1d8a99e-41cf-4af3-bc40-8c434f472115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Method: gd_clipping \n",
      "\n",
      "Iteration     0:  Gradient norm: 9.246165e+04\n",
      "Iteration     1:  Gradient norm: 1.929479e+03\n",
      "Iteration     2:  Gradient norm: 1.742594e+03\n",
      "Iteration     3:  Gradient norm: 1.590277e+03\n",
      "Iteration     4:  Gradient norm: 1.463681e+03\n",
      "Iteration     5:  Gradient norm: 1.356602e+03\n",
      "Iteration 100000:  Gradient norm: 2.059437e-01\n",
      "\n",
      "Method: gd_scaling \n",
      "\n",
      "Iteration     0:  Gradient norm: 9.246165e+04\n",
      "Iteration     1:  Gradient norm: 4.936184e+02\n",
      "Iteration     2:  Gradient norm: 4.820494e+02\n",
      "Iteration     3:  Gradient norm: 4.710316e+02\n",
      "Iteration     4:  Gradient norm: 4.605269e+02\n",
      "Iteration     5:  Gradient norm: 4.505003e+02\n",
      "Iteration 100000:  Gradient norm: 2.058185e-01\n",
      "\n",
      "Method: gd_autocasting \n",
      "\n",
      "Iteration     0:  Gradient norm: 9.246165e+04\n",
      "Iteration     1:  Gradient norm: 4.940542e+02\n",
      "Iteration     2:  Gradient norm: 4.824626e+02\n",
      "Iteration     3:  Gradient norm: 4.714233e+02\n",
      "Iteration     4:  Gradient norm: 4.608983e+02\n",
      "Iteration     5:  Gradient norm: 4.508521e+02\n",
      "Iteration 100000:  Gradient norm: 2.058187e-01\n",
      "\n",
      "Method: initial_guess \n",
      "\n",
      "Iteration     0:  Gradient norm: 5.436564e+00\n",
      "Iteration     1:  Gradient norm: 5.435677e+00\n",
      "Iteration     2:  Gradient norm: 5.434790e+00\n",
      "Iteration     3:  Gradient norm: 5.433904e+00\n",
      "Iteration     4:  Gradient norm: 5.433019e+00\n",
      "Iteration     5:  Gradient norm: 5.432133e+00\n",
      "Iteration 100000:  Gradient norm: 1.840208e-01\n",
      "\n",
      "Method: signGD \n",
      "\n",
      "Iteration     0:  Gradient norm: 9.246165e+04\n",
      "Iteration     1:  Gradient norm: 9.240137e+04\n",
      "Iteration     2:  Gradient norm: 9.234112e+04\n",
      "Iteration     3:  Gradient norm: 9.228091e+04\n",
      "Iteration     4:  Gradient norm: 9.222075e+04\n",
      "Iteration     5:  Gradient norm: 9.216062e+04\n",
      "Iteration 100000:  Gradient norm: 4.234189e-12\n",
      "\n",
      "Method: gradient_change \n",
      "\n",
      "Iteration     0:  Gradient norm: 9.246165e+04\n",
      "Iteration     1:  Gradient norm: 8.370932e+04\n",
      "Iteration     2:  Gradient norm: 7.608416e+04\n",
      "Iteration     3:  Gradient norm: 6.940065e+04\n",
      "Iteration     4:  Gradient norm: 6.351972e+04\n",
      "Iteration     5:  Gradient norm: 5.831797e+04\n",
      "Iteration 100000:  Gradient norm: 2.692959e-01\n"
     ]
    }
   ],
   "source": [
    "f = lambda x : np.exp(x**2) - 1\n",
    "grad = lambda x : 2*x*np.exp(x**2)\n",
    "\n",
    "xmax = op.get_floating_point_parameters()['xmax']\n",
    "\n",
    "def gd_clipping(x, gamma=1e-5, double_precision=False):    \n",
    "    \n",
    "    g = grad(x)\n",
    "\n",
    "    g = g if abs(g) < xmax else xmax*np.sign(x)\n",
    "    \n",
    "    if not double_precision:\n",
    "        g = chop(g, op) # rounds the gradient to fp16 half precision (it stays a Python scalar, this example does not use LPV)\n",
    "    \n",
    "    return x - gamma*g\n",
    "\n",
    "def gd_scaling(x, gamma=1e-5, double_precision=False):    \n",
    "\n",
    "    scale = 2**10\n",
    "    g = grad(x)/scale\n",
    "    \n",
    "    if not double_precision:\n",
    "        g = chop(g, op) # rounds the gradient to fp16 half precision (it stays a Python scalar, this example does not use LPV)\n",
    "    \n",
    "    return x - (gamma*scale)*g\n",
    "\n",
    "def gd_autocasting(x, gamma=1e-5, double_precision=False):\n",
    "    \n",
    "    g = grad(x)\n",
    "    \n",
    "    if not double_precision and abs(g) < xmax:\n",
    "        g = chop(g, op) # rounds the gradient to fp16 half precision (it stays a Python scalar, this example does not use LPV)\n",
    "    \n",
    "    return x - gamma*g\n",
    "\n",
    "def signGD(x, gamma=1e-5, double_precision=False):\n",
    "    \n",
    "    g = grad(x)\n",
    "\n",
    "    g = np.sign(g)\n",
    "    \n",
    "    if not double_precision:\n",
    "        g = chop(g, op) # rounds the gradient to fp16 half precision (it stays a Python scalar, this example does not use LPV)\n",
    "    \n",
    "    return x - gamma*g\n",
    "\n",
    "new_grad = lambda x : x*np.exp(2*x)\n",
    "\n",
    "def gradient_change(x, gamma=1e-5, double_precision=False):\n",
    "    \n",
    "    g = new_grad(x)\n",
    "    \n",
    "    if not double_precision:\n",
    "        g = chop(g, op) # rounds the gradient to fp16 half precision (it stays a Python scalar, this example does not use LPV)\n",
    "    \n",
    "    return x - gamma*g\n",
    "\n",
    "for gd in [gd_clipping, gd_scaling, gd_autocasting, gd0, signGD, gradient_change]:\n",
    "\n",
    "    method_name = gd.__name__\n",
    "    if method_name == 'gd0': method_name = 'initial_guess'\n",
    "    \n",
    "    print(\"\\nMethod: \" + method_name, \"\\n\")\n",
    "    \n",
    "    x = 3.1 if method_name != 'initial_guess' else 1.0\n",
    "    gamma = 1e-5 if method_name != 'signGD' else 1e-4\n",
    "    max_its = 100000\n",
    "    for i in range(max_its+1):\n",
    "        if i%100000 == 0 or i <= 5:\n",
    "            print(\"Iteration %5d:\" % i, \" Gradient norm: %e\" % abs(grad(x)))\n",
    "\n",
    "        x = gd(x, gamma=gamma, double_precision=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuPy Kernel",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
