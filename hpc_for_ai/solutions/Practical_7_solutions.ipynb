{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7faf1cb7-6614-4396-bb5e-3434fa9e48b3",
   "metadata": {},
   "source": [
    "# High-performance and parallel computing for AI - Practical 7: MPI\n",
    "\n",
    "## NOTES\n",
    "\n",
    "* For these practicals we will be using a different `conda environment`. When opening a notebook or a terminal make sure you are using the **CuPy Kernel**!!!\n",
    "* MPI code must typically be invoked from the command line and not from a notebook. However, for this lectures I will be using a workaround. Both approaches are explained in the first MPI lecture Jupyter notebook.\n",
    "* mpi4py documentation [here](https://mpi4py.readthedocs.io/en/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96cbf52-cc6f-4687-a1b4-003f1005763b",
   "metadata": {},
   "source": [
    "## Question 1 - Finding what I need in the MPI documentation.\n",
    "\n",
    "In this question you will learn how to use the mpi4py documentation. Your taks is to open the [mpi4py docs](https://mpi4py.readthedocs.io/en/stable/index.html) and see whether you can find a list of all possible communication functions. Once you find the list, choose $2$-$3$ functions and click on them: it will show you the syntax for calling them and what they return. Finally, for each of these functions, open the official [MPI docs](https://www.mpi-forum.org/) and search for their syntax in the doc pdf. Alternatively, you can google \"MPI parallel FUNCTION_NAME\" and you will likely find useful results. Note that typically all MPI info is provided in either C/C++ or Fortran syntax, but it should be easy enough to understand.\n",
    "\n",
    "**Hint:** In the `mpi4py` docs you do not need to click on search. Remember that communication functions are member functions of the Communicator class, which is part of the `mpi4py.MPI` module, which you can find under \"Contents\" (the menu on the left in the documentation).\n",
    "\n",
    "**Note:** Remembering all MPI functions by heart is difficult if you do not use them often. I typically open the list of available functions and their documentation to remind myself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c520ea-6d78-42c9-8cc1-00112eb81ef8",
   "metadata": {},
   "source": [
    "## Practical 1 - Some reading\n",
    "\n",
    "Read and understand the following section about random number generation in a parallel environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421dec3-a5c9-429c-bfdd-b5286b8dbcb4",
   "metadata": {},
   "source": [
    "## Random number generation in a parallel environment\n",
    "\n",
    "Random numbers are used ubiquitously in AI:\n",
    "* Before training. Initialisation of neural network biases and weights requires random numbers.\n",
    "* During training. Some applications require selecting a random batch for stochastic gradient descent.\n",
    "* As part of the method, e.g., if you work with Gaussian mixture models or stochastic processes.\n",
    "In a parallel environment there is an additional complication arising when generating random numbers.\n",
    "\n",
    "**Computer-generated random numbers are not random**\n",
    "\n",
    "To begin with, note that random numbers are generated on your computer by a specific class of software called random number generators (RNGs). Numbers sampled from RNGs are not really random (non-quantum computers typically work deterministically so they cannot generate truly random sequences): they are only pseudo-random, a term which indicates that they are actually deterministic, yet carefully designed to behave as closely as possible like actual random numbers.\n",
    "\n",
    "**Generators and random seeds**\n",
    "\n",
    "RNGs are typically initialised with a *seed* that defines the initial *state* of the generator. From this initial state, RNGs generate a pseudo-random sequence which depends on the method used. All these sequences are typically periodic and have a *period* which indicates after how many numbers the sequence will repeat itself. The period is typically very large, e.g., $2^{128}$. Specifying a seed is very important for results reproducibility.\n",
    "\n",
    "**Why you need to be careful with MPI and RNGs**\n",
    "\n",
    "Here comes the problem with MPI: each process will have their own indentical copy of every variable including the initial RNG state, which means that every rank will typically have the same RNG generating the same exact numbers. This is a big issue since the generated numbers will be correlated, which is very bad for whatever method based on independent samples!\n",
    "\n",
    "**Possible generic solutions (software-dependent)**\n",
    "\n",
    "* **Skipahead.** This is the safest option. Some sequences allow to skip a chunk of numbers. This can be exploited by having each rank access the same sequence from a diffent initial state so that the generated numbers never overlap (provided the period is large enough and that the ranks are not too many this is typically the safest option).\n",
    "* **Change seed.** Potentially unsafe (software-dependent), but typically fine. This is not the optimal solution, but it is often good enough. If the RNG is initialised with a different seed for each rank, then the generated numbers are likely to be uncorrelated (but not guaranteed, hence why the other method is to be preferred).\n",
    "\n",
    "**Specific `numpy.random` solutions**\n",
    "\n",
    "The numpy team has thought long and hard about random number generation and the `numpy.random` module is now quite advanced. However, one needs to be aware and know how to use these functionalities.\n",
    "\n",
    "**Important:** Do not use `numpy.random.rand` and `numpy.random.randn` functions when using MPI. These are not supposed to be used in a parallel environment.\n",
    "\n",
    "When working with numpy you have multiple options:\n",
    "\n",
    "* **Recommended:** Using `spawn` to generate independent RNGs:\n",
    "```python\n",
    "    from mpi4py import MPI\n",
    "    import numpy as np\n",
    "\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "\n",
    "    seed = 1234 # whatever integer.\n",
    "    parent_RNG = np.random.default_rng(seed)\n",
    "    children_RNG = parent_RNG.spawn(size) # will create n=size independent RNGs.\n",
    "    RNG_to_use = children_RNG[rank] # only use the one corresponding to your rank.\n",
    "```\n",
    "\n",
    "* Using seeds which are guaranteed to lead to independent RNGs:\n",
    "```python\n",
    "    from mpi4py import MPI\n",
    "    import numpy as np\n",
    "    from secrets import randbits # NOTE: the name of this function changed to getrandbits in Python 3.13\n",
    "\n",
    "\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "    \n",
    "    root_seed = randbits(128) # this option requires generating high-quality seeds\n",
    "    RNG = default_rng([rank, root_seed]) # With high-probability the RNGs will now be independent.\n",
    "```\n",
    "\n",
    "* Using skipahead through the use of the `jumped` functionality:\n",
    "```python\n",
    "    from mpi4py import MPI\n",
    "    import numpy as np\n",
    "    from secrets import randbits # NOTE: the name of this function changed to getrandbits in Python 3.13\n",
    "\n",
    "\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()\n",
    "    size = comm.Get_size()\n",
    "    \n",
    "    root_seed = randbits(128) # this option requires generating high-quality seeds\n",
    "    base_RNG = default_rng(root_seed)\n",
    "    RNG = base_RNG.jumped(rank) # With high-probability the RNGs will now be independent.\n",
    "```\n",
    "This option requires understanding what happens under the hood. Perhaps I would not recommend it to a beginner.\n",
    "\n",
    "**Further reading:** Numpy parallel random number generation [documentation](https://numpy.org/doc/stable/reference/random/parallel.html).\n",
    "\n",
    "**General advice:** When using MPI with whatever RNG it is always *very important* to read the documentation of the random number generator to understand well how it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8704cc-17ef-43f9-aad1-76ec77642fa6",
   "metadata": {},
   "source": [
    "## Question 2 - Point-to-point communiation: Ping pong\n",
    "\n",
    "Write a simulation of Ping Pong according to the following rules:\n",
    "* Ranks 0 and 1 participate\n",
    "* Rank 0 starts with the ball\n",
    "* The rank with the ball sends it to the other rank\n",
    "* With probability 0.1 a rank misses and the other rank scores a point.\n",
    "* The rank that scores a point starts with the ball next.\n",
    "* The first rank to score 21 points wins.\n",
    "\n",
    "**Hint:** I did it as follows, but you can try with other strategies. You can send and receive a `ball` variable which takes value 1 if the ball is moving, 0 whenever the ball is missed, and -1 if one of the ranks has reached 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865944e7-0bd1-4dde-9030-f7c3db2939c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "from secrets import randbits\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "root_seed = randbits(128)\n",
    "base_RNG = np.random.default_rng(root_seed)\n",
    "RNG = base_RNG.spawn(size)[rank]\n",
    "\n",
    "score = 0\n",
    "hastheball = 0\n",
    "receivestheball = 1\n",
    "tag = 0\n",
    "ball = 1\n",
    "\n",
    "while ball != -1:\n",
    "    if rank == hastheball:\n",
    "        if score >= 21:\n",
    "            ball = -1\n",
    "        elif RNG.uniform() < 0.1:\n",
    "            ball = 0\n",
    "        comm.send(ball, dest=receivestheball, tag=tag)\n",
    "    else:\n",
    "        ball = comm.recv(source=hastheball, tag=tag)\n",
    "        if ball == 0:\n",
    "            score += 1\n",
    "            ball = 1\n",
    "            print(\"Rank %d scored a point! Current rank %d score is %d\" % (rank, rank, score), flush=True)        \n",
    "\n",
    "    hastheball,receivestheball = receivestheball,hastheball    \n",
    "    tag += 1\n",
    "\n",
    "if score >= 21:\n",
    "    print(\"Rank %d is the winner!\" % rank, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dcf6297-bf0a-4b7f-86f4-6ccfa742d401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 scored a point! Current rank 1 score is 1\n",
      "Rank 1 scored a point! Current rank 1 score is 2\n",
      "Rank 1 scored a point! Current rank 1 score is 3\n",
      "Rank 0 scored a point! Current rank 0 score is 1\n",
      "Rank 1 scored a point! Current rank 1 score is 4\n",
      "Rank 1 scored a point! Current rank 1 score is 5\n",
      "Rank 0 scored a point! Current rank 0 score is 2\n",
      "Rank 0 scored a point! Current rank 0 score is 3\n",
      "Rank 0 scored a point! Current rank 0 score is 4\n",
      "Rank 0 scored a point! Current rank 0 score is 5\n",
      "Rank 1 scored a point! Current rank 1 score is 6\n",
      "Rank 1 scored a point! Current rank 1 score is 7\n",
      "Rank 1 scored a point! Current rank 1 score is 8\n",
      "Rank 1 scored a point! Current rank 1 score is 9\n",
      "Rank 0 scored a point! Current rank 0 score is 6\n",
      "Rank 0 scored a point! Current rank 0 score is 7\n",
      "Rank 0 scored a point! Current rank 0 score is 8\n",
      "Rank 0 scored a point! Current rank 0 score is 9\n",
      "Rank 0 scored a point! Current rank 0 score is 10\n",
      "Rank 1 scored a point! Current rank 1 score is 10\n",
      "Rank 1 scored a point! Current rank 1 score is 11\n",
      "Rank 0 scored a point! Current rank 0 score is 11\n",
      "Rank 1 scored a point! Current rank 1 score is 12\n",
      "Rank 1 scored a point! Current rank 1 score is 13\n",
      "Rank 1 scored a point! Current rank 1 score is 14\n",
      "Rank 1 scored a point! Current rank 1 score is 15\n",
      "Rank 0 scored a point! Current rank 0 score is 12\n",
      "Rank 1 scored a point! Current rank 1 score is 16\n",
      "Rank 1 scored a point! Current rank 1 score is 17\n",
      "Rank 0 scored a point! Current rank 0 score is 13\n",
      "Rank 1 scored a point! Current rank 1 score is 18\n",
      "Rank 0 scored a point! Current rank 0 score is 14\n",
      "Rank 1 scored a point! Current rank 1 score is 19\n",
      "Rank 0 scored a point! Current rank 0 score is 15\n",
      "Rank 0 scored a point! Current rank 0 score is 16\n",
      "Rank 1 scored a point! Current rank 1 score is 20\n",
      "Rank 0 scored a point! Current rank 0 score is 17\n",
      "Rank 0 scored a point! Current rank 0 score is 18\n",
      "Rank 1 scored a point! Current rank 1 score is 21\n",
      "Rank 1 is the winner!\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 2 python3 temp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e0457-4798-48d9-9046-dab54a39df02",
   "metadata": {},
   "source": [
    "## Question 3 - Sharing the workload across workers\n",
    "\n",
    "**Note:** For this question you do not need `mpi4py` nor `mpiexec`. Only use `numpy`.\n",
    "\n",
    "Let $a\\in\\mathbb{R}^n$ be given by `a = np.arange(n)`. Write a Python script that divides the entries of $a$ as equally as possible into $m$ chunks (think of each chunk has being held on one rank). Then sum the entries of each of the $m$ chunks of $a$ and store the result into the variables $s_i$ for $i=1,\\dots,m$ (in MPI there would be only one $s_i$ on each rank). Finally, sum \n",
    "the $s_i$ together and double check that the answer you obtain is correct (in MPI this final sum would be a reduce operation). Code this up with $n=59$ and $m=4$. If you can, avoid ever creating the full array $a$.\n",
    "\n",
    "**Note:** This is a very common operation. If you need to divide $n$ tasks equally across $m$ processors this is what you have to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c2832d-7f23-4a97-934f-805cfb05f7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 0. Value: 105\n",
      "Rank: 1. Value: 330\n",
      "Rank: 2. Value: 555\n",
      "Rank: 3. Value: 721\n",
      "Value: 1711 Correct value: 1711\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 59\n",
    "size = 4\n",
    "\n",
    "k = n//size # integer division\n",
    "howmany = np.concatenate([[0],np.cumsum([k + int(i < n%size) for i in range(size)])])\n",
    "\n",
    "s = 0\n",
    "for rank in range(size): # if you had used mpi4py, then you wouldn't have needed the for loop\n",
    "    out = np.sum(np.arange(howmany[rank],howmany[rank+1]))\n",
    "    s += out\n",
    "    print(\"Rank: %d. Value: %d\" % (rank, out))\n",
    "\n",
    "print(\"Value:\", s, \"Correct value:\", (n*(n-1))//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abdef1-ca67-412a-a3dd-5a8b951cab3d",
   "metadata": {},
   "source": [
    "## Question 4 - Monte Carlo simulation\n",
    "\n",
    "Let $z$ be an $n$-dimensional standard Gaussian vector. Use MPI to estimate $\\mathbb{E}[\\lVert z \\rVert_2^2]$ for $n=10$ by using the Monte Carlo method (what is the exact answer?). Use 4 processes and take $1000$ samples per process so that they overall Monte Carlo estimator will be an average of $M=4000$ samples. Implement this by using collective MPI communication. Do it in two ways: 1) So that each process has the value of the final Monte Carlo mean. 2) So that only rank 0 has it instead. Think (but do not do it) about how you would change the code if the question had asked for a total of $4003$ samples (we saw this in the previous question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d446a841-2d1e-432a-89ad-f855a0f5f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "n = 10 # dimension of the Gaussian vector\n",
    "m = 1000 # samples per rank\n",
    "M = m*size # total number of samples\n",
    "\n",
    "RNG = np.random.default_rng(123*rank) # initialise the RNG\n",
    "z = RNG.standard_normal(size=(n,m)) # Sample random numbers\n",
    "\n",
    "s = np.sum(z**2)\n",
    "\n",
    "# Everyone will have it\n",
    "s1 = comm.allreduce(s, op=MPI.SUM)/M\n",
    "\n",
    "# Only rank 0 will have it. Everyone else will have None.\n",
    "s2 = comm.reduce(s, op=MPI.SUM, root=0)\n",
    "if s2 is not None:\n",
    "    s2 /= M\n",
    "\n",
    "print(\"allreduce. Rank %d. Estimated mean:\" % rank, s1) # exact answer is 10. It won't be exactly 10, but close.\n",
    "\n",
    "print(\"reduce. Rank %d. Estimated mean:\" % rank, s2) # exact answer is 10. It won't be exactly 10, but close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa816e2-68a2-41f3-9127-2100dfb936c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allreduce. Rank 1. Estimated mean: 10.045318325834492\n",
      "reduce. Rank 1. Estimated mean: None\n",
      "allreduce. Rank 2. Estimated mean: 10.045318325834492\n",
      "reduce. Rank 2. Estimated mean: None\n",
      "allreduce. Rank 3. Estimated mean: 10.045318325834492\n",
      "reduce. Rank 3. Estimated mean: None\n",
      "allreduce. Rank 0. Estimated mean: 10.045318325834492\n",
      "reduce. Rank 0. Estimated mean: 10.045318325834492\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python3 temp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692964df-5fb3-46b0-9bfe-3f8bf4f13e76",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "In this questions you will use 4 processes. Have rank 1 create a random array of length $1000$ and then use the buffer version of scatter to divide this vector across all $4$ processes. Then use gather (buffer version) on rank 3 to reconstruct the array on rank 3. Then, use send and recv to have rank 3 send the gathered array directly to rank 1 and have rank 1 check that it matches with the original array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202d13fb-912f-475f-8d0c-65a5827d37fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "assert size == 4\n",
    "\n",
    "N = 1000\n",
    "M = N//size\n",
    "\n",
    "data = None\n",
    "if rank == 1:\n",
    "    data = np.random.randn(N).reshape((size, M))\n",
    "    original_data = data.copy()\n",
    "elif rank == 3:\n",
    "    data = np.empty((size, M))\n",
    "\n",
    "recvbuf = np.empty((M,))\n",
    "\n",
    "comm.Scatter(data, recvbuf, root=1)\n",
    "comm.Gather(recvbuf, data, root=3)\n",
    "\n",
    "if rank == 3:\n",
    "    comm.Send(data, dest=1, tag=123)\n",
    "elif rank == 1:\n",
    "    comm.Recv(data, source=3, tag=123)\n",
    "\n",
    "    if not np.allclose(data, original_data):\n",
    "        comm.Abort()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b334eda-7f39-45a0-833d-f3ff48bf3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -n 4 python3 temp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e246e8-ba2a-49bf-847d-ea0e2f4b2135",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Use $n$ processes with $n=3$. Create a list $a$ of $n$ Python objects of your choice on rank 0, then use scatter to send each object to a different rank. Then, implement an allgather operation \"by hand\" by combining gather with bcast on root 0 and check that the answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f93273-e158-4524-89f2-6360dbd5ad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "assert size == 3\n",
    "\n",
    "a = None\n",
    "if rank == 0:\n",
    "    a = ['this', 'is', 'fun']\n",
    "\n",
    "a = comm.scatter(a, root=0)\n",
    "b = comm.gather(a, root=0)\n",
    "b = comm.bcast(b, root=0)\n",
    "\n",
    "print(\"Rank %d\" % rank, b, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b20d4a-9ac6-4574-a62b-45c47f7541a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 ['this', 'is', 'fun']\n",
      "Rank 1 ['this', 'is', 'fun']\n",
      "Rank 2 ['this', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 3 python3 temp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c6e2e-0b96-48a2-a31a-30007f562017",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "This is to show that sum reduction operations in object mode will apply the Python `+` operation, whatever it means for that object.\n",
    "\n",
    "Use $n$ processes with $n=3$. On each rank, create a list with $1$-$2$ objects of your choice, then use allreduce with `MPI.SUM` so that each process obtains a list with all objects. This is because in Python a sum of lists is a list concatenation operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca13f9aa-c78f-46aa-be1d-6710f71b5176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "a = ['this%d' % rank, 'is%d' % rank, 'fun%d' % rank]\n",
    "\n",
    "b = comm.allreduce(a, op=MPI.SUM)\n",
    "\n",
    "print(\"Rank %d\" % rank, b, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33193cc-ab38-418f-8972-8522b254944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 ['this0', 'is0', 'fun0', 'this1', 'is1', 'fun1', 'this2', 'is2', 'fun2']\n",
      "Rank 1 ['this0', 'is0', 'fun0', 'this1', 'is1', 'fun1', 'this2', 'is2', 'fun2']\n",
      "Rank 2 ['this0', 'is0', 'fun0', 'this1', 'is1', 'fun1', 'this2', 'is2', 'fun2']\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 3 python3 temp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291f414-98c5-4db2-b468-f0507733cb98",
   "metadata": {},
   "source": [
    "## Question 8 - Parallel matrix-vector product\n",
    "\n",
    "For $n=100$ let $A$ be an $n$-by-$n$ square matrix and $b$ be a length $n$ vector. For this question only use $2$ processes.\n",
    "\n",
    "Do the following:\n",
    "* Construct $A$ and $b$ on rank 0, then use scatter to divide the rows of $A$ and the entries of $b$ equally across the two ranks.\n",
    "* Write a function that implements a parallel matrix vector product between $A$ and $b$ so that the resulting vector is also divided equally across the two processors. To minimize memory movement, never communicate matrix entries between ranks.\n",
    "* Check that what you have computed is correct (perhaps by keeping a copy of the correct matvec done in serial on rank 0).\n",
    "\n",
    "Finally, repeat the exercise by dividing the columns of $A$ instead. You can do this in (at least) two ways: either with an `Allreduce` or an `Alltoall`. Try both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2a30987-6ed8-44e8-b416-a656763a31a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting temp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile temp.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "assert size == 2\n",
    "\n",
    "n = 100\n",
    "\n",
    "################## DIVIDING ROWS ##################\n",
    "\n",
    "Af,bf = None,None\n",
    "if rank == 0:\n",
    "    Af = np.random.randn(n,n)\n",
    "    bf = np.random.randn(n)\n",
    "    ex = Af@bf\n",
    "    Af = Af.reshape((size, (n*n)//size))\n",
    "    bf = bf.reshape((size, n//size))\n",
    "\n",
    "b = np.empty((n//2,))\n",
    "A = np.empty((n//2, n))\n",
    "\n",
    "comm.Scatter(bf, b, root=0)\n",
    "comm.Scatter(Af, A, root=0)\n",
    "\n",
    "def matvec(A,b):\n",
    "    bf = np.empty(n)\n",
    "    comm.Allgather(b, bf)\n",
    "    c = A@bf\n",
    "    return c\n",
    "\n",
    "c = matvec(A,b)\n",
    "\n",
    "# for checking\n",
    "cf = None\n",
    "if rank == 0:\n",
    "    cf = np.empty((n,))\n",
    "    \n",
    "comm.Gather(c, cf, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    assert np.allclose(cf, ex)\n",
    "\n",
    "################## DIVIDING COLUMNS ##################\n",
    "\n",
    "Af,bf = None,None\n",
    "if rank == 0:\n",
    "    Af = np.random.randn(n,n)\n",
    "    bf = np.random.randn(n)\n",
    "    ex = Af@bf\n",
    "    Af = Af.T.reshape((size, (n*n)//size))\n",
    "    bf = bf.T.reshape((size, n//size))\n",
    "\n",
    "# Same as before, but now A contains the columns (the rows of A.T)\n",
    "b = np.empty((n//2,))\n",
    "A = np.empty((n//2, n))\n",
    "\n",
    "comm.Scatter(bf, b, root=0)\n",
    "comm.Scatter(Af, A, root=0)\n",
    "\n",
    "#A B  b  | Ab + Bc\n",
    "#C D  c  | Cb + Dc\n",
    "\n",
    "def matvec1(A,b):\n",
    "    c = A.T@b\n",
    "    cf = np.empty(n)\n",
    "    comm.Alltoall(c, cf)\n",
    "    out = cf[:n//2] + cf[n//2:]\n",
    "    return out\n",
    "\n",
    "def matvec2(A,b):\n",
    "    c = A.T@b\n",
    "    cf = np.empty(n)\n",
    "    comm.Allreduce(c, cf, op=MPI.SUM)\n",
    "    out = cf[:n//2] if rank == 0 else cf[n//2:]\n",
    "    return out\n",
    "\n",
    "c1 = matvec1(A,b)\n",
    "c2 = matvec2(A,b)\n",
    "\n",
    "# for checking\n",
    "cf1,cf2 = None,None\n",
    "if rank == 0:\n",
    "    cf1 = np.empty((n,))\n",
    "    cf2 = np.empty((n,))\n",
    "    \n",
    "comm.Gather(c1, cf1, root=0)\n",
    "comm.Gather(c2, cf2, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    assert np.allclose(cf1, ex)\n",
    "    assert np.allclose(cf2, ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8afe22f-e8eb-4b89-ac1d-e0a455069f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec -n 2 python3 temp.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuPy Kernel",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
