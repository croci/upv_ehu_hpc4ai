{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68281218-b119-4671-a586-5d8f3019bf21",
   "metadata": {},
   "source": [
    "High-performance and parallel computing for AI - Practical 1: HPC Architecture\n",
    "==============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1c7e8-2389-412c-9644-bab3729b6c16",
   "metadata": {},
   "source": [
    "IMPORTANT!\n",
    "==========\n",
    "\n",
    "* For these practicals we will be using a different `conda environment`. When opening a notebook or a terminal make sure you are using the **CuPy Kernel**!!!\n",
    "* Do these practicals at your own pace. Solutions will be provided on the same day or on the day after the practical. Do not worry if you do not finish everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae4df79-3978-4919-8018-d5a1559e7749",
   "metadata": {},
   "source": [
    "Question 1\n",
    "----------\n",
    "\n",
    "This question is to help you understand the architecture of a computing server.\n",
    "\n",
    "In Jupyter, open a new window and open a terminal. On the terminal, type\n",
    "\n",
    "```bash\n",
    "lscpu\n",
    "```\n",
    "\n",
    "This outputs a plethora of information about goliat's CPU. Can you answer the following questions?\n",
    "\n",
    "1- Who is the CPU manufacturer? Which year was this CPU commercialized (you will have to google the CPU model)?\n",
    "\n",
    "2- How many cores does the CPU have? Be careful, google what hyperthreading is (goliat has it enabled).\n",
    "\n",
    "3- What is the largest square matrix of doubles (64 bits) that you can fit in the L1i cache?\n",
    "\n",
    "4- Assuming a similar architecture as the one we saw in the lectures in which the L3 cache is shared. What is the largest double (64-bit) square matrix size that you can fit on a socket without spilling over to the RAM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c86fce-5a7e-446a-91f2-0d8f7e5c127b",
   "metadata": {},
   "source": [
    "Question 2\n",
    "----------\n",
    "\n",
    "This question is to teach you how to compute whether a computation is compute or memory bound.\n",
    "\n",
    "Assume you are using a CPU with a peak computational performance of 500 GFLOPs/s and a memory bandwidth of 50 GB/s (this could roughly be a laptop's CPU). Let $n=1024$ and take $A,B\\in\\mathbb{R}^{n\\times n}$, $v\\in\\mathbb{R}^n$. Assume they are stored in single precision (32 bits/number). Are the following computations memory- or compute-bound?\n",
    "\n",
    "1- The matrix-vector product $Av$.\n",
    "\n",
    "2- The matrix-matrix product $AB$.\n",
    "\n",
    "Draw a roofline plot. Where would these computations be on the plot? What would happen if the matrices and vectors were booleans (e.g., only 1 bit)?\n",
    "\n",
    "\n",
    "Hints: First, compute the FLOPs of each operation, then the memory occupied by each variable. Second, compute the arithmetic intensity (FLOPs/memory required). Third, draw the roofline plot. Lastly, add the arithmetic intensities of the operations (include them by drawing a vertical line: the problem does not give you the GFLOPS performance of these operations, you would need to test it in practice). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fbe315-44db-4b10-918b-74e37dd77bab",
   "metadata": {},
   "source": [
    "Question 3\n",
    "----------\n",
    "\n",
    "For this question we will be using numpy and [cupy](https://cupy.dev/)\n",
    "\n",
    "In this question we investigate the cost of data movement. To move data between CPUs in Python you would need to know MPI, which will be taught later on in the course. Here, we move data between CPUs and GPUs instead using cupy.\n",
    "\n",
    "* Read and understand the code below. Run it (possibly twice to make sure there is no caching). Do you expect matrix multiplications to be faster on the CPU or on the GPU? Do you expect it to be faster to move data to or from the GPU (or the same)?\n",
    "* Create a random square matrix $A$ and a vector $v$ of size $n=1024$ on the CPU (using numpy) and on the GPU (using cupy, it does not matter if the values are not the same). Then write three functions: 1) A function that computes $v\\cdot v$ on the CPU using the CPU variables. 2) A function that computes $v\\cdot v$ on the GPU using the GPU variables and then returns the answer to the CPU. 3) A function that copies $v$ onto the GPU, computes $v\\cdot v$ on the GPU, and then returns the answer to the CPU. Time all functions using 10 runs each. Remember to run it twice and to only take the second timing. Please add the lines\n",
    "```python\n",
    "mempool = cp.get_default_memory_pool()\n",
    "mempool.set_limit(size=1.5*1024**3)  # 1.5 GB limit on GPU memory usage.\n",
    "```\n",
    "before your code to make sure we do not overuse the GPU memory.\n",
    "\n",
    "* Repeat the above, but with $Av$.\n",
    "* Repeat the above, but with $A^2$.\n",
    "* Looking at the timings what do you observe? When is it worth it to move data to perform computations on the GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0f6838-325e-4458-8a18-14e371f3609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time matmat CPU: 0.00620572566986084\n",
      "Time matmat GPU: 0.007312226295471192\n",
      "Time CPU to GPU: 0.0027472972869873047\n",
      "Time GPU to CPU: 0.0009063482284545898\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from time import time\n",
    "\n",
    "mempool = cp.get_default_memory_pool()\n",
    "mempool.set_limit(size=1.5*1024**3)  # 1.5 GiB\n",
    "\n",
    "N = 1024\n",
    "\n",
    "x  = np.random.randn(N, N)\n",
    "xg = cp.random.randn(N, N)\n",
    "\n",
    "# Matrix multiplication on CPU\n",
    "tic = time()\n",
    "for i in range(10):\n",
    "    x@x\n",
    "\n",
    "t = (time()-tic)/10\n",
    "print(\"Time matmat CPU:\", t)\n",
    "\n",
    "# Matrix multiplication on GPU\n",
    "tic = time()\n",
    "for i in range(10):\n",
    "    xg@xg\n",
    "\n",
    "t = (time()-tic)/10\n",
    "print(\"Time matmat GPU:\", t)\n",
    "\n",
    "# CPU to GPU data movement\n",
    "tic = time()\n",
    "for i in range(10):\n",
    "    yg = cp.asarray(x) # moves a numpy array stored on the CPU onto the GPU\n",
    "    \n",
    "t = (time()-tic)/10\n",
    "print(\"Time CPU to GPU:\", t)\n",
    "\n",
    "# GPU to CPU data movement.\n",
    "tic = time()\n",
    "for i in range(10):\n",
    "    y = cp.asnumpy(xg) # moves a cupy array stored on the GPU onto the CPU\n",
    "    \n",
    "t = (time()-tic)/10\n",
    "print(\"Time GPU to CPU:\", t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuPy Kernel",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
