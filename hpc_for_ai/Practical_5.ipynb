{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab2b62f-c9f3-44a6-911c-d5fdbda375f9",
   "metadata": {},
   "source": [
    "High-performance and parallel computing for AI - Practical 5: Reduced- and mixed-precision computing\n",
    "===================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c481355-b47f-47b0-83f4-c7322b8a7129",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "=========\n",
    "\n",
    "For these practicals we will be using a different `conda environment`. When opening a notebook or a terminal make sure you are using the **CuPy Kernel**!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df621af-a236-43c8-bd06-cb3de61488cc",
   "metadata": {},
   "source": [
    "Question 1\n",
    "----------\n",
    "\n",
    "The `cast` function provided below takes an `LPV` array and changes its format to a new format specified by an `Option` `op`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30adfa0-0ea8-44a6-ab5f-e7c0e6eceb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141592653589793 3.1415927410125732 3.140625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chopping import *\n",
    "\n",
    "def cast(a, op):\n",
    "    return LPV(a.array(), op)\n",
    "\n",
    "opb = Option(True)\n",
    "opb.set_format('b') # bfloat16 half precision\n",
    "\n",
    "op = Option(True)\n",
    "op.set_format('s') # single precision\n",
    "\n",
    "a = np.array([np.pi])\n",
    "aa = LPV(a, op) # single precision\n",
    "ab = cast(aa, opb) # bfloat16 half precision. Note: this will round the array from single to bfloat16.\n",
    "\n",
    "print(a[0], aa.array()[0], ab.array()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66fc096-21d3-4e9c-b444-b658ac68db74",
   "metadata": {},
   "source": [
    "Use the cast function to define a new function `c = mpp(a,b)` which takes two half-precision `LPV` variables `a` and `b` as inputs and computes their dot product using single precision, returning a single-precision variable `c` as output.\n",
    "\n",
    "Let $x,a,b\\in\\mathbb{R}^n$ be given in numpy by `x = np.linspace(0,1,n)`, `a = np.exp(x)`, `b = np.sin(np.pi*x)**2`. Compute the dot product $a\\cdot b$ using only half precision and using the `mpp` function you just wrote. By evaluating the exact dot product in double precision, compute the resulting relative rounding error for both approaches. Plot (in loglog scale) the error as a function of $n$. Are the results consistent with what we have learnt in the lectures?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304da780-8bd4-4409-be8d-fca01d2c3e7e",
   "metadata": {},
   "source": [
    "Question 2 - Mixed-precision and overflow\n",
    "-----------------------------------------\n",
    "\n",
    "Repeat Question 5 in Practical 4, but now accumulate and store the sum using single precision (use the `cast` function). Does the computation overflow at the value computed in Question 5 of Practical 4?\n",
    "\n",
    "For which theoretical value will the calculation overflow now? You can compute this value numerically, but, please, DO NOT try to make it overflow in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0b4f1-739a-491e-b59d-7d1140a93b87",
   "metadata": {},
   "source": [
    "Question 3 - Mixed-precision Gradient Descent\n",
    "----------------------------------------------\n",
    "\n",
    "Implement the gradient descent method in double precision for minimizing the 2D Rosenbrock function:\n",
    "\n",
    "$$ f(x,y) = (a-x)^2 + b(y-x^2)^2 $$\n",
    "\n",
    "where $a=\\pi-3$, $b=100$. This function has a unique global minimum at the point $(a, a^2)$. For the method, set the learning rate $\\gamma = 10^{-3}$, the initial condition to zero, and the maximum number of iterations to $2 \\times 10^4$. \n",
    "\n",
    "1- Implement the scheme using only double precision (i.e., no need for the `chopping` library).\n",
    "\n",
    "2- Implement the scheme using only bfloat16 half precision.\n",
    "\n",
    "3- Implement the scheme in mixed precision by evaluating gradients in double precision and then casting the result to bfloat16. Use bfloat16 for storing the current iterate as well.\n",
    "\n",
    "4- Same as 3, but evaluate gradients in single precision rather than double (use numpy for this, no need to use the `chopping` library).\n",
    "\n",
    "In order to monitor the convergence behaviour of these methods, compute the Euclidean norm of the gradient at the current iterate every $1000$ iterations. Run all four implementations and compare their gradient norms. Can you explain this convergence behaviour based on what we have learnt in the lectures?\n",
    "\n",
    "**Hint:** The functions provided below may be useful. It is not necessary, but feel free to modify them at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52db18a6-d371-4d45-aeb7-f049d9ef47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chopping import *\n",
    "\n",
    "op = Option(True)\n",
    "op.set_format(\"b\")\n",
    "op.set_round(1)\n",
    "u = op.get_floating_point_parameters()[\"u\"] # roundoff unit\n",
    "\n",
    "# Define rosenbrock function, its gradient and Hessian, and its exact solution\n",
    "a = np.pi-3\n",
    "b = 100\n",
    "rosenbrock = lambda x,y : (a-x)*(a-x) + b*(y - x*x)*(y - x*x)\n",
    "grad = lambda x,y : [-2*a - 4*b*x*(-x*x + y) + 2*x, b*(-2*x*x + 2*y)]\n",
    "hess = lambda x,y : [[2*(4*b*x*x + 2*b*(x*x - y) + 1), -4*b*x], [-4*b*x, 2*b]] # the Hessian is not really needed here\n",
    "ex_sol = np.array([a, a**2])\n",
    "\n",
    "# Computes the gradient in single precision and casts the output to double. Feel free to modify this.\n",
    "def grad_single(x,y):\n",
    "    return list(np.array(grad(x.astype(np.float32), y.astype(np.float32))).astype(np.float64))\n",
    "\n",
    "# libchopping was not really designed for this so we define some utilitiy functions\n",
    "# to cast to and from LPV objects\n",
    "no_lpv = lambda z : (z[0].array()[0], z[1].array()[0]) if isinstance(z[0], LPV) else z\n",
    "to_lpv = lambda z : (LPV(np.array([z[0]]), op), LPV(np.array([z[1]]), op)) if not isinstance(z[0], LPV) else z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433ddee-3ca4-4eb1-82d9-e3918aca9597",
   "metadata": {},
   "source": [
    "Question 4 (Optional)\n",
    "---------------------\n",
    "\n",
    "**Note:** This question is optional, only work on this if you are done with everything and you still have time.\n",
    "\n",
    "Repeat Question 3, but now implement Newton's method rather than gradient descent. Here you can play with the precision of three quantities: the iterate, the gradient, the Hessian (and its inverse). How do all possible precision choices affect convergence? For this question $10$ iterates is enough.\n",
    "\n",
    "**Hint:** The Hessian inverse times the gradient can be computed by hand since the Hessian is a $2$-by-$2$ matrix:\n",
    "\n",
    "```python\n",
    "    # computes H^{-1}g. Feel free to modify it at will\n",
    "    A = H[0][0]\n",
    "    B = H[0][1]\n",
    "    D = H[1][1]\n",
    "    det = A*D - B*B\n",
    "    iH = [[D/det, -B/det], [-B/det, A/det]]\n",
    "    out = [iH[0][0]*g[0] + iH[0][1]*g[1], iH[1][0]*g[0] + iH[1][1]*g[1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132ea51-544c-4056-8108-6c6c0881781a",
   "metadata": {},
   "source": [
    "Question 5 - Scaling\n",
    "--------------------\n",
    "\n",
    "In this problem we will investigate the use of mixed-precision scaling techniques in a simplified setting. Consider the following optimization problem:\n",
    "\n",
    "$$ \\min_x f(x),\\quad\\text{where}\\quad f(x) = e^{x^2} - 1. $$\n",
    "\n",
    "This problem is strongly convex and has a unique global minimum at $x=0$, therefore gradient descent will converge to the global minimum in exact arithmetic and as the number of iterations approach infinity.\n",
    "\n",
    "Consider the following gradient descent code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fe5bd7-9952-4510-bed2-1cfd82152234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     0:  Gradient norm: 9.246165e+04\n",
      "Iteration     1:  Gradient norm: 4.940542e+02\n",
      "Iteration     2:  Gradient norm: 4.824614e+02\n",
      "Iteration     3:  Gradient norm: 4.714230e+02\n",
      "Iteration     4:  Gradient norm: 4.608996e+02\n",
      "Iteration     5:  Gradient norm: 4.508556e+02\n",
      "Iteration 10000:  Gradient norm: 3.157494e+00\n",
      "Iteration 20000:  Gradient norm: 1.664137e+00\n",
      "Iteration 30000:  Gradient norm: 1.112368e+00\n",
      "Iteration 40000:  Gradient norm: 8.137534e-01\n",
      "Iteration 50000:  Gradient norm: 6.226381e-01\n",
      "Iteration 60000:  Gradient norm: 4.885887e-01\n",
      "Iteration 70000:  Gradient norm: 3.892610e-01\n",
      "Iteration 80000:  Gradient norm: 3.130816e-01\n",
      "Iteration 90000:  Gradient norm: 2.533453e-01\n",
      "Iteration 100000:  Gradient norm: 2.058187e-01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chopping import *\n",
    "\n",
    "op = Option(True)\n",
    "op.set_format('h') # fp16 half precision\n",
    "\n",
    "f = lambda x : np.exp(x**2) - 1\n",
    "grad = lambda x : 2*x*np.exp(x**2)\n",
    "\n",
    "def gd0(x, gamma=1e-5, double_precision=False):    \n",
    "    \n",
    "    g = grad(x)\n",
    "    \n",
    "    if not double_precision:\n",
    "        g = chop(g, op) # rounds the gradient to fp16 half precision (it stays a Python scalar, this example does not use LPV)\n",
    "    \n",
    "    return x - gamma*g\n",
    "\n",
    "x = 3.1\n",
    "gamma = 1e-5\n",
    "max_its = 100000\n",
    "for i in range(max_its+1):\n",
    "    if i%10000 == 0 or i <= 5:\n",
    "        print(\"Iteration %5d:\" % i, \" Gradient norm: %e\" % abs(grad(x)))\n",
    "\n",
    "    x = gd0(x, gamma=gamma, double_precision=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793025a3-a0ec-49d0-bf2e-6c9c2ef9894e",
   "metadata": {},
   "source": [
    "Read the code and understand what it does, then change the `double_precision` flag to `False` and observer that the method blows up.\n",
    "\n",
    "This happens because the function gradient is very large at the initial condition and causes an overflow in fp16 half precision.\n",
    "\n",
    "By only changing either the `grad` function or by modifying the computed gradient before or after `chop` is called, try to make gradient descent converge in half-precision by preventing an overflow. You have different options (try them all, they are quick):\n",
    "\n",
    "1- **Gradient clipping.** Set the gradient to the maximum value in the half-precision range before it is rounded.\n",
    "\n",
    "2- **Gradient scaling.** Divide the gradient by a scalar $s$ in high precision before rounding and then multiply `gamma` by $s$ before the update. Note that gradient scaling works a bit differently in practice, but this gives you an idea.\n",
    "\n",
    "3- **Autocasting.** Only for this one, do not apply the function `chop` if the computed gradient is beyond `xmax`, the largest number representable in float16 half precision.\n",
    "\n",
    "4- **Initialisation.** Change the initial guess. Note that this is much trickier in AI, but there is lots of literature on good initialization techniques.\n",
    "\n",
    "5- **SignGD.** This is a crazy one: replace the gradient with its sign before the chop. Here you can set the learning rate `gamma` to be $10$ times larger.\n",
    "\n",
    "6- **Change gradient.** This is somehow related to the previous one. Replace the gradient function with another function that has smaller magnitude, but has the same sign as the gradient. Note: this is just for fun, it will work here, but it is something that is impossible to do (or at least very very hard) for more complicated problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuPy Kernel",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
