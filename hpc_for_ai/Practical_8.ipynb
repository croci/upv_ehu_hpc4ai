{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30064e54-b029-4b4d-934a-717b160469cb",
   "metadata": {},
   "source": [
    "# High-performance and parallel computing for AI - Practical 8: NVIDIA-SMI, environment variables, and CuPy programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431376a-ccf3-4363-bb6f-bf439327b872",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "=========\n",
    "\n",
    "* CuPy behaves weirdly for me. Restart the kernel if you encounter weird errors.\n",
    "* For these practicals we will be using a different `conda environment`. When opening a notebook or a terminal make sure you are using the **CuPy Kernel**!!!\n",
    "* It's fine if you do not finish everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5cfde-69b2-466d-967f-d773eba57b3f",
   "metadata": {},
   "source": [
    "## Question 1 - CUDA driver and available GPUs\n",
    "\n",
    "Open the launcher, open a terminal, and enter the bash command `nvidia-smi`. This command gives you information about the installed CUDA driver version, the GPUs available on the system (our server, Goliat), and their current utilization.\n",
    "\n",
    "The three NVIDIA L40S (48 GB RAM) are the most powerful GPUs available. Can you find the following information online?\n",
    "\n",
    "* Number of CUDA cores.\n",
    "* Number of Tensor cores.\n",
    "* Number of SMs.\n",
    "* Size of L1 SM-shared cache.\n",
    "* Size of L2 GPU-shared cache.\n",
    "\n",
    "Use the above to roughly compute the number of CUDA cores and tensor cores per SM.\n",
    "\n",
    "Then, run the command `nvidia-smi -q`. It will output lots of information about the GPUs. For the next question we will need the GPU UUID, which is needed to select the GPUs we want to use. You can obtain these more easily via `grep`: Run the command `nvidia-smi -q | grep UUID` and check that the first two correspond to the Quadro GPU and the latter three to the L40s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888116a-89b5-469b-ba3e-8083162580a9",
   "metadata": {},
   "source": [
    "## Question 2 - Memory movement in CuPy - Part 2\n",
    "\n",
    "Before you start (and before running any other GPU code on the servers) please run the following code, which limits the maximum GPU memory usage to $1.5$ GB and picks an L40s GPU and a Quadro GPU at random. **Please only run the code below once every time you restart the kernel!** \n",
    "\n",
    "While it won't hurt to understand what the whole code does, it is very useful to at least understand the GPU selection part and the setting of the `CUDA_VISIBLE_DEVICES` environment variable since this is useful for whatever GPU software, not just CuPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04aeea5d-0afe-402f-8988-e1dce92543d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CuPy-specific environment variables\n",
    "os.environ[\"CUPY_GPU_MEMORY_LIMIT\"] = \"1573741824\" # roughly 1.5 GB\n",
    "os.environ[\"CUPY_ACCELERATORS\"]=\"cutensor\" # activates cutensor acceleration\n",
    "os.environ[\"CUPY_TF32\"] = \"1\" # activates tf32 tensor cores\n",
    "\n",
    "## On goliat we have FIVE GPUs so here we pick two of those at random\n",
    "## so that we do not overload the system.\n",
    "## The way we do it is by figuring out the GPU UUIDs and then setting\n",
    "## The CUDA_VISIBLE_DEVICES environment variable.\n",
    "## Note: this is useful for other libraries as well (e.g., Jax, PyTorch, TF) in multi-GPU servers.\n",
    "\n",
    "# To get these UUIDs you need to run nvidia-smi -q on the command line\n",
    "quadro_UUIDs = [\"GPU-4efa947b-abbd-7c6e-84f5-61241d34bb4b\",\n",
    "                \"GPU-5eb524b0-2b1b-fe98-e6ed-b8fb5185e993\"]\n",
    "\n",
    "L40s_UUIDs = [\"GPU-7bba1f33-03d2-016b-d42e-ced83c3ac243\",\n",
    "              \"GPU-179d068a-3bea-91d7-1a8c-7017f55d6298\",\n",
    "              \"GPU-ae634859-dd49-de46-9182-195639405eaa\"]\n",
    "\n",
    "from numpy.random import randint\n",
    "# Picks an L40s and a Quadro GPU at random. The others will be invisible to CuPy\n",
    "# NOTE: this only works if the environment variable is set BEFORE CuPy is first imported.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = L40s_UUIDs[randint(3)] + \",\" + quadro_UUIDs[randint(2)]\n",
    "\n",
    "## CuPy will only see these GPUs and will assign them these device numbers:\n",
    "L40sID = 0\n",
    "quadro_ID = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d5443-078f-4ebf-ab2b-66661cd2668d",
   "metadata": {},
   "source": [
    "First of all, give a quick look to Q3 in Practical 1 to remember how to move data between host and device (i.e., CPU and GPU). You won't be needing this here, but it is good to remember.\n",
    "\n",
    "Then, create two random $n$-by-$n$ single precision matrices on the L40s GPU, where $n=4096$. Then compute and time their matrix product. Finally move both matrices to one of the Quadro GPUs and perform and time the matmat again. Which one is faster?\n",
    "\n",
    "Finally, change the dtype so that computations use double precision. Time everything again. How do the timings change? This is why working in reduced precision is important!\n",
    "\n",
    "### Hints - Please read\n",
    "\n",
    "**Hint 1:** To figure out which cupy device id the Quadro GPU are use the above `quadro_device_ids` list. To check a cupy array current device, use `myarray.device` (it will return the GPU number). In Practical 1 we saw how we move an array from the CPU to the GPU by using `cp.asarray`. This function can also be used to move data to another GPU, e.g.,\n",
    "```python\n",
    "# Make sure to keep track of where arrays live in memory!!!\n",
    "with cp.cuda.Device(0):\n",
    "    x_gpu_0 = cp.ndarray([1, 2, 3])  # create an array in GPU 0\n",
    "\n",
    "with cp.cuda.Device(1):\n",
    "    x_gpu_1 = cp.asarray(x_gpu_0)  # move the array to GPU 1 \n",
    "```\n",
    "\n",
    "**Hint 2:** I recommend using `with cp.cuda.Device(deviceID):` to ensure computations are actually done with device number `deviceID` and not with another one. Any change in device will trigger memory movement to and from devices which is slow.\n",
    "\n",
    "**Hint 3:** To time each operation, put it into a function and use `cupyx.profiler.benchmark`, e.g.,\n",
    "```python\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "def my_func(a):\n",
    "    return cp.sqrt(cp.sum(a**2, axis=-1))\n",
    "\n",
    "a = cp.random.random((256, 1024))\n",
    "print(benchmark(my_func, (a,), n_repeat=128, devices=(a.device,)))\n",
    "```\n",
    "Note that benchmark will output the time spent by both CPU and GPU.\n",
    "\n",
    "**Hint 4:** It is always veery good practice to use variable names which help reminding you where is the variable stored in memory (e.g., use `_h` for host, `_d` for device)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caffe728-9049-498a-89e2-4b919db3db9f",
   "metadata": {},
   "source": [
    "## Tutorial 1 - An overview of CuPy kernels\n",
    "\n",
    "Here you will learn a bit about how to write your own kernels in CuPy. It is good to be aware of these although my personal opinion is that they are not that great. Note that:\n",
    "\n",
    "* You can always use CuPy built-in numpy-like functions without having to write any kernels.\n",
    "* CuPy kernel writing functionalities are many, but most are limited and poorly documented.\n",
    "* I am not sure I would recommend using CuPy kernels unless you use the Raw CUDA kernels (see below) or you are trying to do something simple.\n",
    "\n",
    "To learn more, see the [official CuPy docs](https://docs.cupy.dev/en/latest/user_guide/kernel.html#).\n",
    "\n",
    "### Possible CuPy Kernel writing strategies with my personal comments\n",
    "\n",
    "* **Elementwise and reduction kernels (poor documentation)**. These let you write and JIT your own kernels using a simplified syntax. However, the set of allowed operations and syntax is poorly documented. There is some mention of having to use CUDA C/C++ fragments. Very unclear so I would skip this.\n",
    "\n",
    "* **Kernel fusion (limited, but easy)**. This option is easy to use, yet its functionality is limited. CuPy provides a decorator, `cupy.fuse` which you can use to turn a set of CuPy operations into a single kernel which will then be JIT-ed and should then run faster. For instance:\n",
    "```python\n",
    "import cupy as cp\n",
    "@cp.fuse\n",
    "def myfun(x, y):\n",
    "    return cp.sin(x - y)*x + cp.exp(y)\n",
    "```\n",
    "However, important operations such as `matmul` are unsupported. Worth keeping in mind, but not great.\n",
    "\n",
    "* **Raw Kernels (Powerful, but requires C/C++)**. This option lets you write your own CUDA C/C++ kernels which will then be JITed so that you can pass cupy arrays into it. This is an easy option for wrapping CUDA code into Python and it is powerful since you can wrap whatever you want. However, it requires knowing how to write CUDA C/C++ code.\n",
    "\n",
    "* **JIT Rawkernels (Perhaps a good compromise)**. This option is similar to the way numba works. CuPy provides a JIT decorator - `cupyx.jit.rawkernel` - which can be used like `numba.jit` to turn a Python function into a CUDA kernel which is then JIT-ed. The good thing is that doing so allows the user to write CUDA-like code without ever leaving Python and that CuPy maths functions can be used. The problem is that documentation is poor, array operations are unsupported, and the functionality is marked as experimental.\n",
    "\n",
    "Example of `cupyx.jit.rawkernel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b9f5eb-0d51-4be8-97e4-3548f8ffca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/cupy/lib/python3.12/site-packages/cupyx/jit/_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from cupyx import jit\n",
    "\n",
    "@jit.rawkernel()\n",
    "def myfun(x, y, size): # can pass arrays and scalar variables\n",
    "    tid = jit.blockIdx.x * jit.blockDim.x + jit.threadIdx.x # thread ID as in CUDA\n",
    "    ntid = jit.gridDim.x * jit.blockDim.x # total number of threads in the grid/stride\n",
    "    for i in range(tid, size, ntid):\n",
    "        y[i] = cp.sin(x[i])**2 # y gets overwritten\n",
    "\n",
    "gridDim = (128,)\n",
    "blockSize = (1024,)\n",
    "\n",
    "n = gridDim[0]*blockSize[0]*2**5\n",
    "x = cp.random.normal(size=(n,), dtype=cp.float32)\n",
    "y = cp.zeros((n,), dtype=cp.float32)\n",
    "\n",
    "# Kernel invocation as CuPy RawKernels\n",
    "myfun(gridDim, blockSize, (x, y, n))  \n",
    "assert (cp.sin(x)**2 == y).all()\n",
    "\n",
    "# Kernel invocation as in Numba-CUDA\n",
    "myfun[gridDim[0], blockSize[0]](x, y, n)\n",
    "assert (cp.sin(x)**2 == y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c139587-2e16-49dc-aa25-5b4b6dd64f1e",
   "metadata": {},
   "source": [
    "## Question 3 - Custom CuPy kernels\n",
    "\n",
    "Write two CuPy kernels (one via `cupy.fuse` and the other via JIT-rawkernel) that compute the entrywise cosine of a matrix entrywise product between two square single-precision matrices of size $n=4096$ (intialise the matrix entries at random). For the JIT-rawkernel version, use entrywise for loops and 2D grid and block sizes with `gridDim = (64, 64)` and `blockSize = (32, 32)`. Time both functions using `cupyx.profiler.benchmark`.\n",
    "\n",
    "Then, modify the JIT-rawkernel version:\n",
    "* Try exchanging the order of the for loop. One of the two versions will be faster. Why?\n",
    "* Try playing with the gridDim and (reducing the) blockSize and see how the timings change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5f68f-7e6a-4246-b949-4c15a999f8fe",
   "metadata": {},
   "source": [
    "## Question 4 - Reductions\n",
    "\n",
    "Implementing reductions in CUDA is non-trivial. We won't cover it in detail in these lectures, but luckily we can use CuPy built-in reduction kernels to do the work for us.\n",
    "\n",
    "Write a kernel which does the same computation as in the previous exercise, but this time it returns the sum all the entries of the output. Again you have multiple ways of doing it. Do it in four ways:\n",
    "\n",
    "* Using a lambda function and standard CuPy operations (i.e., `cp.cos(a*b).sum()`).\n",
    "* Using `cupy.fuse`.\n",
    "* Applying `cp.sum` to what the JIT-Rawkernel computes.\n",
    "* (Optional) Using a CuPy [ElementwiseKernel](https://docs.cupy.dev/en/latest/user_guide/kernel.html#basics-of-elementwise-kernels) followed by a CuPy [ReductionKernel](https://docs.cupy.dev/en/latest/user_guide/kernel.html#reduction-kernels).\n",
    "\n",
    "Time all of them. Which one is faster? For me surprisingly the fuse and the Elementwise/Reduction kernel options are extremely slow. I do not know why, but this shows that these options are not great in general even though they are an important part of CuPy key documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuPy Kernel",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
