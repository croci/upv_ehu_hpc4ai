{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec3111e-7fa5-427b-9ac2-1900b51b1bdf",
   "metadata": {},
   "source": [
    "High-performance and parallel computing for AI - Practical 4: Floating-point arithmetic and rounding error analysis\n",
    "==================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6228d-246e-4e65-a335-d17a3a4689e0",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "=========\n",
    "\n",
    "For these practicals we will be using a different `conda environment`. When opening a notebook or a terminal make sure you are using the **CuPy Kernel**!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e37fa6d-331d-4534-9bf4-1a8779fefb77",
   "metadata": {},
   "source": [
    "Premise\n",
    "=======\n",
    "\n",
    "Goliat's hardware has limited support for reduced-precision arithmetic and the Python software support is likewise incomplete (this is a problem which we would not have if we were to use C++ or Julia on more recent hardware). Therefore, for this practical we will be using a *floating-point emulator*, which is a software tool that entirely works in double precision, but automatically rounds numbers as if computations were done in a lower-precision format. Note that **emulation is slow**: you are essentially applying a nonlinear function at every operation you perform. Bottom line: avoid crazy computations.\n",
    "\n",
    "In particular, we will be using the `chopping` library. This is a software I wrote so:\n",
    "\n",
    "1- Hopefully there won't be any bugs, but there might be.\n",
    "\n",
    "2- Not all functionalities may be present.\n",
    "\n",
    "3- I should be able to fix most issues.\n",
    "\n",
    "**IMPORTANT:** The point of this practical is not to learn how to use the library, but simply to learn about rounding errors, reduced- and mixed-precision computations, and floating-point arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd01642-a9af-4a31-81bb-1561071d9594",
   "metadata": {},
   "source": [
    "Tutorial 1\n",
    "----------\n",
    "\n",
    "Read the following code (including the comments) and try to understand how the `chopping` library works. You can open a console if you want (remember to use the CuPy kernel) to play with the library.\n",
    "\n",
    "First, we import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8d27d7-0811-4f08-b6fd-2f3ab0dae0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ea499-6393-42e8-b931-4158ee3458c5",
   "metadata": {},
   "source": [
    "Second, we import the `chopping` library as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec375376-4d4e-40e5-ae9f-3a085591058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chopping import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cbd55f-4e5f-4e72-ade2-024097bf9fd6",
   "metadata": {},
   "source": [
    "The first thing to do when using `chopping` is to define an Option variable which defines the floating-point format we want to use and the way numbers will be rounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1bd0ba4-4811-49c0-bb3c-e7e7e13e5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = Option()\n",
    "# Set floating point format. Supported formats:\n",
    "# \"h\" for fp16, \"b\" for bf16, \"s\" for single, \"d\" for double.\n",
    "# Can also define custom formats, but this is for another day\n",
    "op.set_format(\"h\")\n",
    "# Set rounding mode. Supported modes:\n",
    "# 1 - Round-to-nearest\n",
    "# 2 - Round up\n",
    "# 3 - Round down\n",
    "# 4 - Round towards 0\n",
    "# 5 - Unbiased stochastic rounding\n",
    "# 6 - Biased stochastic rounding\n",
    "op.set_round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca53af-bb3a-4f04-b19c-d1de40c3a4a2",
   "metadata": {},
   "source": [
    "You can obtain info about the floating point format chosen with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa1fe1d-b71c-4398-b37d-346ea1da4d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 11, 'u': 0.00048828125, 'emax': 15, 'emin': -14, 'emins': -24, 'xmax': 65504.0, 'xmin': 6.103515625e-05, 'xmins': 5.960464477539063e-08}\n"
     ]
    }
   ],
   "source": [
    "print(op.get_floating_point_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2022eb85-2803-408a-8baa-1bef42e64141",
   "metadata": {},
   "source": [
    "The `chopping` library works via *operator overloading*. It implements a new array class, called `LPV` (it stand for \"low precision variable\") which looks like a very limited version of `numpy.ndarray`. This class requires an `Option` variable so that it is aware of how it should round things. When `LPV` arrays are used in arithmetic operations, the result is automatically rounded so that you do not need to worry about the rounding yourself.\n",
    "\n",
    "An `LPV` object is defined by an underlying numpy array and an Option: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b72b54-d8c7-478c-ad33-c71bf53c2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can only wrap numpy arrays\n",
    "A = np.random.randn(5,4)\n",
    "Ar = LPV(A, op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c9a9a-2862-499b-a5ce-98da49b3432b",
   "metadata": {},
   "source": [
    "`Ar` is now an LPV variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94101f7f-ea57-44c1-926f-6eeaeceb47a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chopping.LPV"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c3fe2-9c73-4c0d-86d4-e33c3f741dbb",
   "metadata": {},
   "source": [
    "basic arithmetic operations are supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d49b5c-1c40-450d-8666-c90ca6025f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'chopping.LPV'>\n",
      "Exact:\n",
      " [[-1.09235609 -0.6587883   2.15514362  0.72898289]\n",
      " [-0.37980895 -0.76137605 -0.33892428 -0.10008303]\n",
      " [-1.80111793 -0.63093532  1.26057337 -2.17724014]\n",
      " [-0.12459186  0.32682657 -1.98867638  0.55509363]\n",
      " [-0.88334936  0.629848    0.0753798   0.97994233]]\n",
      "Reduced-precision:\n",
      " LPV_array([[-1.09277344, -0.65869141,  2.15429688,  0.72900391],\n",
      "           [-0.37988281, -0.76123047, -0.33886719, -0.10009766],\n",
      "           [-1.80078125, -0.63085938,  1.26074219, -2.17773438],\n",
      "           [-0.12457275,  0.3269043 , -1.98828125,  0.55517578],\n",
      "           [-0.88330078,  0.62988281,  0.07537842,  0.97998047]])\n",
      "<class 'chopping.LPV'>\n",
      "Exact:\n",
      " [[ 6.80330393  0.11308294  3.5126608  -3.9604403   1.42681104]\n",
      " [ 0.11308294  0.8488346   0.95512563  0.41693843 -0.26767082]\n",
      " [ 3.5126608   0.95512563  9.971525   -3.69724641 -0.84492499]\n",
      " [-3.9604403   0.41693843 -3.69724641  4.38530142  0.70996293]\n",
      " [ 1.42681104 -0.26767082 -0.84492499  0.70996293  2.14298368]]\n",
      "Reduced-precision:\n",
      " LPV_array([[ 6.80078125,  0.11352539,  3.51367188, -3.95898438,  1.42773438],\n",
      "           [ 0.11352539,  0.84863281,  0.95507812,  0.41650391, -0.26757812],\n",
      "           [ 3.51367188,  0.95507812,  9.96875   , -3.69726562, -0.84667969],\n",
      "           [-3.95898438,  0.41650391, -3.69726562,  4.3828125 ,  0.70996094],\n",
      "           [ 1.42773438, -0.26757812, -0.84667969,  0.70996094,  2.14257812]])\n"
     ]
    }
   ],
   "source": [
    "C = (A + A)/2\n",
    "Cr = (Ar + Ar)/2\n",
    "\n",
    "print(type(Ar*Ar))\n",
    "\n",
    "print(\"Exact:\\n\", C)\n",
    "print(\"Reduced-precision:\\n\", Cr)\n",
    "\n",
    "C = C@C.T\n",
    "Cr = Cr@Cr.T() # for LPV arrays .T() is a function so you need to call it\n",
    "\n",
    "print(type(Cr))\n",
    "\n",
    "print(\"Exact:\\n\", C)\n",
    "print(\"Reduced-precision:\\n\", Cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a418b5-3430-4715-945b-d4c1fb329b23",
   "metadata": {},
   "source": [
    "However, **array slicing and numpy functions (including powers) are not supported**.\n",
    "\n",
    "Nevertheless, there is a workaround: The underlying numpy array can be extracted with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16087d88-65d2-4f40-a4a3-fe49fcf8992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Underlying array:\n",
      " [[-1.09277344 -0.65869141  2.15429688  0.72900391]\n",
      " [-0.37988281 -0.76123047 -0.33886719 -0.10009766]\n",
      " [-1.80078125 -0.63085938  1.26074219 -2.17773438]\n",
      " [-0.12457275  0.3269043  -1.98828125  0.55517578]\n",
      " [-0.88330078  0.62988281  0.07537842  0.97998047]]\n"
     ]
    }
   ],
   "source": [
    "Ar_value = Ar.array()\n",
    "print(\"Underlying array:\\n\", Ar_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b744d6-cb70-4b9a-8457-03d6ede67cc3",
   "metadata": {},
   "source": [
    "And more complicated numpy functions can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a3cd86-2a87-4819-87a5-61038ee12f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complicated numpy functions are not supported, but there is a workaround:\n",
    "sinCr = LPV(np.sin(Cr.array()), Cr.option)\n",
    "# However, note that this means that the function is applied in double precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe0adb-67ec-45e2-b3b9-db5387242c98",
   "metadata": {},
   "source": [
    "Precision and/or rounding mode mixing is NOT directly allowed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f877ff6-ee0d-4446-baec-051c583b60da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You shouldn't have done this!\n"
     ]
    }
   ],
   "source": [
    "op2 = Option()\n",
    "op2.set_format(\"b\")\n",
    "op2.set_round(5)\n",
    "\n",
    "Ar = LPV(A, op)\n",
    "Br = LPV(A, op2)\n",
    "\n",
    "try:\n",
    "    Ar + Br\n",
    "except ValueError:\n",
    "    print(\"You shouldn't have done this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9990391-c4e4-4c1f-ab2c-0bdbf6d27a5e",
   "metadata": {},
   "source": [
    "However, you can still do it by casting one array to the other format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9b548cf-ffac-4ee6-8be9-5532ccf6307b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LPV_array([[-2.171875  , -1.3125    ,  4.3125    ,  1.4609375 ],\n",
       "           [-0.7578125 , -1.515625  , -0.67578125, -0.20019531],\n",
       "           [-3.59375   , -1.2578125 ,  2.53125   , -4.34375   ],\n",
       "           [-0.24902344,  0.65625   , -3.96875   ,  1.109375  ],\n",
       "           [-1.765625  ,  1.2578125 ,  0.15039062,  1.9609375 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LPV(Ar.array(), op2) + Br # performs the addition in bf16."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbddbf-a8ac-44d1-8a9a-78f57b3e1966",
   "metadata": {},
   "source": [
    "Finally. If really needed, can do the rounding by hand without using the LPV class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "168a0768-7f87-4b24-b3b3-636ea3d82437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005413414796065511\n",
      "0.000967653589793116\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(10)\n",
    "b = a.copy()\n",
    "chop(b, op) # rounding done in-place\n",
    "print(np.linalg.norm(a-b))\n",
    "\n",
    "# chop also works with non-numpy inputs (unlike LPV) and in this case it returns a copy\n",
    "a = np.pi\n",
    "b = chop(a, op)\n",
    "print(abs(a-b))\n",
    "\n",
    "# If you use scalars and want to do things by hand you can simply define\n",
    "c = lambda x : chop(x, op)\n",
    "b = c(c(3)*c(np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba6fcd-a88d-490a-a41d-0a9a7fdad46d",
   "metadata": {},
   "source": [
    "Practical 2 - Rounding error behaviour\n",
    "--------------------------------------\n",
    "\n",
    "** **Cancellation** **\n",
    "\n",
    "In this excercise we will investigate the phenomenon of cancellation. Cancellation is what happens when two nearly equal numbers are subtracted and it is often, but not always, a bad thing.\n",
    "\n",
    "Consider the quantity\n",
    "\n",
    "$$f(x) = \\frac{1-\\cos(x)}{x^2}$$.\n",
    "\n",
    "If we evaluate it in single precision at $x=3 \\times 10^{-4}$ we obtain: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c9ff5d8-df37-4fb3-94c3-7ccaebb24670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact: [0.5] \n",
      "Single precision: [0.66227376]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chopping import *\n",
    "\n",
    "op = Option()\n",
    "op.set_format('s')\n",
    "\n",
    "def f(x):\n",
    "    c = LPV(np.cos(x.array()), x.option) if isinstance(x,LPV) else np.cos(x)\n",
    "    return (1-c)/(x*x) # pow is not implemented so need to do x**2 by hand\n",
    "\n",
    "y = np.array([3e-4])\n",
    "x = LPV(y, op)\n",
    "\n",
    "print(\"Exact:\", f(y), \"\\nSingle precision:\", f(x).array())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983a436-fcf7-4014-b630-2b885d2d0e21",
   "metadata": {},
   "source": [
    "Note that $0\\leq f(x) \\leq 0.5$ for all $x$ so the single precision result is super wrong. What is happening?\n",
    "\n",
    "Here, the exact value of $\\cos(x)$ is very close to $1$, namely:\n",
    "\n",
    "$$\\cos(x) = 0.99999995500000033749999898750000162723214122991071539519074620460454744421,$$\n",
    "\n",
    "while $1 - \\cos(x)$ is given by\n",
    "\n",
    "$$1 - \\cos(x) = 4.499999966250000101249999837276785877008928460480925379539545255579 \\times 10^{-8}.$$\n",
    "\n",
    "Since $\\cos(x)$ is of order $1$, rounding errors will be of order $1\\times u$, which for single precision is roughly $2^{-24}=6\\times 10^{-8}$. Hence, rounding errors are of the same order of $1 - \\cos(x)$ and we cannot trust that our computations will approximate the numerator correctly (essentially we have almost no digits of accuracy). If we weren't dividing by $x^2$ this would not be a problem since we would notice that what we obtained is zero to within the machine precision. However, we do divide by $x^2$ which is extremely small and amplifies the error in the numerator.\n",
    "\n",
    "This phenomenon is called *catastrophic error cancellation*. It occurs when terms which are really close to each other are subtracted and the result is fed to a routine that amplifies the resulting error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f97988-6e56-4529-9bab-7ef83318e8e9",
   "metadata": {},
   "source": [
    "Question 1\n",
    "----------\n",
    "\n",
    "Use the relation $\\cos(x) = 1 - 2\\sin^2(x/2)$ to rewrite the function $f(x)$. Re-evaluate it in single precision. The result is now correct. Why?\n",
    "\n",
    "What happens if you evaluate the original $f(x)$ at $x=10^{-4}$? This is also the wrong answer. Can you (handwavily) explain why you obtain this value? \n",
    "\n",
    "(Theoretical) Let $a,b\\in \\mathbb{R}$ and assume we compute instead $\\hat{a}=a(1+\\delta_a)$, $\\hat{b}=b(1+\\delta_b)$, where $|\\delta_a|,|\\delta_b| \\leq u$. Let $x = a - b$ and $\\hat{x} = \\hat{a} - \\hat{b}$. Can you provide an upper bound to the relative (forward) error $|x - \\hat{x}|/|x|$ in terms of $u$, $a$, and $b$? Assuming that this bound should be in the form $\\kappa(a,b)u$ what is the value of $\\kappa(a,b)$? Taking $\\kappa(a,b)$ to be the *condition number* of the computation of $x$ what can you tell about the conditioning of this computation when $a$ is close to $b$ and $|a|\\gg 0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540aef08-6495-43b5-a880-ef7d091b8f18",
   "metadata": {},
   "source": [
    "Question 2\n",
    "----------\n",
    "\n",
    "Consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37923fb4-b997-4772-a90a-a16b23c2dd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial x: [100.    0.9] \n",
      "After applying the identity function: [1. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGwCAYAAABPSaTdAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJ1tJREFUeJzt3X90VPWdxvEnCWQSMIkokhAMJlq2GJEEEpMNVK3HaIqUPWyPuxQpsFHp6mIbmLVbQEmK/Ig/SkrFQAot6ukRRaWybXVpMUWRQwRMiCtVoFTZUCABimZC1AQzs390mWl2AiWTIXf48H6dM+cwN/fOfDIq8/h97tyJ8vl8PgEAABgV7fQAAAAA5xNhBwAAmEbYAQAAphF2AACAaYQdAABgGmEHAACYRtgBAACm9XF6AKd5vV4dPnxYCQkJioqKcnocAABwDnw+n1paWpSamqro6LOv3Vz0Yefw4cNKS0tzegwAABCCgwcP6sorrzzrPhd92ElISJD0lxcrMTHR4WkAAMC58Hg8SktL87+Pn81FH3ZOV1eJiYmEHQAALjDncgoKJygDAADTCDsAAMA0wg4AADCNsAMAAEwj7AAAANMIOwAAwDTCDgAAMI2wAwAATCPsAAAA0wg7AADAtIgKO1u2bNGECROUmpqqqKgobdiw4W8e88Ybb2j06NFyuVz60pe+pGeeeea8zwkAAC4cERV2WltblZWVpcrKynPa/6OPPtL48eN1yy23qL6+XrNmzdK9996r3/zmN+d5UgAAcKGIqC8CHTdunMaNG3fO+1dVVSkjI0NLly6VJF177bXaunWrfvSjH6moqOh8jWlbe7t05IjTUwAALHG5pJQUx54+osJOd9XU1KiwsLDTtqKiIs2aNeuMx7S1tamtrc1/3+PxnK/xLjxffCFlZkp//KPTkwAALCkokLZtc+zpL+iw09jYqOTk5E7bkpOT5fF49Nlnnyk+Pj7omPLyci1YsKC3RrywnDgRCDpxcc7OAgCwIzbW0ae/oMNOKObOnSu32+2/7/F4lJaW5uBEEeqzz5yeAACAsLigw05KSoqampo6bWtqalJiYmKXqzqS5HK55HK5emO8C4/P5/QEAACEXUR9Gqu7CgoKVF1d3Wnbpk2bVFBQ4NBEAAAg0kRU2Dl58qTq6+tVX18v6S8fLa+vr1dDQ4Okv1RQ06ZN8+9/33336cMPP9R//Md/aM+ePVqxYoVefPFFzZ4924nx7YiKcnoCAADCJqLCzjvvvKNRo0Zp1KhRkiS3261Ro0aptLRUknTkyBF/8JGkjIwMvfrqq9q0aZOysrK0dOlS/fSnP+Vj56GixgIAGBTl813c73Aej0dJSUlqbm5WYmKi0+M4q7FRGjz4Lys7Xq/T0wAAcEbdef+OqJUdRAhqLACAIYQdBFzci3wAAKMIOwAAwDTCDoJRYwEADCHsIIAaCwBgEGEHAACYRthBMGosAIAhhB0EUGMBAAwi7AAAANMIOwhGjQUAMISwgwBqLACAQYQdAABgGmEHwaixAACGEHYQQI0FADCIsAMAAEwj7CAYNRYAwBDCDgKosQAABhF2AACAaYQdBKPGAgAYQthBADUWAMAgwg4AADCNsINg1FgAAEMIOwigxgIAGETYAQAAphF2EHB6ZYcaCwBgCGEHAACYRtgBAACmEXYQQI0FADCIsAMAAEwj7CAYKzsAAEMIOwjgOjsAAIMIOwAAwDTCDoJRYwEADCHsIIAaCwBgEGEHAACYRthBMGosAIAhhB0EUGMBAAwi7AAAANMIOwhGjQUAMISwgwBqLACAQYQdAABgGmEHwaixAACGEHYQQI0FADCIsAMAAEwj7CAYNRYAwBDCDgKosQAABhF2AACAaYQdBKPGAgAYQthBADUWAMAgwg4AADCNsINg1FgAAEMIOwigxgIAGETYAQAAphF2EIwaCwBgCGEHAdRYAACDCDsAAMA0wg6CUWMBAAyJuLBTWVmp9PR0xcXFKT8/Xzt27Djr/suWLdOXv/xlxcfHKy0tTbNnz9bnn3/eS9MaQ40FADAoosLOunXr5Ha7VVZWprq6OmVlZamoqEhHjx7tcv+1a9dqzpw5Kisr0wcffKCf/exnWrdunebNm9fLkwMAgEgVUWGnoqJCM2bMUHFxsTIzM1VVVaV+/fppzZo1Xe6/bds2jR07VnfddZfS09N1++23a/LkyWddDWpra5PH4+l0w/85vbJDjQUAMCRiwk57e7tqa2tVWFjo3xYdHa3CwkLV1NR0ecyYMWNUW1vrDzcffvihXnvtNd1xxx1nfJ7y8nIlJSX5b2lpaeH9RQAAQETp4/QApx0/flwdHR1KTk7utD05OVl79uzp8pi77rpLx48f11e+8hX5fD598cUXuu+++85aY82dO1dut9t/3+PxEHj+P1Z2AACGRMzKTijeeOMNLVmyRCtWrFBdXZ1+8Ytf6NVXX9XChQvPeIzL5VJiYmKnG/4PJygDAAyKmJWdgQMHKiYmRk1NTZ22NzU1KSUlpctj5s+fr6lTp+ree++VJF1//fVqbW3Vt7/9bT300EOKjr6gsxwAAAiDiEkDsbGxysnJUXV1tX+b1+tVdXW1CgoKujzm008/DQo0MTExkiQfqxSho8YCABgSMSs7kuR2uzV9+nTl5uYqLy9Py5YtU2trq4qLiyVJ06ZN05AhQ1ReXi5JmjBhgioqKjRq1Cjl5+dr//79mj9/viZMmOAPPegGAiIAwKCICjuTJk3SsWPHVFpaqsbGRmVnZ2vjxo3+k5YbGho6reQ8/PDDioqK0sMPP6xDhw7piiuu0IQJE7R48WKnfgUAABBhonwXed/j8XiUlJSk5uZmTlZ+910pO1saPFg6fNjpaQAAOKPuvH9HzDk7iAAXd+4FABhF2AEAAKYRdhCMT2MBAAwh7CCAGgsAYBBhBwAAmEbYQTBqLACAIYQdBFBjAQAMIuwAAADTCDsIRo0FADCEsIMAaiwAgEGEHQAAYBphB8GosQAAhhB2EECNBQAwiLADAABMI+wgGDUWAMAQwg4CqLEAAAYRdgAAgGmEHQSjxgIAGELYQQA1FgDAIMIOAAAwjbCDYNRYAABDCDsIoMYCABhE2AEAAKYRdhBwemWHGgsAYAhhBwAAmEbYQTBWdgAAhhB2EMAJygAAgwg7AADANMIOglFjAQAMIewggBoLAGAQYQcAAJhG2EEwaiwAgCGEHQRQYwEADCLsAAAA0wg7CEaNBQAwhLCDAGosAIBBhB0AAGAaYQfBqLEAAIYQdhBAjQUAMIiwAwAATCPsIBg1FgDAEMIOAqixAAAGEXYAAIBphB0Eo8YCABhC2EEANRYAwCDCDgAAMI2wg2DUWAAAQwg7CKDGAgAYRNgBAACmEXYQjBoLAGAIYQcB1FgAAIMIOwAAwDTCDoJRYwEADCHsIIAaCwBgEGEHAACYRthBwOmVHWosAIAhERd2KisrlZ6erri4OOXn52vHjh1n3f+TTz7RzJkzNXjwYLlcLv3d3/2dXnvttV6aFgAARLo+Tg/w19atWye3262qqirl5+dr2bJlKioq0t69ezVo0KCg/dvb23Xbbbdp0KBBevnllzVkyBD9z//8jy699NLeHx4AAESkiAo7FRUVmjFjhoqLiyVJVVVVevXVV7VmzRrNmTMnaP81a9boxIkT2rZtm/r27StJSk9PP+tztLW1qa2tzX/f4/GE7xe40FFjAQAM6lGNderUKR08eFB79+7ViRMnejRIe3u7amtrVVhYGBguOlqFhYWqqanp8phf/vKXKigo0MyZM5WcnKwRI0ZoyZIl6ujoOOPzlJeXKykpyX9LS0vr0dwAACCydTvstLS0aOXKlbr55puVmJio9PR0XXvttbriiit01VVXacaMGdq5c2e3Bzl+/Lg6OjqUnJzcaXtycrIaGxu7PObDDz/Uyy+/rI6ODr322muaP3++li5dqkWLFp3xeebOnavm5mb/7eDBg92e1TxWdgAAhnSrxqqoqNDixYt1zTXXaMKECZo3b55SU1MVHx+vEydOaPfu3Xrrrbd0++23Kz8/X8uXL9ewYcPO1+zyer0aNGiQVq1apZiYGOXk5OjQoUN64oknVFZW1uUxLpdLLpfrvM10QeM6OwAAg7oVdnbu3KktW7bouuuu6/LneXl5uvvuu1VVVaWnn35ab7311jmHnYEDByomJkZNTU2dtjc1NSklJaXLYwYPHqy+ffsqJibGv+3aa69VY2Oj2tvbFRsbe46/GQAAsKpbNdbzzz/vDzotLS1n3M/lcum+++7T3Xfffc6PHRsbq5ycHFVXV/u3eb1eVVdXq6CgoMtjxo4dq/3798vr9fq37du3T4MHDybo9AQ1FgDAkJBPUL7xxhvPeC5NqNxut1avXq1nn31WH3zwge6//361trb6P501bdo0zZ0717///fffrxMnTqikpET79u3Tq6++qiVLlmjmzJlhneuiQY0FADAo5LAzatQo5efna8+ePZ2219fX64477gjpMSdNmqQf/vCHKi0tVXZ2turr67Vx40b/ScsNDQ06cuSIf/+0tDT95je/0c6dOzVy5Eh997vfVUlJSZcfUwcAABenKJ8v9P+dLysrU2VlpTZs2KBBgwbp4Ycf1vr163XHHXfoV7/6VTjnPG88Ho+SkpLU3NysxMREp8dx1m9/KxUVSdnZ0q5dTk8DAMAZdef9u0cXFVywYIFcLpduu+02dXR06NZbb1VNTY3y8vJ68rBwCjUWAMCgkGuspqYmlZSUaNGiRcrMzFTfvn31L//yLwQdAAAQUUIOOxkZGdqyZYteeukl1dbWav369fr2t7+tJ554IpzzwQl8GgsAYEjINdaaNWv0zW9+03//a1/7mjZv3qyvf/3rOnDggCorK8MyIHoRNRYAwKCQV3b+OuicNnr0aG3btk2/+93vejQUAABAuPToi0C7kp6erm3btoX7YdGbqLEAAIZ0K+w0NDSc034DBgyQJB06dKj7E8E51FgAAIO6FXZuuOEG/eu//utZv9W8ublZq1ev1ogRI7R+/foeDwgAANAT3TpB+f3339fixYt12223KS4uTjk5OUpNTVVcXJw+/vhjvf/++/r973+v0aNH6/HHHw/5SspwGDUWAMCQbq3sXH755aqoqNCRI0f01FNPadiwYTp+/Lj+8Ic/SJKmTJmi2tpa1dTUEHQuRNRYAACDQvroeXx8vO68807deeed4Z4HAAAgrEL+NNb06dO1ZcuWcM6CSEGNBQAwJOSw09zcrMLCQg0bNkxLlizhk1cWUGMBAAwKOexs2LBBhw4d0v33369169YpPT1d48aN08svv6xTp06Fc0YAAICQ9eiigldccYXcbrfeffddbd++XV/60pc0depUpaamavbs2f4Tl3GBocYCABgSlisoHzlyRJs2bdKmTZsUExOjO+64Q++9954yMzP1ox/9KBxPgd5AjQUAMCjksHPq1CmtX79eX//613XVVVfppZde0qxZs3T48GE9++yzev311/Xiiy/qkUceCee8AAAA3RLyt54PHjxYXq9XkydP1o4dO5SdnR20zy233KJLL720B+PBEdRYAABDQg47JSUl+vd//3f169ev03afz6eDBw9q6NChuvTSS/XRRx/1eEj0EmosAIBBIddYP/jBD3Ty5Mmg7SdOnFBGRkaPhgIAAAiXkMOO7wyrACdPnlRcXFzIAyECUGMBAAzpdo3ldrslSVFRUSotLe1UY3V0dGj79u1dnr+DCwA1FgDAoG6HnV27dkn6y8rOe++9p9jYWP/PYmNjlZWVpQcffDB8EwIAAPRAt8PO5s2bJUnFxcX68Y9/rMTExLAPBYecXtmhxgIAGBLyp7GefvrpcM4BAABwXnQr7Ljdbi1cuFD9+/f3n7tzJhUVFT0aDA5iZQcAYEi3ws6uXbv8X/J5+tydrkTxZnlh4gRlAIBB3Qo7p8/X+f9/BgAAiFQhX2enoaHhjNfaaWhoCHkgRABW5gAAhoQcdjIyMnTs2LGg7X/+85+5gvKFihoLAGBQj66g3NW5OVxBGQAARJIeXUF5/vz5XEHZImosAIAhXEEZAdRYAACDenQF5SeffFIJCQlhHwoAACBcenQF5erqalVXV+vo0aPyer2dfr5mzZoeDweHUGMBAAwJOew88sgjWrBggXJzczV48GAuJGgBNRYAwKCQw87KlSv1zDPPaOrUqeGcBwAAIKxC/uh5e3u7xowZE85ZEClYpQMAGBJy2Ln33nu1du3acM4Cp1FjAQAMCrnG+vzzz7Vq1Sq9/vrrGjlypPr27dvp53zrOQAAiAQhh53//u//9l88cPfu3Z1+xsnKFzj++QEADAk57PCt5wZRYwEADAr5nB0AAIALQY/CzltvvaVvfetbKigo0KFDhyRJP//5z7V169awDAeHUGMBAAwJOeysX79eRUVFio+P165du9TW1iZJam5u1pIlS8I2IHoRNRYAwKCQw86iRYtUVVWl1atXd/ok1tixY1VXVxeW4QAAAHoq5LCzd+9e3XTTTUHbk5KS9Mknn/RkJjiNGgsAYEjIYSclJUX79+8P2r5161ZdffXVPRoKDqHGAgAYFHLYmTFjhkpKSrR9+3ZFRUXp8OHDeu655/Tggw/q/vvvD+eMAAAAIQv5Ojtz5syR1+vVrbfeqk8//VQ33XSTXC6XHnzwQX3nO98J54zobdRYAABDQg47UVFReuihh/S9731P+/fv18mTJ5WZmalLLrkknPOhN1FjAQAMCjnsnBYbG6vMzMxwzAIAABB23Qo7brf7nPfli0AvYNRYAABDuhV2du3a1el+XV2dvvjiC335y1+WJO3bt08xMTHKyckJ34ToPdRYAACDuhV2/vrLPysqKpSQkKBnn31WAwYMkCR9/PHHKi4u1o033hjeKQEAAEIU8kfPly5dqvLycn/QkaQBAwZo0aJFWrp0aViGQy87vbJDjQUAMCTksOPxeHTs2LGg7ceOHVNLS0uPhqqsrFR6erri4uKUn5+vHTt2nNNxL7zwgqKiojRx4sQePT8AALAj5LDzj//4jyouLtYvfvEL/elPf9Kf/vQnrV+/Xvfcc4++8Y1vhDzQunXr5Ha7VVZWprq6OmVlZamoqEhHjx4963EHDhzQgw8+SIUGAAA6CTnsVFVVady4cbrrrrt01VVXaejQobrrrrv0ta99TStWrAh5oIqKCs2YMUPFxcXKzMxUVVWV+vXrpzVr1pzxmI6ODk2ZMkULFizgqyp6ghoLAGBQyGGnX79+WrFihf785z9r165dqq+v14kTJ7RixQr1798/pMdsb29XbW2tCgsLAwNGR6uwsFA1NTVnPO6RRx7RoEGDdM899/zN52hra5PH4+l0AwAAdnX7OjsLFy5U//79/+Y1d0K5zs7x48fV0dGh5OTkTtuTk5O1Z8+eLo/ZunWrfvazn6m+vv6cnqO8vFwLFizo9mwXFVZ2AACGdPs6O6dOnfL/+UyieunNsqWlRVOnTtXq1as1cODAczpm7ty5nYKax+NRWlra+RrxwsJ1dgAABoV8nZ2//nO4DBw4UDExMWpqauq0vampSSkpKUH7//GPf9SBAwc0YcIE/zav1ytJ6tOnj/bu3atrrrmm0zEul0sulyvsswMAgMgU8jk750NsbKxycnJUXV3t3+b1elVdXa2CgoKg/YcPH6733ntP9fX1/ts//MM/6JZbblF9fT0rNqGixgIAGNLjLwINN7fbrenTpys3N1d5eXlatmyZWltbVVxcLEmaNm2ahgwZovLycsXFxWnEiBGdjr/00kslKWg7zgE1FgDAoIgLO5MmTdKxY8dUWlqqxsZGZWdna+PGjf6TlhsaGhQdHVELUgAAIIJF+XwX9//OezweJSUlqbm5WYmJiU6P46yf/1yaNk0qKpI2bnR6GgAAzqg7798skSDg4s69AACjCDsAAMA0wg6C8WksAIAhhB0EUGMBAAwi7AAAANMIOwhGjQUAMISwgwBqLACAQYQdAABgGmEHwaixAACGEHYQQI0FADCIsAMAAEwj7CAYNRYAwBDCDgKosQAABhF2AACAaYQdBKPGAgAYQthBADUWAMAgwg4AADCNsINg1FgAAEMIOwigxgIAGETYAQAAphF2EIwaCwBgCGEHAdRYAACDCDsAAMA0wg4CTq/sUGMBAAwh7AAAANMIOwjGyg4AwBDCDgI4QRkAYBBhBwAAmEbYQTBqLACAIYQdBFBjAQAMIuwAAADTCDsIRo0FADCEsIMAaiwAgEGEHQAAYBphB8GosQAAhhB2EECNBQAwiLADAABMI+wgGDUWAMAQwg4CqLEAAAYRdgAAgGmEHQSjxgIAGELYQQA1FgDAIMIOAAAwjbCDYNRYAABDCDsIoMYCABhE2AEAAKYRdhCMGgsAYAhhBwHUWAAAgwg7AADANMIOglFjAQAMIewggBoLAGAQYQcAAJhG2EEwaiwAgCGEHQRQYwEADCLsAAAA0wg7CDi9skONBQAwJCLDTmVlpdLT0xUXF6f8/Hzt2LHjjPuuXr1aN954owYMGKABAwaosLDwrPsDAICLS8SFnXXr1sntdqusrEx1dXXKyspSUVGRjh492uX+b7zxhiZPnqzNmzerpqZGaWlpuv3223Xo0KFenhwAAESiiAs7FRUVmjFjhoqLi5WZmamqqir169dPa9as6XL/5557Tv/2b/+m7OxsDR8+XD/96U/l9XpVXV3dy5MbQI0FADAoosJOe3u7amtrVVhY6N8WHR2twsJC1dTUnNNjfPrppzp16pQuu+yyLn/e1tYmj8fT6QYAAOyKqLBz/PhxdXR0KDk5udP25ORkNTY2ntNjfP/731dqamqnwPTXysvLlZSU5L+lpaX1eG5zWNkBABgSUWGnpx599FG98MILeuWVVxQXF9flPnPnzlVzc7P/dvDgwV6eMoJxnR0AgEF9nB7grw0cOFAxMTFqamrqtL2pqUkpKSlnPfaHP/yhHn30Ub3++usaOXLkGfdzuVxyuVxhmRcAAES+iFrZiY2NVU5OTqeTi0+fbFxQUHDG4x5//HEtXLhQGzduVG5ubm+Mahs1FgDAkIha2ZEkt9ut6dOnKzc3V3l5eVq2bJlaW1tVXFwsSZo2bZqGDBmi8vJySdJjjz2m0tJSrV27Vunp6f5zey655BJdcskljv0eFyRqLACAQREXdiZNmqRjx46ptLRUjY2Nys7O1saNG/0nLTc0NCg6OrAgtXLlSrW3t+vOO+/s9DhlZWX6wQ9+0JujAwCACBRxYUeSHnjgAT3wwANd/uyNN97odP/AgQPnf6CLDTUWAMCQiDpnBw6jxgIAGETYAQAAphF2EIwaCwBgCGEHAdRYAACDCDsAAMA0wg6CUWMBAAwh7CCAGgsAYBBhBwAAmEbYQTBqLACAIYQdBFBjAQAMIuwAAADTCDsIRo0FADCEsIMAaiwAgEGEHQAAYBphB8GosQAAhhB2EECNBQAwiLADAABMI+wgGDUWAMAQwg4CqLEAAAYRdgAAgGmEHQSjxgIAGELYQQA1FgDAIMIOAAAwjbCDgNMrO9RYAABDCDsAAMA0wg6CsbIDADCEsIMATlAGABhE2AEAAKYRdhCMGgsAYAhhBwHUWAAAgwg7AADANMIOglFjAQAMIewggBoLAGAQYQcAAJhG2EEwaiwAgCGEHQRQYwEADCLsAAAA0wg7CEaNBQAwhLCDAGosAIBBhB0AAGAaYQfBqLEAAIYQdhBAjQUAMIiwAwAATCPsIBg1FgDAEMIOAqixAAAGEXYAAIBphB0Eo8YCABhC2EEANRYAwCDCDgAAMI2wg2DUWAAAQwg7CKDGAgAYRNgBAACmEXYQjBoLAGAIYQcB1FgAAIMIOwAAwDTCDgJOr+xQYwEADInIsFNZWan09HTFxcUpPz9fO3bsOOv+L730koYPH664uDhdf/31eu2113ppUgAAEOkiLuysW7dObrdbZWVlqqurU1ZWloqKinT06NEu99+2bZsmT56se+65R7t27dLEiRM1ceJE7d69u5cnBwAAkSjK54uss1Lz8/N1ww036KmnnpIkeb1epaWl6Tvf+Y7mzJkTtP+kSZPU2tqqX//61/5tf//3f6/s7GxVVVX9zefzeDxKSkpSc3OzEhMTw/eLtLVJjY3he7ze8OijUlWV5HZLS5c6PQ0AAGfUnffvPr000zlpb29XbW2t5s6d698WHR2twsJC1dTUdHlMTU2N3G53p21FRUXasGFDl/u3tbWpra3Nf9/j8fR88K7s2iUVFJyfxwYAAOcsosLO8ePH1dHRoeTk5E7bk5OTtWfPni6PaWxs7HL/xjOsqpSXl2vBggXhGfhsoqKkuLjz/zzhlpAgjRvn9BQAAIRNRIWd3jB37txOK0Eej0dpaWnhf6L8fOmzz8L/uAAAoFsiKuwMHDhQMTExampq6rS9qalJKSkpXR6TkpLSrf1dLpdcLld4BgYAABEvoj6NFRsbq5ycHFVXV/u3eb1eVVdXq+AM578UFBR02l+SNm3adMb9AQDAxSWiVnYkye12a/r06crNzVVeXp6WLVum1tZWFRcXS5KmTZumIUOGqLy8XJJUUlKim2++WUuXLtX48eP1wgsv6J133tGqVauc/DUAAECEiLiwM2nSJB07dkylpaVqbGxUdna2Nm7c6D8JuaGhQdHRgQWpMWPGaO3atXr44Yc1b948DRs2TBs2bNCIESOc+hUAAEAEibjr7PS283adHQAAcN505/07os7ZAQAACDfCDgAAMI2wAwAATCPsAAAA0wg7AADANMIOAAAwjbADAABMI+wAAADTCDsAAMC0iPu6iN52+gLSHo/H4UkAAMC5Ov2+fS5fBHHRh52WlhZJUlpamsOTAACA7mppaVFSUtJZ97novxvL6/Xq8OHDSkhIUFRUVFgf2+PxKC0tTQcPHuR7t84jXufewevcO3idew+vde84X6+zz+dTS0uLUlNTO31BeFcu+pWd6OhoXXnllef1ORITE/kPqRfwOvcOXufewevce3ite8f5eJ3/1orOaZygDAAATCPsAAAA0wg755HL5VJZWZlcLpfTo5jG69w7eJ17B69z7+G17h2R8Dpf9CcoAwAA21jZAQAAphF2AACAaYQdAABgGmEHAACYRtg5TyorK5Wenq64uDjl5+drx44dTo9kTnl5uW644QYlJCRo0KBBmjhxovbu3ev0WKY9+uijioqK0qxZs5wexaRDhw7pW9/6li6//HLFx8fr+uuv1zvvvOP0WKZ0dHRo/vz5ysjIUHx8vK655hotXLjwnL5fCWe2ZcsWTZgwQampqYqKitKGDRs6/dzn86m0tFSDBw9WfHy8CgsL9Yc//KHX5iPsnAfr1q2T2+1WWVmZ6urqlJWVpaKiIh09etTp0Ux58803NXPmTL399tvatGmTTp06pdtvv12tra1Oj2bSzp079ZOf/EQjR450ehSTPv74Y40dO1Z9+/bVf/3Xf+n999/X0qVLNWDAAKdHM+Wxxx7TypUr9dRTT+mDDz7QY489pscff1zLly93erQLWmtrq7KyslRZWdnlzx9//HE9+eSTqqqq0vbt29W/f38VFRXp888/750BfQi7vLw838yZM/33Ozo6fKmpqb7y8nIHp7Lv6NGjPkm+N9980+lRzGlpafENGzbMt2nTJt/NN9/sKykpcXokc77//e/7vvKVrzg9hnnjx4/33X333Z22feMb3/BNmTLFoYnskeR75ZVX/Pe9Xq8vJSXF98QTT/i3ffLJJz6Xy+V7/vnne2UmVnbCrL29XbW1tSosLPRvi46OVmFhoWpqahyczL7m5mZJ0mWXXebwJPbMnDlT48eP7/TvNcLrl7/8pXJzc/VP//RPGjRokEaNGqXVq1c7PZY5Y8aMUXV1tfbt2ydJevfdd7V161aNGzfO4cns+uijj9TY2Njp74+kpCTl5+f32vviRf9FoOF2/PhxdXR0KDk5udP25ORk7dmzx6Gp7PN6vZo1a5bGjh2rESNGOD2OKS+88ILq6uq0c+dOp0cx7cMPP9TKlSvldrs1b9487dy5U9/97ncVGxur6dOnOz2eGXPmzJHH49Hw4cMVExOjjo4OLV68WFOmTHF6NLMaGxslqcv3xdM/O98IOzBh5syZ2r17t7Zu3er0KKYcPHhQJSUl2rRpk+Li4pwexzSv16vc3FwtWbJEkjRq1Cjt3r1bVVVVhJ0wevHFF/Xcc89p7dq1uu6661RfX69Zs2YpNTWV19kwaqwwGzhwoGJiYtTU1NRpe1NTk1JSUhyayrYHHnhAv/71r7V582ZdeeWVTo9jSm1trY4eParRo0erT58+6tOnj9588009+eST6tOnjzo6Opwe0YzBgwcrMzOz07Zrr71WDQ0NDk1k0/e+9z3NmTNH3/zmN3X99ddr6tSpmj17tsrLy50ezazT731Ovi8SdsIsNjZWOTk5qq6u9m/zer2qrq5WQUGBg5PZ4/P59MADD+iVV17R7373O2VkZDg9kjm33nqr3nvvPdXX1/tvubm5mjJliurr6xUTE+P0iGaMHTs26NIJ+/bt01VXXeXQRDZ9+umnio7u/NYXExMjr9fr0ET2ZWRkKCUlpdP7osfj0fbt23vtfZEa6zxwu92aPn26cnNzlZeXp2XLlqm1tVXFxcVOj2bKzJkztXbtWv3nf/6nEhIS/N1vUlKS4uPjHZ7OhoSEhKBzoPr376/LL7+cc6PCbPbs2RozZoyWLFmif/7nf9aOHTu0atUqrVq1yunRTJkwYYIWL16soUOH6rrrrtOuXbtUUVGhu+++2+nRLmgnT57U/v37/fc/+ugj1dfX67LLLtPQoUM1a9YsLVq0SMOGDVNGRobmz5+v1NRUTZw4sXcG7JXPfF2Eli9f7hs6dKgvNjbWl5eX53v77bedHskcSV3enn76aadHM42Pnp8/v/rVr3wjRozwuVwu3/Dhw32rVq1yeiRzPB6Pr6SkxDd06FBfXFyc7+qrr/Y99NBDvra2NqdHu6Bt3ry5y7+Pp0+f7vP5/vLx8/nz5/uSk5N9LpfLd+utt/r27t3ba/NF+XxcNhIAANjFOTsAAMA0wg4AADCNsAMAAEwj7AAAANMIOwAAwDTCDgAAMI2wAwAATCPsAAAA0wg7AADANMIOAAAwjbADAABMI+wAMOf5559XfHy8jhw54t9WXFyskSNHqrm52cHJADiBLwIFYI7P51N2drZuuukmLV++XGVlZVqzZo3efvttDRkyxOnxAPSyPk4PAADhFhUVpcWLF+vOO+9USkqKli9frrfeeougA1ykWNkBYNbo0aP1+9//Xr/97W918803Oz0OAIdwzg4AkzZu3Kg9e/aoo6NDycnJTo8DwEGs7AAwp66uTl/96lf1k5/8RM8884wSExP10ksvOT0WAIdwzg4AUw4cOKDx48dr3rx5mjx5sq6++moVFBSorq5Oo0ePdno8AA5gZQeAGSdOnNCYMWP01a9+VVVVVf7t48ePV0dHhzZu3OjgdACcQtgBAACmcYIyAAAwjbADAABMI+wAAADTCDsAAMA0wg4AADCNsAMAAEwj7AAAANMIOwAAwDTCDgAAMI2wAwAATCPsAAAA0/4XLt+sF1iogJgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from chopping import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "op = Option()\n",
    "op.set_format('s')\n",
    "\n",
    "x = np.array([100., 0.9])\n",
    "\n",
    "f = lambda x : LPV(np.sqrt(x.array()), op) # computing the sqrt\n",
    "\n",
    "N = 50\n",
    "\n",
    "def identity(x):\n",
    "    x = LPV(x, op)\n",
    "    for i in range(N):\n",
    "        x = f(x) # takes the square root\n",
    "\n",
    "    for i in range(N):\n",
    "        x = x*x # squares the result\n",
    "\n",
    "    return x.array()\n",
    "\n",
    "result = identity(x)\n",
    "print(\"Initial x:\", x, \"\\nAfter applying the identity function:\", result)\n",
    "\n",
    "x = np.linspace(0,10,1000)\n",
    "res = identity(x)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('identity$(x)$')\n",
    "plt.plot(x,res, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f68c468-7ec3-4c22-bd12-ebf0f08a7964",
   "metadata": {},
   "source": [
    "**Question:** Can you explain what is happening? Think about values of $x>1$ first and then about $x<1$. If you spend too much time on this move on to the next problem and come back to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0b881-dba4-4120-b4a4-3341e39d06e4",
   "metadata": {},
   "source": [
    "Question 3\n",
    "----------\n",
    "\n",
    "Using the dot product rounding error bound that we have seen in the lectures, derive the following rounding error bound for matrix-vector products:\n",
    "\n",
    "$$\\widehat{Ab}=(A+\\Delta A)b,\\quad\\text{where}\\quad |\\Delta A| \\leq cnu|A|$$.\n",
    "\n",
    "Let $\\hat{x} = \\widehat{Ab}$ and $x = Ab$. Use the above to derive an upper bound for the forward error $|\\hat{x} - x|$ (if you are not a theoretical person you can skip this part).\n",
    "\n",
    "Let $b$ be a Gaussian vector of length $n$ and let $A$ be a square Gaussian matrix of size $n$. For $n=2^i$ and for $i=6,\\dots,11$, evaluate $x$ in half precision using the bfloat16 format and compute the resulting error in the infinity norm. Plot this error against $n$ in a loglog plot and compare it with the quantity $un\\lVert x \\rVert_\\infty$ ($u=2^{-8}$ for bfloat16). Does the experiment match with what expected from the theory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c17e844-959e-4f83-9bad-37b373dce5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chopping import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "op = Option(True)\n",
    "op.set_format('b')\n",
    "\n",
    "err = []\n",
    "\n",
    "NN = 2**np.arange(6,12)\n",
    "\n",
    "u = 2.**-8\n",
    "bound = u*NN*abs(x).max()\n",
    "\n",
    "## The rest is up to you. Some plotting utilities are below.\n",
    "\n",
    "#plt.loglog(NN, err, 'r*', label='rounding error')\n",
    "#plt.loglog(NN, bound, 'k--', label='upper bound')\n",
    "#plt.xlabel('$n$')\n",
    "#plt.ylabel('error')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73328a7-8ca9-4919-8af8-7ede1e91ed75",
   "metadata": {},
   "source": [
    "Question 4 - Compensated summation\n",
    "----------------------------------\n",
    "\n",
    "Let $a\\in\\mathbb{R}^n$ be a vector such that $a_i = \\frac{2i}{n}$ for $i=1,\\dots,n$. For $n=1000$ compute $s = \\sum_{i=1}^n a_i$ by looping over each entry using bfloat16 half-precision. Let $\\hat{s}$ be the computed bfloat16 value and compute the error $|\\hat{s} - s|$ by computing the exact value of $s$.\n",
    "\n",
    "Now, perform the half precision calculation again using the following algorithm, which is a version of **Kahan's compensated summation**: \n",
    "\n",
    "```python\n",
    "s = 0; e = 0\n",
    "for i in range(n):\n",
    "    temp = s\n",
    "    y = a[i] + e\n",
    "    s = temp + y\n",
    "    e = (temp - s) + y\n",
    "```\n",
    "\n",
    "Compute the error between this new $\\hat{s}$ and $s$, i.e., $|s - \\hat{s}|$. Is the error larger or smaller? Compare this error with the value of $e$. What do you observe? What do you think $e$ may mean?\n",
    "\n",
    "Redo the exercise by varying $n$ and plotting (in loglog scale) the errors for the two algorithms for $n=2^i$ for $i=8,\\dots,16$. How do the errors behave with respect to $n$ for the two algorithms?\n",
    "\n",
    "Compute the FLOPs and memory cost of the standard VS compensated summation. Considering the FLOP and memory cost and the resulting accuracy, what are the pros and cons of using the compensated summation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622bfd69-2d4a-413f-9515-d16ee76f0770",
   "metadata": {},
   "source": [
    "Question 5 - Floating point parameters and Overflow\n",
    "---------------------------------------------------\n",
    "\n",
    "Define an fp16 half-precision option:\n",
    "\n",
    "```python\n",
    "op = Option(True)\n",
    "op.set_format('h')\n",
    "```\n",
    "Then call `op.get_floating_point_parameters()`. Read the [fp16 wikipedia page](https://en.wikipedia.org/wiki/Half-precision_floating-point_format#IEEE_754_half-precision_binary_floating-point_format:_binary16) and see if you understand what these parameters mean.  \n",
    "\n",
    "\n",
    "Let $a\\in\\mathbb{R}^n$ be a vector such that $a_i = 2i$ for $i=1,\\dots,n$ and consider again $s$ to be the sum of all of its entries. Assume you are computing $s$ in float16 half precision and obtain $\\hat{s}$. What is the smallest value of $n$ which makes the calculation overflow? Estimate this value theoretically and demonstrate that your calculations are correct via a numerical experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuPy Kernel",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
