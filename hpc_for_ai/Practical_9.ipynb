{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30064e54-b029-4b4d-934a-717b160469cb",
   "metadata": {},
   "source": [
    "# High-performance and parallel computing for AI - Practical 9: Numba-CUDA and more GPU programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431376a-ccf3-4363-bb6f-bf439327b872",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "=========\n",
    "\n",
    "* CuPy behaves weirdly for me. Restart the kernel if you encounter weird errors.\n",
    "* For these practicals we will be using a different `conda environment`. When opening a notebook or a terminal make sure you are using the **CuPy Kernel**!!!\n",
    "* It's fine if you do not finish everything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6888116a-89b5-469b-ba3e-8083162580a9",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "\n",
    "Before you start (and before running any other GPU code on the servers) please run the following code, which limits the maximum GPU memory usage to $1.5$ GB and picks an L40s GPU and a Quadro GPU at random. **Please only run the code below once every time you restart the kernel!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04aeea5d-0afe-402f-8988-e1dce92543d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# CuPy-specific environment variables\n",
    "os.environ[\"CUPY_GPU_MEMORY_LIMIT\"] = \"1573741824\" # roughly 1.5 GB\n",
    "os.environ[\"CUPY_ACCELERATORS\"]=\"cutensor\" # activates cutensor acceleration\n",
    "os.environ[\"CUPY_TF32\"] = \"1\" # activates tf32 tensor cores\n",
    "\n",
    "## On goliat we have FIVE GPUs so here we pick two of those at random\n",
    "## so that we do not overload the system.\n",
    "## The way we do it is by figuring out the GPU UUIDs and then setting\n",
    "## The CUDA_VISIBLE_DEVICES environment variable.\n",
    "## Note: this is useful for other libraries as well (e.g., Jax, PyTorch, TF) in multi-GPU servers.\n",
    "\n",
    "# To get these UUIDs you need to run nvidia-smi -q on the command line\n",
    "quadro_UUIDs = [\"GPU-4efa947b-abbd-7c6e-84f5-61241d34bb4b\",\n",
    "                \"GPU-5eb524b0-2b1b-fe98-e6ed-b8fb5185e993\"]\n",
    "\n",
    "L40s_UUIDs = [\"GPU-7bba1f33-03d2-016b-d42e-ced83c3ac243\",\n",
    "              \"GPU-179d068a-3bea-91d7-1a8c-7017f55d6298\",\n",
    "              \"GPU-ae634859-dd49-de46-9182-195639405eaa\"]\n",
    "\n",
    "from numpy.random import randint\n",
    "# Picks an L40s and a Quadro GPU at random. The others will be invisible to CuPy\n",
    "# NOTE: this only works if the environment variable is set BEFORE CuPy is first imported.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = L40s_UUIDs[randint(3)] + \",\" + quadro_UUIDs[randint(2)]\n",
    "\n",
    "## CuPy and Numba will only see these GPUs and will assign them these device numbers:\n",
    "L40sID = 0\n",
    "quadro_ID = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81648cee-8dd5-4d3b-9e72-0ab35c5a1cfd",
   "metadata": {},
   "source": [
    "## Tutorial 1 - Numba-CUDA\n",
    "\n",
    "Numba-CUDA is a a spinoff of Numba which is now being developed separately. However, the numba-cuda docs are still currently part of the mainline numba docs. You can find them [here](https://numba.readthedocs.io/en/stable/cuda/index.html).\n",
    "\n",
    "Numba-CUDA works similarly to CuPy JIT-Rawkernel. However, it is much better documented. What I like of it is that it has a lot of functionalities which are very close to actual CUDA.\n",
    "\n",
    "As a first example we show the solution to Question 3 of Practical 8 implemented using numba-CUDA (with a little help from CuPy). Please study this code as it will help you understand the basics of Numba-CUDA.\n",
    "\n",
    "**Note:** While you can use numpy functions inside kernels, you cannot use numpy functions which allocate memory inside a kernel. All memory must be allocated either outside the kernel (global/device memory) or via Numba-CUDA functions (e.g., if you want to allocate shared/constant memory). This mirrors the CUDA programming model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1766bde0-23fa-4795-a23c-e7681567eedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mywrappedfun        :    CPU:   157.941 us   +/-  5.933 (min:   150.755 / max:   232.530) us     GPU-0:   438.105 us   +/-  5.850 (min:   430.080 / max:   509.952) us\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "# NOTE: the following CUDA variables are available\n",
    "# when writing Numba-CUDA kernels.\n",
    "#\n",
    "#     cuda.threadIdx.x, cuda.blockIdx.x, cuda.blockDim.x\n",
    "#\n",
    "# So that, for instance, you can get the thread ID\n",
    "# and the stride in the x,y directions as follows:\n",
    "#\n",
    "#     tidx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "#     ntidx = cuda.gridDim.x * cuda.blockDim.x\n",
    "#     tidy = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "#     ntidy = cuda.gridDim.y * cuda.blockDim.y\n",
    "#\n",
    "# However, in Numba-CUDA this is made simpler thanks\n",
    "# to the helper functions cuda.grid(dim) and cuda.gridsize(dim).\n",
    "# Just do:\n",
    "#\n",
    "#     tidx, tidy = cuda.grid(2)\n",
    "#     ntidx,ntidy = cuda.gridsize(2)\n",
    "\n",
    "@cuda.jit\n",
    "def myfun(x, y, z): # z is the OUTPUT. It will be overwritten!\n",
    "    tidx, tidy = cuda.grid(2) \n",
    "    ntidx,ntidy = cuda.gridsize(2)\n",
    "    for i in range(tidy, n, ntidy):\n",
    "        for j in range(tidx, n, ntidx):\n",
    "            z[i, j] = x[i,j]*y[i,j]\n",
    "\n",
    "gridDim = (128, 128)\n",
    "blockSize = (32, 32)\n",
    "\n",
    "n = 4096 \n",
    "\n",
    "# Initialising CuPy arrays directly on the device\n",
    "a = cp.random.randn(n, n, dtype=np.float32)\n",
    "b = cp.random.randn(n, n, dtype=np.float32)\n",
    "\n",
    "# Numba-CUDA kernels require Numba-CUDA arrays.\n",
    "# However, these are compatible with CuPy arrays\n",
    "# so you can simply pass CuPy arrays to Numba-CUDA.\n",
    "#\n",
    "# The above sounds great, but I notice a loss in performance\n",
    "# if CuPy arrays are not converted to Numba-CUDA arrays beforehand.\n",
    "# This does not appear anywhere in the docs, alas.\n",
    "# Luckily this is simple enough: to convert CuPy arrays to\n",
    "# Numba-CUDA arrays by hand, you simply call\n",
    "#\n",
    "# a_numba = cuda.as_cuda_array(a_cupy)\n",
    "#\n",
    "# The docs say that this does not actually move/copy any memory so it\n",
    "# should not make a difference in terms of performance.\n",
    "#\n",
    "# Similarly, you can convert a Numba-CUDA array to a CuPy\n",
    "# array with (no memory movement/copy done):\n",
    "#\n",
    "# a_cupy = cp.asarray(a_numba)\n",
    "#\n",
    "# Note: If you instead want to move arrays from host to device\n",
    "# so that they are already in the Numba-CUDA format you must use instead\n",
    "#\n",
    "# a_numba = cuda.to_device(a_host)\n",
    "#\n",
    "# To do the opposite, use instead:\n",
    "#\n",
    "# a_host = a_numba.copy_to_host()\n",
    "#\n",
    "\n",
    "# You can call the kernel as follows (c will be overwritten):\n",
    "\n",
    "# VERSION 1. Using CuPy arrays (slower in my experience) \n",
    "c = cp.zeros((n, n), dtype=np.float32)\n",
    "myfun[gridDim, blockSize](a, b, c)\n",
    "assert (c == a*b).all() # check computations are correct\n",
    "\n",
    "# VERSION 2. Using Numba arrays (faster in my experience).\n",
    "# Since no memory movement happens, the CuPy c will be modified\n",
    "c = cp.zeros((n, n), dtype=np.float32)\n",
    "a_numba, b_numba, c_numba = (cuda.as_cuda_array(item) for item in (a,b,c)) # will not move any memory\n",
    "myfun[gridDim, blockSize](a_numba, b_numba, c_numba)\n",
    "# The following line also shows that using CuPy arrays in Numba-CUDA does not move/copy any memory\n",
    "assert (c == a*b).all() # check computations are correct\n",
    "\n",
    "# Wrapping VERSION 2 into a function \n",
    "# so that we can benchmark it with benchmark\n",
    "# Try replacing numba_inputs in the third line below with cupy_inputs.\n",
    "# It will take longer and the CPU time will increase (I suspect there is\n",
    "# some host/device memory movement happening, odd).\n",
    "def mywrappedfun(gridDim, blockSize, cupy_inputs):\n",
    "    numba_inputs = (cuda.as_cuda_array(item) for item in cupy_inputs)\n",
    "    myfun[gridDim,blockSize](*numba_inputs)\n",
    "\n",
    "# You can use the above like this (same as typical CuPy benchmark syntax)\n",
    "c = cp.zeros((n, n), dtype=np.float32)\n",
    "mywrappedfun(gridDim, blockSize, (a, b, c))\n",
    "\n",
    "# Sanity check: just to make sure c actually gets overwritten.\n",
    "assert c.sum() != 0 \n",
    "\n",
    "print(benchmark(mywrappedfun, (gridDim, blockSize, (a,b,c)), n_repeat=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775afef-0b4c-4e78-a15f-1a9dddffab43",
   "metadata": {},
   "source": [
    "Note that the above is as fast as the CuPy JIT-Rawkernel.\n",
    "However, the Numba-CUDA documentation is more extensive so I hope that it gives the developer more options. Both Numba-CUDA and CuPy are in active development so it is hard to tell how these functionalities will evolve in the future. It may be useful to know both, and Numba-CUDA shares some similarities with actual CUDA so it is good for teaching.\n",
    "\n",
    "Before we proceed, a quick timing of the cost of memory movement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dece90a8-1a3a-4a94-85ef-5293bae3e17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asarray             :    CPU:     9.941 us   +/-  1.112 (min:     9.198 / max:    27.102) us     GPU-0:    12.876 us   +/-  1.321 (min:    11.264 / max:    30.720) us\n",
      "as_cuda_array       :    CPU:    38.066 us   +/-  8.386 (min:    35.757 / max:   294.156) us     GPU-0:    41.830 us   +/-  8.503 (min:    38.912 / max:   300.032) us\n",
      "<lambda>            :    CPU: 10749.809 us   +/- 1164.147 (min:  9649.930 / max: 17616.893) us     GPU-0: 10756.978 us   +/- 1164.496 (min:  9656.160 / max: 17628.448) us\n",
      "asnumpy             :    CPU: 10016.088 us   +/- 666.673 (min:  9613.961 / max: 11921.931) us     GPU-0: 10021.711 us   +/- 666.670 (min:  9620.192 / max: 11927.072) us\n"
     ]
    }
   ],
   "source": [
    "# From Numba-CUDA to CuPy\n",
    "c_numba = cuda.as_cuda_array(c)\n",
    "print(benchmark(cp.asarray, (c_numba,), n_repeat=1000))\n",
    "\n",
    "# From CuPy to Numba-CUDA (it seems like it takes more this way around)\n",
    "print(benchmark(cuda.as_cuda_array, (c,), n_repeat=1000))\n",
    "\n",
    "# From Numba-CUDA to Host\n",
    "print(benchmark(lambda x : x.copy_to_host(), (c_numba,), n_repeat=100))\n",
    "\n",
    "# From CuPy to host\n",
    "print(benchmark(cp.asnumpy, (c,), n_repeat=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde1f83-5f46-4424-9846-c0e69d981105",
   "metadata": {},
   "source": [
    "As you can see, moving memory between host and device is much more expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34558d1-52e4-4fa4-aeab-9bc677d70acc",
   "metadata": {},
   "source": [
    "## Question 1 - Local reductions and dynamic shared memory\n",
    "\n",
    "Consider the following CUDA C++ snippet that implements a local reduction operation:\n",
    "\n",
    "```C++\n",
    "__global__ void reduction(float *g_odata, float *g_idata)\n",
    "{\n",
    "    // dynamically allocated shared memory\n",
    "    extern  __shared__  float temp[];\n",
    "\n",
    "    int tid = threadIdx.x;\n",
    "\n",
    "    // first, each thread loads data into shared memory\n",
    "    temp[tid] = g_idata[tid];\n",
    "\n",
    "    // next, we perform binary tree reduction\n",
    "    for (int d=blockDim.x/2; d>0; d=d/2) {\n",
    "      __syncthreads();  // ensure previous step completed \n",
    "      if (tid<d)\n",
    "          temp[tid] += temp[tid+d];\n",
    "    }\n",
    "\n",
    "    // finally, first thread puts result into global memory\n",
    "    if (tid == 0)\n",
    "        g_odata[0] = temp[0];\n",
    "}\n",
    "```\n",
    "\n",
    "Note that this uses syncthreads and dynamic shared memory. First, read the code above and understand what it does (it computes a sum of all entries in `g_idata` and stores the output in `g_odata`).\n",
    "\n",
    "* Why was syncthreads used?\n",
    "* (Optional, come back to it later perhaps) This looks like a convoluted approach to perform a sum. Can you think about why something like this would be needed?\n",
    "\n",
    "By looking at the above tutorial and at the lecture slides convert this code into Python by using Numba-CUDA and call it by inserting it into the Python script provided below, which you will also have to modify (follow the code comments).\n",
    "\n",
    "**Hint:** Note that this example uses dynamic shared memory! The [Numba docs](https://numba.readthedocs.io/en/stable/cuda/memory.html) and the course slides may be helpful.\n",
    "\n",
    "**Hint:** In Python you will have to use the following while loop in place of a foor loop:\n",
    "```python\n",
    "d = cuda.blockDim.x//2 # You need integer division here\n",
    "while d > 0:\n",
    "    # loop body here\n",
    "    d = d//2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fbf6619-4f34-451d-914c-373b970c6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script true\n",
    "# NOTE: remove the line above else the cell won't run\n",
    "\n",
    "import cupy as cp\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "num_blocks = 1\n",
    "num_threads = 512\n",
    "num_elements = num_blocks*num_threads\n",
    "\n",
    "# NOTE: Allocated on the host\n",
    "h_idata = 10.*np.random.rand(num_elements)\n",
    "h_idata = h_idata.astype(np.float32) # input data\n",
    "ex_sum = h_idata.sum() # exact sum computed by host\n",
    "\n",
    "d_idata = None # FIX THIS. Move h_idata onto the device\n",
    "d_odata = None # Fix this. Initialise an empty array with a single float32 entry on the device to hold the output\n",
    "\n",
    "# Modify this and JIT-it with Numba\n",
    "def reduction(g_odata, g_idata):\n",
    "    raise NotImplementedError\n",
    "\n",
    "shared_memory_size = None # MODIFY THIS. How big should this be? Remember: here you need memory in bytes, not array entries!!!\n",
    "reduction[FIXME](d_odata, d_idata) # MODIFY THIS by replacing FIXME with the correct kernel parameters.\n",
    "\n",
    "computed_sum = None # Get d_odata back to the host. Its first entry is the computed sum.\n",
    "\n",
    "print(\"Reduction error: %.3e\" % abs(computed_sum[0] - ex_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d1cab9-0ad9-454f-ba95-5df2da47a323",
   "metadata": {},
   "source": [
    "## Question 2 - Constant memory, static shared memory, device functions\n",
    "\n",
    "This question is a bit silly, but is a good way to try different things. Modify the code from Question 1 in three ways:\n",
    "\n",
    "1- Using static memory.\n",
    "\n",
    "2- Saving the number `2` used in the loop as a constant variable using `cuda.const.array_like`. Hint: define a numpy array `np.array([2], dtype=np.int32)` outside the kernel, cast it to constant inside the kernel, then index it.\n",
    "\n",
    "3- Defining a [device function](https://numba.readthedocs.io/en/stable/cuda/device-functions.html) that computes $d=d//2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86848d4d-852e-4e21-8395-18065e0c9fa5",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "The code from Question 1 currently performs the reduction operation for a single thread block.\n",
    "Modify the code to perform reduction using multiple blocks (say $16$ of them) with each block working with a different section of the input array.\n",
    "\n",
    "There are two ways in which the partial sums from each block can be summed:\n",
    "* Each block puts its partial sum into a different element of the output\n",
    "array, and then these are transferred to the host and summed there;\n",
    "* An atomic addition is used to safely increment a single global sum.\n",
    "\n",
    "Implement both and check that you get the correct answer.\n",
    "\n",
    "Finally, implement the reduction using:\n",
    "* The `cuda.reduce` decorator.\n",
    "* `cupy.sum`.\n",
    "\n",
    "Time all four versions. Which one is faster? You can play with block sizes if you want. Can you see why it is much easier to not write reductions yourself?\n",
    "\n",
    "**Note:** In my code `cuda.reduce` was surprisingly slow. I suspect it does some host/device memory movement, which it shouldn't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5e890-f7a7-4e72-ad40-3a13d0396ee2",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "The code from Question 1 currently assumes the number of threads is a power of 2.\n",
    "Extend it to handle the general case by finding the largest power of 2 less than\n",
    "`blockSize`, and adding the elements beyond that point to the corresponding\n",
    "first set of elements of that size. Test it with 192 threads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuPy Kernel",
   "language": "python",
   "name": "cupy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
